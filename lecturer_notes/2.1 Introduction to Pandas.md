
# Introduction to Pandas for Data Analysis

**Pandas** is a popular open-source Python library for data analysis and manipulation. It provides high-performance, easy-to-use data structures (notably, Series and DataFrame) and numerous functions to clean, transform, and analyze data ([Python Pandas Tutorial: A Beginners Guide | Flexiple - Flexiple](https://flexiple.com/python/python-pandas-tutorial#:~:text=Pandas%20in%20Python%20is%20a,science%20and%20programming%20in%20Python)). The name "pandas" is derived from **panel data**, an econometrics term for multidimensional structured datasets ([Pandas Material - Pandas: pandas is a software library written for the Python programming language - Studocu](https://www.studocu.com/en-us/document/texas-tech-university/computational-thinking-with-data-science/pandas-material/50443842#:~:text=pandas%20is%20a%20software%20library,language%20for%20data%20manipulation%20and)). In essence, pandas makes working with **tabular data** (like spreadsheets or SQL tables) in Python simple and intuitive. This session will introduce the core concepts of pandas and demonstrate how to use it for basic data analysis tasks.

## Installation and Setup

Before using pandas, you need to install it and set up a working environment (ideally a Jupyter Notebook for interactive experimentation):

- **Using pip (Python's package manager):** In your command-line or terminal, run:  
  ```bash
  pip install pandas
  ```  
  This installs pandas from the Python Package Index. You can then import it in Python with `import pandas as pd`.

- **Using Anaconda/conda:** If you are using the Anaconda distribution (commonly used for data science), pandas is likely already included. Otherwise, you can install it via the Anaconda Prompt or terminal:  
  ```bash
  conda install pandas
  ```  
  This will install pandas into your conda environment.

- **Jupyter Notebook setup:** For an interactive environment, install Jupyter:  
  ```bash
  pip install jupyterlab
  ```  
  *(Or use `conda install jupyter` if using Anaconda.)*  
  After installation, start Jupyter Notebook or JupyterLab by running `jupyter notebook` or `jupyter lab`. In a notebook, you can write code in cells and see outputs immediately, which is great for data analysis.

> **Tip:** It's common to import pandas with the alias `pd` (i.e., `import pandas as pd`). This is a convention that makes it quicker to reference pandas functions. Remember to run this import at the top of your scripts or notebooks.

## What is Pandas and Why Use It?

Pandas is a **data analysis toolkit** built on top of NumPy, designed to handle **tabular data** (i.e., data with rows and columns) efficiently. It offers two primary data structures:

- **Series:** a one-dimensional labeled array (like a column of data).
- **DataFrame:** a two-dimensional table with labeled axes (rows and columns).

Pandas excels at tasks like **data cleaning, transformation, and analysis**. It simplifies reading/writing data from various formats (CSV, Excel, SQL, JSON), handling missing values, filtering data, computing statistics, and even basic visualization. Analysts and data scientists use pandas to **load**, **prepare**, and **analyze** datasets of all sizes. With pandas, many data operations that would require lengthy pure Python code can be done in a few lines, making your code more concise and readable ([Python Pandas Tutorial: A Beginners Guide | Flexiple - Flexiple](https://flexiple.com/python/python-pandas-tutorial#:~:text=Pandas%20in%20Python%20is%20a,science%20and%20programming%20in%20Python)).

Some reasons to use pandas for data analysis:

- It’s **fast and efficient** for manipulating large datasets.
- It provides expressive syntax to **filter, aggregate, or transform** data with ease.
- It seamlessly integrates with other libraries like NumPy, Matplotlib (for plotting), and scikit-learn (for machine learning).
- It handles **missing data** gracefully (using NaN markers) ([Intro to data structures — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/dsintro.html#:~:text=Note)) and provides tools to fill or drop those as needed.
- It has powerful **grouping ("split-apply-combine")** and **joining/merging** capabilities for combining datasets.

Overall, pandas is a foundational tool in the Python data science ecosystem, often the first step in turning raw data into insights.

> **Note:** Pandas is typically imported as `pd`. So when you see code like `pd.Series()` or `pd.DataFrame()`, it means we're using the pandas library. Make sure you've run `import pandas as pd` beforehand, or these will not work (a common beginner mistake is forgetting the import).

## Pandas Series

A **Series** in pandas is a one-dimensional labeled array that can hold any data type (integers, floats, strings, etc.) ([Intro to data structures — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/dsintro.html#:~:text=Series%20%20is%20a%20one,39%20is%20to%20call)). You can think of a Series as a **column of data** with an index. Each value in a Series has an **index label**, which defaults to integers 0, 1, 2, ... but can be set to other values (like dates or strings) to label each data point.

**Creating a Series:** You can create a Series from a list, NumPy array, scalar value, or a Python dictionary. Here are a few examples:

```python
import pandas as pd

# Create a Series from a Python list
sales = pd.Series([10, 20, 30])
print(sales)
```

**Output:**
```
0    10
1    20
2    30
dtype: int64
```

By default, pandas assigns an index starting from 0. In the above example, the index is `0, 1, 2` and the values are 10, 20, 30. The `dtype: int64` at the bottom indicates the data type of the Series (64-bit integer in this case).

We can also specify a custom index:

```python
# Create a Series with a custom index (e.g., labels for each data point)
salesQ1 = pd.Series([10, 20, 30], index=['Jan', 'Feb', 'Mar'])
print(salesQ1)
```

**Output:**
```
Jan    10
Feb    20
Mar    30
dtype: int64
```

Here, we created a Series `salesQ1` with index labels 'Jan', 'Feb', 'Mar' instead of default 0,1,2. This Series might represent sales in thousands for the first quarter months. We can retrieve values by these labels:

```python
# Accessing elements in a Series by index label
print(salesQ1['Feb'])   # value for index 'Feb'
```

**Output:**
```
20
```

If you use numeric indices (0, 1, 2, ...), you can also access by position similar to a list:

```python
print(sales[1])   # second element of the 'sales' Series
```

**Output:**
```
20
```

However, be careful: if your Series has a numeric index (e.g., 0,1,2 as labels), `series_obj[1]` will refer to the label `1`, which in this case is also the second element. With a non-integer index (like 'Jan', 'Feb'), `series_obj[1]` will instead get the item at position 1 (because label 1 doesn't exist). To avoid confusion between label-based and position-based indexing, pandas provides **`.loc`** (label-based) and **`.iloc`** (integer position-based) accessors. For a Series, `s.loc[label]` and `s.iloc[position]` can be used explicitly.

**Series from a dictionary:** If you create a Series from a dict, the keys become the index labels and the values become the data:

```python
grades = pd.Series({'Alice': 85, 'Bob': 90, 'Charlie': 95})
print(grades)
```

**Output:**
```
Alice      85
Bob        90
Charlie    95
dtype: int64
```

This Series `grades` might represent student scores. The index is `'Alice', 'Bob', 'Charlie'`. We can easily access a specific value by name:

```python
print(grades['Bob'])
```
```
90
```

Notice how similar this is to looking up a value by key in a dictionary, but the Series provides additional functionality and compatibility with other pandas operations.

**Basic operations on Series:** Pandas Series support vectorized operations and NumPy functions. For example, you can do arithmetic on a Series across all elements at once:

```python
# Arithmetic on Series
double_sales = sales * 2
print(double_sales)
```

**Output:**
```
0    20
1    40
2    60
dtype: int64
```

This multiplied every value in the `sales` Series by 2. Similarly, many NumPy functions will work element-wise on Series:

```python
import numpy as np
print(np.log(sales))  # natural log of each sales value
```

Pandas will align operations by index, meaning if you have two Series with the same index, you can add or subtract them and pandas will match values by the index labels.

> **Tip:** A Series is like a **column** in a table. But if you print a Series, it looks similar to a column with its index. If you get output that looks like `Name: <something>, dtype: <type>` at the end, that indicates it's a Series (with that name and data type). If you accidentally put an extra set of brackets when selecting data from a DataFrame (e.g., `df[['col']]`), you might get a DataFrame instead of Series. Remember: single bracket `df['col']` -> Series, double brackets `df[['col']]` -> DataFrame.

## Pandas DataFrame

A **DataFrame** is the most important data structure in pandas. It is a two-dimensional labeled data structure with columns of potentially different types ([Intro to data structures — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/dsintro.html#:~:text=DataFrame%20is%20a%202,many%20different%20kinds%20of%20input)). You can think of it as a **table (spreadsheet)** in Python: it has rows and columns, each column can have a name (label), and each row has an index (which can be numbers or labels).

Key characteristics of DataFrames:
- It’s like a dict of Series objects – each column in the DataFrame is a Series.
- It can be created from many sources: dictionaries, lists, external files (CSV, Excel), databases, etc.
- You can specify an index (row labels) and column labels; if not provided, pandas will default to numeric indices.

**Creating DataFrames:**

1. **From a dictionary of lists:** Each key becomes a column name, and the list/array associated with it becomes the column’s data.

   ```python
   data = {
       'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],
       'Age': [24, 27, 22, 32, 29],
       'City': ['NY', 'LA', 'Chicago', 'NY', 'LA']
   }
   df = pd.DataFrame(data)
   print(df)
   ```

   **Output:**
   ```
         Name  Age     City
   0    Alice   24       NY
   1      Bob   27       LA
   2  Charlie   22  Chicago
   3    David   32       NY
   4      Eva   29       LA
   ```

   Here, we have a DataFrame `df` with 5 rows and 3 columns (Name, Age, City). pandas automatically assigned the index 0 to 4 for the rows.

2. **From a list of dictionaries:** Each dictionary in the list could represent a row.

   ```python
   data_list = [
       {'Name': 'Alice', 'Age': 24, 'City': 'NY'},
       {'Name': 'Bob', 'Age': 27, 'City': 'LA'},
       {'Name': 'Charlie', 'Age': 22, 'City': 'Chicago'}
   ]
   df2 = pd.DataFrame(data_list)
   print(df2)
   ```

   **Output:**
   ```
         Name  Age     City
   0    Alice   24       NY
   1      Bob   27       LA
   2  Charlie   22  Chicago
   ```

   This accomplishes the same as above for 3 rows. Using a list of dicts or a dict of lists is mostly preference or convenience based on data layout.

3. **Read from a CSV file:** Often, data comes in files. Pandas makes it easy to read a CSV (comma-separated values) file into a DataFrame:

   ```python
   df_sales = pd.read_csv('sales_data.csv')
   ```
   This will load the contents of *sales_data.csv* into `df_sales` (you need to have the file path correct). We will discuss I/O (input/output) more later. For now, just know that `pd.read_csv` is a very common way to create a DataFrame from data stored in a file.

You can get a quick overview of a DataFrame using methods like `df.head()` (shows the first 5 rows by default), `df.tail()` (last 5 rows), `df.shape` (gives a tuple of (num_rows, num_columns)), and `df.info()` (summary of the DataFrame including data types and non-null counts). For example:

```python
print(df.head(3))    # first 3 rows
print(df.shape)      # (number of rows, number of columns)
df.info()            # summary info printed
```

> **Tip:** Use `df.head()` to peek at large datasets instead of printing the entire DataFrame. This prevents overwhelming output and is useful to ensure data loaded correctly.

**Selecting Data from DataFrame (Indexing):**

Selecting data in a DataFrame can be done by column or by row (or both). There are several ways:

- **By column name:** `df['Age']` will give the **Series** corresponding to the "Age" column. Similarly, `df[['Name', 'City']]` will give a new DataFrame with only the "Name" and "City" columns.

  ```python
  ages = df['Age']
  print("Age column as Series:")
  print(ages)
  ```
  ```
  Age column as Series:
  0    24
  1    27
  2    22
  3    32
  4    29
  Name: Age, dtype: int64
  ```

  If you need multiple columns, pass a list of column names: `df[['Name', 'Age']]`.

- **By row index:** Pandas uses the `.loc` and `.iloc` accessors for rows.
  - `df.loc[index_label]` selects a row by the **index label**. For example, `df.loc[2]` would get the row with index 2 (here, index are 0-4 by default). If your DataFrame has a custom index (say, names or dates), you would use those labels with `.loc`.
  - `df.iloc[index_position]` selects a row by the **integer position** (similar to how Python list indexing works). `df.iloc[2]` will get the third row (since Python indexing is 0-based).

  For example:
  ```python
  print("Row with index 2 using loc:")
  print(df.loc[2])
  print("\nRow with index 2 using iloc:")
  print(df.iloc[2])
  ```
  **Output:**
  ```
  Row with index 2 using loc:
  Name      Charlie
  Age            22
  City     Chicago
  Name: 2, dtype: object

  Row with index 2 using iloc:
  Name      Charlie
  Age            22
  City     Chicago
  Name: 2, dtype: object
  ```
  In this case they look the same because our index label is the same as the position. If the DataFrame had a custom index (say we set `Name` as index), `.loc` would use that name.

  You can also select a subset of rows and columns together by using `df.loc[row_index, col_index]`. For instance, `df.loc[0:2, ['Name', 'Age']]` would give a DataFrame of rows 0 through 2 (inclusive) with only the Name and Age columns.

- **Slicing:** Similar to Python lists, you can slice DataFrame rows by index. `df[1:4]` will give rows with index 1, 2, 3 (note: when slicing with `df[start:stop]` the stop is excluded, just like normal Python slicing). However, it's often clearer to use `.iloc` for slicing by position or `.loc` for slicing by label range.

> **Tip:** Remember that `.loc` is label-based and includes the end of the slice, whereas `.iloc` is position-based and works like typical Python slicing (excluding the end). For example, if your index is 0...4, `df.loc[0:2]` returns indices 0,1,2 **inclusive**, while `df.iloc[0:2]` returns indices 0,1. This is a common source of confusion for beginners.

**DataFrame vs Series output:** Selecting a single column or a single row yields a Series. Selecting multiple columns or multiple rows yields a DataFrame. Keep this in mind, as operations and available methods can differ between Series and DataFrames.

## Data Types and Type Conversion

Each column in a DataFrame has a **dtype** (data type), which pandas assigns at creation. You can check the data types of all columns using `df.dtypes` or `df.info()`. Common pandas dtypes include `int64` (integer), `float64` (float), `object` (typically strings or mixed types), `bool` (boolean), `datetime64` (dates and times), etc.

For example, in our `df`:

```python
print(df.dtypes)
```
**Output:**
```
Name    object
Age      int64
City    object
dtype: object
```

This tells us Name and City are of type `object` (string in this context) and Age is `int64`. 

Sometimes, you may need to **convert data types**. For instance, a column of numbers might be read as strings (object dtype), or a numeric column might need to be converted to categorical type to save memory.

- To **convert** a column’s type, use `astype`. For example, if `df['Age']` were read as strings, we could do `df['Age'] = df['Age'].astype(int)` to convert it to integers.
- Pandas also provides `pd.to_datetime()` to convert a date string column to actual datetime objects, `pd.to_numeric()` to force convert strings to numbers (useful if some values are non-numeric), and `astype('category')` to convert to categorical type.

Example:
```python
# Suppose Age is read as string, convert to numeric
df['Age'] = df['Age'].astype('int64')
# Convert City to category (useful if City has a few distinct values repeated)
df['City'] = df['City'].astype('category')
print(df.dtypes)
```

**Output:**
```
Name      object
Age        int64
City    category
dtype: object
```

In this example, `City` became a categorical type, which can be more memory-efficient if there are only a few unique cities. Converting types is often useful for optimizing or preparing data for certain operations (e.g., modeling or merging).

> **Tip:** If you have numeric data stored as strings (e.g., `"25"` instead of 25), use `pd.to_numeric(df['col'], errors='coerce')` to convert them. The `errors='coerce'` will turn non-convertible strings into NaN. This is a handy way to clean columns with mixed types.

## Basic DataFrame Operations

Once you have your DataFrame, you’ll want to explore and manipulate it. Here are some fundamental operations:

### Filtering Rows (Conditional Selection)

You can filter the DataFrame based on conditions on its columns. This is sometimes called **boolean indexing**:

```python
# Filter the DataFrame to get only people older than 25
older_than_25 = df[df['Age'] > 25]
print(older_than_25)
```

**Output:**
```
    Name  Age City
1    Bob   27   LA
3  David   32   NY
4    Eva   29   LA
```

This returns a new DataFrame with only the rows where the condition `df['Age'] > 25` is True. In our data, Bob (27), David (32), and Eva (29) meet the condition. You can combine multiple conditions using `&` (and), `|` (or), and `~` (not), but remember to wrap each condition in parentheses. For example: 
```python
df[(df['Age'] > 25) & (df['City'] == 'NY')]
``` 
would filter for rows where Age > 25 **and** City is 'NY'.

### Sorting

To sort the DataFrame by one or more columns, use **`df.sort_values()`**:

```python
# Sort by Age in ascending order
sorted_by_age = df.sort_values('Age')
print(sorted_by_age)
```

**Output:**
```
      Name  Age     City
2  Charlie   22  Chicago
0    Alice   24       NY
1      Bob   27       LA
4      Eva   29       LA
3    David   32       NY
```

Charlie (22) is now first, and David (32) last, as expected. By default, `sort_values` sorts ascending; use `ascending=False` for descending order:

```python
# Sort by Age in descending order
df.sort_values('Age', ascending=False)
```

You can also sort by multiple columns:
```python
# Sort by City, then by Age within each City
df.sort_values(['City', 'Age'])
```
This will group by City and sort ages within each city group.

### Arithmetic on Columns and Creating New Columns

Pandas allows vectorized arithmetic operations on columns. For example, you can add, subtract, multiply, divide entire columns (Series) by other columns or scalars.

```python
# Create a new column based on existing ones
df['AgeNextYear'] = df['Age'] + 1
print(df[['Name', 'Age', 'AgeNextYear']])
```

**Output:**
```
      Name  Age  AgeNextYear
0    Alice   24           25
1      Bob   27           28
2  Charlie   22           23
3    David   32           33
4      Eva   29           30
```

We added 1 to everyone’s age to project their age next year. We could similarly create columns as combinations of others (e.g., ratio, difference, etc.).

If we had another numeric column, we could do element-wise operations. For instance, imagine a `df['Sales']` and `df['Costs']`, then `df['Profit'] = df['Sales'] - df['Costs']` would compute profit for each row.

### Summary Statistics

Pandas makes it easy to quickly get summary statistics of your data:

- **Single aggregation functions:** You can call methods like `.mean()`, `.median()`, `.sum()`, `.min()`, `.max()`, etc., on a Series (or numeric DataFrame columns) to get that statistic.

  ```python
  print("Average age:", df['Age'].mean())
  print("Max age:", df['Age'].max())
  ```
  ```
  Average age: 26.8
  Max age: 32
  ```
  (The average age of our sample is 26.8 years, and the max is 32.)

- **`.describe()` method:** This is a handy function that gives a bunch of statistics at once for numeric columns:

  ```python
  print(df.describe())
  ```

  **Output:**
  ```
              Age  AgeNextYear
  count   5.000000     5.000000
  mean   26.800000    27.800000
  std     3.420526     3.420526
  min    22.000000    23.000000
  25%    24.000000    25.000000
  50%    27.000000    28.000000
  75%    29.000000    30.000000
  max    32.000000    33.000000
  ```
  It shows count, mean, standard deviation, min, various percentiles, and max for each numeric column. Non-numeric columns are ignored by default (or given summary like count of unique, etc., if using `describe(include='all')`).

These quick summaries help in understanding the distribution of your data and catching any anomalies (e.g., an absurdly high max might indicate an outlier or data error).

> **Note:** By default, these operations skip missing values. For instance, `mean()` will calculate the mean of available (non-missing) values. We will cover missing data handling next.

### Modifying Data

You can modify values in the DataFrame by assignment:

- Single value: `df.loc[0, 'Age'] = 25` would change Alice’s age to 25.
- A column: `df['Age'] = df['Age'] + 1` would increase everyone’s age by 1 (you'd effectively be overwriting the Age column).
- A slice or subset: `df.loc[df['Name']=='Bob', 'City'] = 'San Francisco'` would change Bob’s city to San Francisco.

When modifying, using `.loc` is recommended to avoid the dreaded SettingWithCopy warning (which happens if pandas is unsure whether you're modifying the original DataFrame or a copy). `.loc` helps explicitly target the original.

## Handling Missing Data

Real-world data is often incomplete. Pandas uses `NaN` (Not a Number) as the default marker for missing values ([Intro to data structures — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/dsintro.html#:~:text=Note)) (you may also see `None` in object-type columns, which pandas typically treats as NaN under the hood). It's crucial to identify and handle missing data appropriately.

**Detecting missing data:**  
- `df.isna()` or `df.isnull()` returns a DataFrame of booleans of the same shape, with True indicating a missing value.
- `df.notna()` or `df.notnull()` does the opposite.
- You can chain `.sum()` to count missing values per column (since True will be treated as 1 in a sum):

  ```python
  print(df.isna().sum())
  ```
  This might output something like:
  ```
  Name    0
  Age     1
  City    1
  dtype: int64
  ```
  indicating one missing Age and one missing City in the DataFrame (just as an example).

**Example with missing data:** Let's create a small DataFrame with some missing entries:

```python
data = {
    'Name': ['Tom', 'Jane', 'Steve'],
    'Score': [90, None, 85],    # Jane's score is missing (None -> NaN)
    'Grade': [None, 'B', 'A']   # Tom's grade is missing
}
df2 = pd.DataFrame(data)
print(df2)
```

**Output:**
```
   Name  Score Grade
0   Tom   90.0  None
1  Jane    NaN     B
2 Steve   85.0     A
```

You can see `NaN` for Jane's Score and `None` for Tom's Grade (displayed as None). Next, common operations to handle these:

- **Dropping missing data:** If a few rows have missing values and you prefer to drop them:
  - `df.dropna()` will drop any row with at least one missing value.
  - `df.dropna(axis=1)` would drop columns that have any missing values (useful if a column is too incomplete to be useful).
  - You can also pass parameters like `how='all'` (to drop only if all values in the row are missing) or `thresh=` (to drop if fewer than a certain number of non-NaNs).

  ```python
  print("Drop rows with any missing:")
  print(df2.dropna())
  ```
  **Output:**
  ```
  Drop rows with any missing:
     Name  Score Grade
  2 Steve   85.0     A
  ```
  Only Steve's row had no missing values, so Tom and Jane were dropped.

- **Filling missing data:** Often, it's better to fill missing values with some appropriate substitute:
  - Use `df.fillna(value)` to fill all NaNs with the given value (such as 0, or 'Unknown').
  - You can fill with different values for different columns by passing a dict: `df.fillna({'Score': 0, 'Grade': 'Unknown'})`.
  - Methods like `fillna(method='ffill')` (forward-fill) or `'bfill'` (backward-fill) propagate last known value forward or backward, which can be useful in time-series data.
  - You might fill numeric values with the mean/median: e.g., `df['Score'].fillna(df['Score'].mean())`.

  ```python
  filled_df2 = df2.fillna({'Score': 0, 'Grade': 'Unknown'})
  print("Filled missing values:")
  print(filled_df2)
  ```
  **Output:**
  ```
  Filled missing values:
     Name  Score    Grade
  0   Tom   90.0  Unknown
  1  Jane    0.0        B
  2 Steve   85.0        A
  ```
  Now Jane's Score is 0 instead of NaN (maybe 0 is a placeholder indicating no score) and Tom's Grade is 'Unknown' instead of None.

Choosing whether to drop or fill missing data depends on context and how much missing data there is. For instance, dropping 2 rows out of 10000 might be fine, but dropping 2 out of 5 would severely reduce your data! Filling requires thinking about what a missing value means (e.g., should a missing score be treated as 0, or maybe the average?).

> **Tip:** By default, `dropna()` and `fillna()` return a new DataFrame and do not change the original data. If you want to modify in place, you can do `df.dropna(inplace=True)`. Otherwise, remember to assign the result back to a variable (or to `df` itself) if you want to keep the changes. A common mistake is calling `df.fillna(...)` without assignment and assuming `df` is updated – it isn't, unless you passed `inplace=True` or captured the return.

Pandas also has more advanced missing data handling (like interpolation or using `fillna` with different strategies), but for beginners, knowing `dropna` and `fillna` is a great start.

## Basic Plotting with pandas

Pandas integrates with the **Matplotlib** library to provide basic plotting capabilities. This means you can quickly visualize your data directly from a DataFrame or Series. The built-in plotting is accessible via the `.plot()` method on Series/DataFrame.

For example, if you have a DataFrame of some sample data:

```python
import matplotlib.pyplot as plt

# Sample data for plotting
months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun"]
sales = [150, 200, 180, 220, 210, 230]
expenses = [120, 160, 150, 180, 170, 190]
df_plot = pd.DataFrame({"Month": months, "Sales": sales, "Expenses": expenses})
```

We have `df_plot` with columns for Month, Sales, and Expenses. To create a simple line plot of sales over months:

```python
df_plot.plot(x='Month', y='Sales', kind='line', marker='o', title='Monthly Sales')
plt.xlabel('Month')
plt.ylabel('Sales')
plt.show()
```

Running the above in a Jupyter notebook will display a line chart with Month on the x-axis and Sales on the y-axis, with a point marker for each month. You can also plot multiple columns at once. For instance:

```python
df_plot.plot(x='Month', y=['Sales', 'Expenses'], kind='line', marker='o', title='Monthly Sales and Expenses')
plt.xlabel('Month'); plt.ylabel('Amount'); plt.show()
```

This will plot two lines (one for Sales, one for Expenses) on the same chart, making it easy to compare trends.

Pandas' `.plot` method is high-level: under the hood, it uses Matplotlib (and in recent versions can use other backends) ([Pandas - Plotting - W3Schools](https://www.w3schools.com/python/pandas/pandas_plotting.asp#:~:text=Pandas%20,the%20diagram%20on%20the%20screen)). You can specify `kind` parameter to create different plot types:
- `kind='line'` for line chart (default for numeric data),
- `kind='bar'` or `'barh'` for bar charts,
- `kind='hist'` for histogram,
- `kind='box'` for boxplot,
- `kind='scatter'` for scatter (in DataFrame.plot, you also specify `x` and `y` for scatter),
- etc.

For example, to quickly visualize the distribution of ages in our earlier DataFrame:

```python
df['Age'].plot(kind='hist', bins=5, title='Age Distribution')
plt.xlabel('Age'); plt.show()
```

This will show a histogram of the Age column.

> **Note:** Plotting in pandas is intended for convenience. For more complex plots or customization, you might use Matplotlib or specialized libraries (seaborn, plotly, etc.) directly. But for exploratory data analysis, pandas plots are often enough to get a sense of the data.

## GroupBy and Aggregation

One of the most powerful features of pandas is the ability to **group data** and compute aggregate statistics on those groups, often referred to as the "split-apply-combine" paradigm ([Group by: split-apply-combine — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/groupby.html#:~:text=By%20%E2%80%9Cgroup%20by%E2%80%9D%20we%20are,more%20of%20the%20following%20steps)). The idea is:
1. **Split** the data into groups based on some criteria (e.g., a column value).
2. **Apply** a function to each group independently (e.g., calculate group’s average).
3. **Combine** the results into a new data structure.

In pandas, grouping is done with the `.groupby()` method, and then you can apply aggregate functions.

For example, consider we have a DataFrame `df3` of employees with their department and salary:

```python
data = {
    'Department': ['Sales', 'Sales', 'HR', 'HR', 'HR', 'IT'],
    'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank'],
    'Salary': [50000, 60000, 52000, 58000, 60000, 70000]
}
df3 = pd.DataFrame(data)
print(df3)
```
**Output:**
```
  Department Employee  Salary
0      Sales    Alice   50000
1      Sales      Bob   60000
2         HR  Charlie   52000
3         HR    David   58000
4         HR      Eva   60000
5         IT    Frank   70000
```

We might want to know the average salary in each department. Using groupby:

```python
avg_salary = df3.groupby('Department')['Salary'].mean()
print(avg_salary)
```

**Output:**
```
Department
HR       56666.666667
IT       70000.000000
Sales    55000.000000
Name: Salary, dtype: float64
```

This output is a Series where the index is Department and the values are the mean salaries. So:
- HR average is ~56666.67,
- IT average is 70000,
- Sales average is 55000.

We could also get the sum of salaries by department:

```python
total_salary = df3.groupby('Department')['Salary'].sum()
print(total_salary)
```
```
Department
HR       170000
IT        70000
Sales    110000
Name: Salary, dtype: int64
```

Grouping can be done by one or multiple keys (e.g., you could group by Region and Department together if those were columns). After grouping, you can apply a variety of aggregation functions:
- Direct methods like `.mean()`, `.sum()`, `.count()`, `.max()`, `.min()`, etc., as we saw.
- The `.agg()` method, which can apply one or multiple functions at once. For example: 
  ```python
  df3.groupby('Department')['Salary'].agg(['mean', 'max', 'min])
  ``` 
  would give a DataFrame with mean, max, and min salary for each department.

Pandas groupby is very powerful for summarizing data: it's how you can go from raw data to aggregated information. This is similar to SQL "GROUP BY" or Excel pivot tables. In fact, the process is often described as split-apply-combine in data analysis ([Group by: split-apply-combine — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/groupby.html#:~:text=By%20%E2%80%9Cgroup%20by%E2%80%9D%20we%20are,more%20of%20the%20following%20steps)) – you split data into groups, apply calculations on each, and combine the results.

> **Tip:** After a `groupby`, the result is a grouped object. You typically need to follow it with an aggregation function (like `.mean()` or `.sum()`) or an `.agg()` call to get a useful result. If you see something like `<pandas.core.groupby.generic.DataFrameGroupBy object at ...>` it means you just grouped but didn't do anything with it yet. Also, if you want the result as a DataFrame instead of Series, you can group by a column and aggregate multiple columns or use `reset_index()` on the result to turn the group labels back into a normal column.

## Saving and Loading Data

One of the reasons pandas is so popular is the ease of reading from and writing to various data formats. In real use, you'll often load data from files, work on it, then save your results.

Here we'll cover **CSV** and **Excel**, as they are common formats:

- **Reading a CSV:** Use `pd.read_csv('file_path.csv')` to load data. For example:
  ```python
  df = pd.read_csv('mydata.csv')
  ```
  pandas will infer the delimiter (comma by default, but you can specify others via `sep=`) and read the file into a DataFrame. You might need to specify options if the file has a header row (by default, first row is assumed to be column names) or an index column, etc., but for most basic cases `read_csv` is straightforward.

- **Writing to CSV:** Use `df.to_csv('file_path.csv', index=False)` to save. For example:
  ```python
  df.to_csv('cleaned_data.csv', index=False)
  ```
  This will write the DataFrame to a CSV file. We often use `index=False` to avoid writing the row index to the file (otherwise, the index will be an extra column in the CSV).

  > **Tip:** If you open a CSV saved from pandas and see an extra unnamed column, it's likely the index. Setting `index=False` when saving prevents that.

- **Reading an Excel file:** Use `pd.read_excel('file.xlsx', sheet_name='Sheet1')`. You need to have `openpyxl` or `xlrd` installed for pandas to read Excel files. This works similarly to read_csv. You can specify which sheet to read via `sheet_name`. It returns a DataFrame.

- **Writing to Excel:** Use `df.to_excel('file.xlsx', index=False)`. This will create an Excel file (typically using openpyxl engine). Be mindful that writing to Excel is a bit slower than CSV, and it’s not intended for extremely large datasets, but it's very useful for sharing data with others in a familiar format.

**Example:**

```python
# Save df to CSV and read it back
df.to_csv('people.csv', index=False)
df_loaded = pd.read_csv('people.csv')
print(df_loaded.head())

# Save to Excel and read back (requires openpyxl installed)
df.to_excel('people.xlsx', index=False)
df_loaded_xls = pd.read_excel('people.xlsx')
print(df_loaded_xls.head())
```

If `df` was our earlier DataFrame, after writing and reading, `df_loaded` should look the same as `df`. The `head()` call will show the first few rows to confirm.

Pandas supports many other formats:
- **JSON:** `pd.read_json()`, `df.to_json()`
- **HTML:** `pd.read_html()` to parse tables from HTML pages.
- **SQL:** If you have a database connection, `pd.read_sql()` to run a SQL query and get DataFrame, and `df.to_sql()` to write a DataFrame to a SQL table.
- **Pickle:** `df.to_pickle()` and `pd.read_pickle()` to serialize DataFrames (useful for quick save/load of Python objects).
- **HDF5:** `df.to_hdf()` for large data storage in binary format.

For this beginner session, focus on CSV and Excel as they are straightforward and common. Always ensure you have the necessary permissions and correct file path when reading/writing, and note that Excel support requires an extra dependency (which Anaconda usually includes by default).

> **Note:** When using `read_csv` or `read_excel`, pandas may make educated guesses about data types which sometimes need adjustment (e.g., a column of IDs might be read as int, but if there's a missing value it becomes float). After loading, always verify `df.dtypes` and the content (using `df.head()`) to ensure data is read as expected.

## Real-World Examples of Pandas in Different Domains

To conclude, let's look at a few short examples of how pandas can be applied to various real-world data analysis scenarios:

- **Business (Sales data):** Imagine a CSV file with monthly sales for different regions. You can use pandas to read the sales data, compute total sales per region or per product, and sort to find the best-performing region ([Group by: split-apply-combine — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/groupby.html#:~:text=,Some%20examples)). For example, group sales by region:
  ```python
  sales_df = pd.read_csv('monthly_sales.csv')
  regional_totals = sales_df.groupby('Region')['Revenue'].sum()
  print(regional_totals)
  ```
  This would quickly give the total revenue for each region. Pandas could also help in computing growth month-over-month, merging targets vs actuals, or filtering transactions for a given client. Many business analysts use pandas to replace or supplement Excel for larger datasets.

- **Science (Experimental data):** Suppose you have a dataset of daily temperature readings from multiple sensors. You can use pandas to clean the data (handle missing readings), calculate summary statistics like mean, min, max temperature for each day or each sensor, and visualize the trends. For example, to find the average temperature per day from a time-series DataFrame:
  ```python
  avg_temp_per_day = temp_df.groupby(temp_df['Date'].dt.date)['Temperature'].mean()
  ```
  This would give a daily average (assuming `Date` is a datetime column). Scientists often use pandas to reshape data (pivot tables), filter out outliers, or combine measurements from different sources for analysis.

- **Social Media/Web (Log or tweet analysis):** If you have a collection of tweets in a JSON or CSV (with columns like username, timestamp, content, likes), pandas can be used to parse this data. For instance, you could use pandas to count how many tweets each user made:
  ```python
  tweets = pd.read_json('tweets.json')
  tweet_counts = tweets['username'].value_counts()
  print(tweet_counts.head())
  ```
  This might show the top 5 most active users and their tweet counts. You could also filter tweets containing certain keywords, or use pandas datetime functionality to analyze activity over time (e.g., tweets per day or hour). Web logs (with URLs, timestamps) can similarly be parsed to find most visited pages, response code counts, etc.

- **Education (Grades and attendance):** Consider a scenario with students' grades and attendance records in two CSV files. Pandas can merge these datasets on a student ID, allowing educators to analyze the relationship between attendance and performance. For example:
  ```python
  grades = pd.read_csv('grades.csv')         # contains: StudentID, Exam1, Exam2, ...
  attendance = pd.read_csv('attendance.csv') # contains: StudentID, DaysPresent, DaysAbsent
  merged = pd.merge(grades, attendance, on='StudentID')
  avg_grade = merged[['Exam1', 'Exam2', 'Exam3']].mean(axis=1)
  merged['AverageGrade'] = avg_grade
  print(merged[['StudentID','AverageGrade','DaysPresent']].head())
  ```
  This would combine the data and compute each student's average grade, which could then be compared to their attendance. Pandas makes it easy to do such joins and column-wise computations. Educators can quickly identify if poor attendance correlates with lower grades, for instance.

Each of these examples shows how pandas serves as a **Swiss army knife for data** in different domains: from aggregating business metrics to cleaning scientific data or analyzing user behavior. The flexibility of pandas means that once you grasp the basics covered in this session (Series, DataFrames, indexing, operations, groupby, etc.), you can apply those same skills to a wide variety of datasets and problems.

---

By now, you should have a solid introductory understanding of pandas. We've covered how to install it, create and manipulate Series and DataFrames, handle missing data, perform basic computations, visualize data, and save results. These are the fundamental building blocks for any data analysis in Python using pandas. As you practice, you'll discover more advanced features (like merging DataFrames, working with time series, pivot tables, categorical data, etc.), but the concepts in these notes will remain core to your workflow. Happy data analysis with pandas! ([Group by: split-apply-combine — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/groupby.html#:~:text=By%20%E2%80%9Cgroup%20by%E2%80%9D%20we%20are,more%20of%20the%20following%20steps)) ([Intro to data structures — pandas 2.1.1 documentation](https://pandas.pydata.org/pandas-docs/version/2.1.1/user_guide/dsintro.html#:~:text=Note))


# Pandas DataFrame Concepts Quiz

**Question 1:** Which of the following will correctly create a pandas DataFrame with columns "col1" and "col2"?  
**Options:**  
- A. `pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})`  
- B. `pd.DataFrame(['col1': [1, 2, 3], 'col2': [4, 5, 6]])`  
- C. `pd.DataFrame([1, 2, 3], [4, 5, 6])`  
- D. `pd.DataFrame('col1', [1, 2, 3], 'col2', [4, 5, 6])`  

**Answer:** A. Using a dictionary of equal-length lists for each column is the correct way to create a DataFrame ([10 minutes to pandas — pandas 2.2.3 documentation](https://pandas.pydata.org/docs/user_guide/10min.html#:~:text=Pandas%20provides%20two%20types%20of,classes%20for%20handling%20data)). Options B, C, and D have improper syntax. Option A will produce a DataFrame with two columns (`col1` and `col2`) and three rows.

**Question 2:** What is the default index of a DataFrame if none is specified when creating it?  
**Options:**  
- A. An integer index starting at 0 (0, 1, 2, …)  
- B. An integer index starting at 1 (1, 2, 3, …)  
- C. No index (rows are not indexed by default)  
- D. An index named "index" with default values  

**Answer:** A. By default, pandas assigns a **RangeIndex** starting from 0 to the DataFrame ([10-question multiple-choice quiz on Pandas ~ Computer Languages (clcoding)](https://www.clcoding.com/2024/02/10-question-multiple-choice-quiz-on.html#:~:text=3,file%20into%20a%20Pandas%20DataFrame)). This means the first row will have index 0, the second row index 1, and so on. (It does **not** start at 1, and there is always an index even if you don’t explicitly provide one.)

**Question 3:** How can you select a column named `"Age"` from a DataFrame `df`?  
**Options:**  
- A. `df["Age"]`  
- B. `df.select("Age")`  
- C. `df.get_column("Age")`  
- D. `df.column("Age")`  

**Answer:** A. The syntax `df["Age"]` will retrieve the `"Age"` column. In pandas, using the DataFrame with a column name in square brackets is the standard way to select that column. Options B, C, and D are not valid pandas methods for selecting a column.

**Question 4:** True or False: Selecting a single column from a DataFrame returns a pandas Series, not a DataFrame.  

**Options:**  
- True  
- False  

**Answer:** **True.** When you select one column (e.g. `df["Age"]`), the result is a pandas Series (one-dimensional), whereas selecting multiple columns (e.g. `df[["Age", "Name"]]`) gives a DataFrame ([Pandas: Indexing and Slicing | CommonLounge Archive](https://www.commonlounge.com/pandas-indexing-and-slicing-e383c6227e494c5c97081196e0637ea1#:~:text=For%20example%2C%20selecting%20a%20single,2%20and%20all%20the%20rows)). This is why the type of `df["Age"]` would be `<class 'pandas.core.series.Series'>`.

**Question 5:** True or False: `df.loc` uses label-based indexing (index names/labels), whereas `df.iloc` uses positional indexing (integer positions).  

**Options:**  
- True  
- False  

**Answer:** **True.** The `.loc` indexer selects by label (e.g. `df.loc["row_label", "col_label"]`), and `.iloc` selects by zero-based integer position (e.g. `df.iloc[0, 1]`) ([Loc vs Iloc in Pandas: A Guide With Examples | DataCamp](https://www.datacamp.com/tutorial/loc-vs-iloc#:~:text=,to%20use%20both%20with%20examples)). For example, `df.loc[2]` looks for an index labeled 2, while `df.iloc[2]` returns the third row (position 2).

**Question 6:** What will the following code output?  

```python
import pandas as pd
df = pd.DataFrame({'A': [1, 2, 3],
                  'B': ['x', 'y', 'z']},
                  index=['row1', 'row2', 'row3'])
print(df.iloc[1])
```  

**Answer:** It will print the second row of the DataFrame as a pandas Series, because `df.iloc[1]` selects the row at index position 1 (which has the label `"row2"`). The output will be: 

```plaintext
A    2
B    y
Name: row2, dtype: object
``` 

Here, `"A    2"` and `"B    y"` are the values from columns A and B for the second row, and `Name: row2` indicates the original index label of that row.

**Question 7:** If `df` has numeric columns `"A"` and `"B"`, what is the result of executing `df["C"] = df["A"] + df["B"]`?  
**Options:**  
- A. A new column `"C"` is added to `df`, where each value is the sum of the corresponding values in A and B.  
- B. A new row is appended to `df` with label `"C"` containing the sums of A and B.  
- C. An error is raised because `"C"` is not an existing column.  
- D. The statement has no effect on `df`.  

**Answer:** A. This operation creates a new column `"C"` in the DataFrame. Each entry in `"C"` will be the sum of the entries from columns `"A"` and `"B"` in that same row. (No error occurs – pandas will add the new column automatically.)

**Question 8:** Which of the following will drop (remove) the column `"A"` from DataFrame `df`?  
**Options:**  
- A. `df.drop("A", axis=1)`  
- B. `df.drop("A")`  
- C. `df.drop_column("A")`  
- D. `df.remove("A", axis=1)`  

**Answer:** A. The correct method is `df.drop("A", axis=1)`. By specifying `axis=1`, you tell pandas to drop a column (as opposed to a row). Option B, `df.drop("A")`, would try to drop a row labeled "A" by default (axis=0), not a column. Options C and D are not valid pandas DataFrame methods for dropping columns. (Alternatively, one could use `df.drop(columns=["A"])` which is equivalent.)

**Question 9:** How can you sort a DataFrame `df` by the values in column `"Age"` in ascending order?  
**Options:**  
- A. `df.sort_values("Age")`  
- B. `df.sort("Age")`  
- C. `df.order_by("Age")`  
- D. `df.sort_index("Age")`  

**Answer:** A. The method `df.sort_values("Age")` will sort the DataFrame by the `"Age"` column in ascending order by default. Option B (`sort`) is deprecated/not used for value sorting in pandas, C is not a pandas method, and D (`sort_index`) would sort by the DataFrame’s index (row labels), not by a column’s values.

**Question 10:** Which code will filter the DataFrame `df` to include only rows where the column `"Score"` is greater than 50?  
**Options:**  
- A. `df[df["Score"] > 50]`  
- B. `df.filter(df["Score"] > 50)`  
- C. `df["Score"] > 50`  
- D. `df.loc["Score"] > 50`  

**Answer:** A. The syntax `df[df["Score"] > 50]` uses a boolean condition to filter rows – it returns a DataFrame containing only the rows where the condition is True. Option B is incorrect because `DataFrame.filter` is used for filtering by labels (not by boolean condition in that way). Option C only produces a boolean Series (it doesn’t filter the DataFrame), and D is not a valid usage of `.loc` for filtering rows (you would need to pass the condition inside `.loc` as `df.loc[df["Score"] > 50]` or use the shorthand df[...] as in option A).

**Question 11:** What will be the output of the following code?  

```python
df = pd.DataFrame({
    "Team": ["Red", "Blue", "Red", "Blue"],
    "Points": [5, 3, 6, 4]
})
result = df.groupby("Team")["Points"].sum()
print(result)
```  

**Answer:** It will print the total points for each team as a pandas Series indexed by team name. The output will be: 

```plaintext
Team
Blue     7
Red     11
Name: Points, dtype: int64
``` 

Here, **Blue** has 7 (since 3+4) and **Red** has 11 (5+6). We grouped the DataFrame by the `"Team"` column and summed the `"Points"` for each group, following the *split-apply-combine* strategy of groupby (split data by team, apply sum, combine results) ([Group by: split-apply-combine — pandas 2.2.3 documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#:~:text=By%20%E2%80%9Cgroup%20by%E2%80%9D%20we%20are,more%20of%20the%20following%20steps)).

**Question 12:** Which method can you use to replace all *missing values* (NaNs) in DataFrame `df` with 0?  
**Options:**  
- A. `df.fillna(0)`  
- B. `df.remove_na(0)`  
- C. `df.dropna(0)`  
- D. `df.fillmissing(0)`  

**Answer:** A. `df.fillna(0)` will return a new DataFrame (or you can do it in-place) where all NA/NaN entries are filled with 0 ([Working with missing data — pandas 2.2.3 documentation](https://pandas.pydata.org/docs/user_guide/missing_data.html#:~:text=fillna,NA%20data)). Option B (`remove_na`) is not a pandas function. Option C, `dropna()`, would **drop** rows/columns with missing values rather than filling them (and it doesn’t take a fill value argument). Option D is not a valid pandas function.

**Question 13:** True or False: You can generate a simple plot of a DataFrame’s data using the DataFrame’s built-in `.plot()` method.  

**Options:**  
- True  
- False  

**Answer:** **True.** Pandas DataFrames have an integrated `.plot()` method that leverages Matplotlib to create charts. For example, `df.plot()` will by default produce a line plot for all numeric columns, making basic visualization quick and easy (one can customize the kind of plot via arguments).

**Question 14:** How do you read a CSV file named `"data.csv"` into a pandas DataFrame?  
**Options:**  
- A. `pd.read_csv("data.csv")`  
- B. `pd.load_csv("data.csv")`  
- C. `pd.import_csv("data.csv")`  
- D. `df.read_csv("data.csv")`  

**Answer:** A. The correct function is `pd.read_csv("data.csv")`. This is a top-level pandas **reader** function that returns a DataFrame by reading the CSV file ([IO tools (text, CSV, HDF5, …) — pandas 2.2.3 documentation](https://pandas.pydata.org/docs/user_guide/io.html#:~:text=IO%20tools%20,%E2%80%A6)). (Options B and C are made-up functions; Option D is incorrect because `read_csv` is a function in the pandas module, not a method on an existing DataFrame.)

**Question 15:** Which of the following will save the DataFrame `df` to an Excel file named `"output.xlsx"`?  
**Options:**  
- A. `df.to_excel("output.xlsx")`  
- B. `pd.to_excel(df, "output.xlsx")`  
- C. `df.to_csv("output.xlsx")`  
- D. `df.save_excel("output.xlsx")`  

**Answer:** A. The DataFrame method `df.to_excel("output.xlsx")` writes the DataFrame to an Excel file. Pandas provides such writer methods (like `to_excel`) as instance methods of DataFrame ([IO tools (text, CSV, HDF5, …) — pandas 2.2.3 documentation](https://pandas.pydata.org/docs/user_guide/io.html#:~:text=Binary%20Excel%20%28,cookbook%20for%20some%20advanced%20strategies)). Option B is not the correct usage (there is no pandas-level `to_excel` function that takes a DataFrame as an argument; instead, the method is called on the DataFrame). Option C would save a CSV-format file with an `.xlsx` extension (not actually an Excel binary file). Option D is not a real method. (Similarly, to save as CSV, one would use `df.to_csv("output.csv")`.)