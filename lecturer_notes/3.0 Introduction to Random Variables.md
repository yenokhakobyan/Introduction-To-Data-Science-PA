# Random Variables and Exploratory Data Analysis (EDA)

## Learning Objectives

* **Define random variables:** Explain what a random variable is and why it models data uncertainty.
* **Discrete vs Continuous:** Distinguish between discrete (countable) and continuous (interval) random variables, with examples.
* **Random variables in data:** See how data columns or features (in tables, text, images) arise from random variables.
* **EDA connections:** Relate random variables to descriptive statistics, plots, and distributional assumptions.
* **Sample vs Population:** Understand that observed data are a *sample* drawn from a theoretical population distribution; distinguish observed variables from latent (hidden) variables.
* **Practical implications:** Learn how variable types influence summary statistics, visualizations, and modeling choices.

## What Is a Random Variable?

A *random variable* is a function that assigns a numeric value to each outcome of a random process.  In practice, think of it as a quantity whose value depends on chance.  For example, the roll of a six-sided die, the height of a randomly chosen person, or tomorrow’s stock price can all be viewed as random variables: their values are not fixed but vary according to an underlying probability model.  Equivalently, a random variable’s value is unknown ahead of time and is determined by the outcome of some experiment or sampling.  Mathematically, if Ω is the set of all possible outcomes, a real-valued random variable X is a function \$X:Ω→\mathbb{R}\$ assigning a number to each outcome.

Because the values arise from randomness, we cannot know a random variable’s exact value in advance; instead we study its probability distribution.  For example, if X is the sum of two dice, X is *random* because it could be 2, 3, …, 12 with different probabilities.  Random variables form the basis of statistical modeling: we assume data are generated by some random process, and use observed data to infer properties of that process.

## Discrete vs Continuous Random Variables

Random variables are classified into **discrete** and **continuous** types based on their range of possible values.  A *discrete random variable* takes on a finite or countable set of distinct values.  For instance, the number of heads in 10 coin flips (0–10) or the count of emails arriving in an hour (0,1,2,…) are discrete random variables.  More formally, a random variable is **discrete** if its range is a countable set.  In contrast, a *continuous random variable* can assume any value in an interval (often real-valued).  Examples include a person’s exact height, the temperature tomorrow, or the weight of a random object.  In a continuous case, the variable’s range is uncountably infinite (e.g. an interval of \$\mathbb{R}\$).  Discrete variables use *probability mass functions* (PMFs) to give probabilities for each value, while continuous variables use *probability density functions* (PDFs).

> *Example:* “Number of students in a class” is discrete (takes integer values), whereas “time to complete a task” is continuous (any positive real value).

The distinction is essential in practice.  As noted by Potts et al., “numerical variables can be discrete (e.g., household size) or continuous (e.g., age or weight)”.  Common distributions reflect this: continuous variables often follow families like Normal or Exponential, while discrete counts follow Binomial or Poisson (for example).  In general, **“distribution types correspond to data types”**: continuous variables → continuous distributions; discrete variables → discrete distributions.

## Random Variables in Real-World Data

In any dataset, each feature or column can be viewed as a realization of a random variable.  The *type* of each variable (discrete vs continuous) determines how we summarize and analyze it.  We illustrate with examples in tabular, text, and image data.

### Tabular Data (e.g. Titanic, Iris)

In structured (tabular) data, each column is a random variable measured on each row (observation).  For example, in the **Titanic** dataset (passenger records) we have variables such as *Age* (continuous numeric), *Fare* (continuous numeric), *Pclass* (discrete categorical: 1,2,3), *Sex* (discrete categorical: male/female), and *Survived* (binary: 0/1).  As a rule, numeric columns (integers or floats) are treated as quantitative random variables, and text/categorical columns as discrete random variables.  Recognizing this guides which summaries are appropriate: e.g. for a discrete variable we compute frequency counts, whereas for a continuous variable we compute means or variances.

> **Data types (Tabular):** The main data types are numerical (quantitative) and categorical.  Numerical variables can be discrete (e.g. counts like “household size”) or continuous (e.g. age in years).  Categorical variables (which take values in a finite set) are discrete random variables by nature.

For illustration, consider the classic **Iris** dataset (150 flowers, 3 species).  The four measurements (sepal length, sepal width, petal length, petal width) are continuous variables, while the *species* (Setosa, Versicolor, Virginica) is a categorical variable (discrete).  Below we use Python (pandas, seaborn) to load the data, then plot it to visualize variable types and relationships:

```python
from sklearn.datasets import load_iris
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load Iris dataset into a DataFrame
iris = load_iris()
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df["species"] = pd.Categorical.from_codes(iris.target, iris.target_names)
iris_df.head(3)
```

```
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm) species
0                5.1               3.5                1.4               0.2   setosa
1                4.9               3.0                1.4               0.2   setosa
2                4.7               3.2                1.3               0.2   setosa
```

All four numeric columns above are continuous random variables.  We can plot two of them and color by species (a discrete variable) to see how they relate:

```python
plt.figure(figsize=(5,4))
sns.scatterplot(data=iris_df, x="sepal length (cm)", y="sepal width (cm)", hue="species", palette="deep")
plt.title("Iris Dataset: Sepal Length vs. Width (by Species)")
plt.show()
```

The scatter plot shows each point’s sepal measurements, colored by species.  We see that *species* (a discrete/categorical variable) tends to separate the points into groups, while the axes (sepal length/width) are continuous measurements.  Similarly, we can examine the distribution of a continuous variable across categories. For example, petal length by species:

```python
plt.figure(figsize=(5,4))
sns.histplot(data=iris_df, x="petal length (cm)", hue="species", element="step", stat="density")
plt.title("Iris: Petal Length Distribution by Species")
plt.xlabel("Petal length (cm)")
plt.ylabel("Density")
plt.show()
```

This histogram (overlaid by species) emphasizes that the continuous petal length variable has different distributions for each categorical species.

*Key idea:* Each column in a table is a random variable.  Numeric columns lead to histograms, means, etc.; categorical columns lead to count tables and bar plots.  We choose summaries and plots appropriate to each variable’s type.

### Text Data (e.g. word counts, sentiment)

Text data is unstructured, but we often convert it into numerical features, which are random variables.  For instance, the **word count** of a document is a discrete random variable (takes integer values).  If we compute the *sentiment polarity* (e.g. using TextBlob or a lexicon) on a sentence, we get a continuous score (usually in \[-1,1]).  Here’s a small example using Python to illustrate:

```python
from textblob import TextBlob
texts = [
    "I love data science", 
    "This is an amazing day", 
    "I hate rainy days", 
    "Data analysis is neither easy nor fun", 
    "Happy learning!"
]
df_text = pd.DataFrame({"text": texts})
df_text["word_count"] = df_text["text"].apply(lambda s: len(s.split()))
df_text["sentiment"] = df_text["text"].apply(lambda s: TextBlob(s).sentiment.polarity)
print(df_text)
```

```
                                text  word_count  sentiment
0             I love data science           4   0.500000
1      This is an amazing day           5   0.600000
2           I hate rainy days           4  -0.800000
3  Data analysis is neither easy nor fun           7   0.366667
4                    Happy learning!           2   1.000000
```

Each row is an “observation” (a sentence).  The *word\_count* column is a discrete random variable (taking values 2,4,5,7 above).  The *sentiment* column is continuous (with fractional values between -1 and 1).  We can plot these to see their relationship:

```python
plt.figure(figsize=(4,3))
plt.scatter(df_text["word_count"], df_text["sentiment"], color='purple', s=50, marker='x')
plt.title("Text Data Example: Word Count vs Sentiment")
plt.xlabel("Word count (discrete)")
plt.ylabel("Sentiment polarity (continuous)")
plt.grid(True)
plt.show()
```

The scatter plot above shows *word count* (x-axis, discrete values) versus *sentiment* (y-axis, continuous).  Each point is a sentence.  This illustrates a mix of variable types: one discrete, one continuous.

In general, textual features often include discrete counts (e.g. word frequencies, length) and continuous scores (e.g. tf-idf values, sentiment scores).  Recognizing the type guides our summaries (we may report mean/variance of the continuous scores, and frequency counts for the discrete ones).

### Image Data (e.g. pixel intensities, labels)

Image data is typically represented as matrices of pixel values.  Each pixel intensity is a numeric random variable (often discrete 0–255 for grayscale or 3-channel 0–255 values for RGB).  For example, in the 8×8 “digits” dataset (from scikit-learn), each image has 64 pixels with integer intensities 0–16.  The *label* of the image (e.g. digit “0” or “7”) is a discrete random variable (categorical from {0,…,9}).

As an example, load the digit dataset and display one image and its pixel-intensity histogram:

```python
from sklearn.datasets import load_digits
digits = load_digits()
img0 = digits.images[0]   # an 8x8 array for the first image
label0 = digits.target[0]
print(f"Label of first digit: {label0}")
plt.figure(figsize=(3,3))
plt.imshow(img0, cmap='gray')
plt.title(f"Digit {label0} (8×8 image)")
plt.axis('off')
plt.show()

# Histogram of pixel intensities
plt.figure(figsize=(4,3))
plt.hist(img0.ravel(), bins=range(18), color='orange', edgecolor='black')
plt.title("Histogram of pixel intensities (digit 0)")
plt.xlabel("Pixel intensity (0–16)")
plt.ylabel("Frequency")
plt.show()
```

The image display (not shown here) would show a small gray-scale “0”.  The histogram of pixel intensities shows that most pixels are 0 (black), and some have higher values.  This histogram is a probability histogram of a **continuous** (or actually ordinal) variable (pixel brightness), and the height of each bar is the count (frequency) of pixels with that intensity.  The key point: *each pixel is a numeric random variable*.  Meanwhile, the **class label** (digit 0–9) is a discrete variable that we might analyze with a different approach (e.g. classification count by class).

**Summary:** In image data, raw pixels or features extracted from images are continuous/discrete numeric variables, while labels or categories of images are discrete random variables.  We use histograms, scatterplots of pixel values, and so on to explore image data.

## Sample vs Population (Observed vs Latent)

In statistics, a **population** refers to the entire group of interest, and a **sample** is a subset of that population.  A sample is what we actually observe; it provides data for our random variables.  For example, a population might be *all adult humans in a country*, and a sample could be the 200 people we survey.  Importantly, we think of our data as drawn from an underlying random-variable model of the population.  In other words, each column’s distribution in the sample approximates the unknown *population distribution* of that random variable.  As noted by Seltman (CMU Stat), observed measurements represent a *“sample distribution”* of the variable, which in turn represents the underlying *“population distribution”*.  This perspective is why we say EDA lets us make tentative conclusions about the population distribution(s) compatible with the sample.

Another perspective is **observed vs latent variables**.  Most features in a dataset are *observed* random variables.  In some models, however, we introduce *latent* (hidden) random variables that cannot be directly measured.  Latent variables are inferred indirectly via observed data.  For example, in topic modeling on text, the assignment of topics to documents is latent; in factor analysis, underlying factors are latent.  Formally, latent variables “can only be inferred indirectly through a mathematical model from other observable variables”.  In EDA we usually focus on observed variables, but it’s good to remember that they may arise from latent processes.

## EDA: Descriptive Statistics and Visualizations

Exploratory Data Analysis (EDA) is the process of summarizing and visualizing variables to understand their properties.  Since data columns are realizations of random variables, EDA techniques depend on the variable type:

* **Numeric variables (continuous or discrete):** Common summary statistics include mean, median, variance, standard deviation.  Visualization tools include histograms, density plots, box plots (for univariate distribution) and scatterplots or correlation plots (for relationships between two numeric variables).  For example, for a continuous variable we might compute its mean and plot a histogram to see its distribution shape.  If the variable is discrete (like counts), we might still use a histogram or bar chart of frequencies.
* **Categorical variables:** We summarize by frequency or proportion in each category.  Visualizations include bar charts or pie charts.  For instance, if *Pclass* in Titanic takes values {1,2,3}, we would count how many passengers are in each class and plot a bar chart of class frequencies.

Additionally, EDA often involves checking distributional assumptions before modeling.  Many statistical methods assume certain distributions (e.g. linear regression assumes the residuals are normally distributed).  We frequently check if a continuous variable appears *approximately* Normal (e.g. with a histogram or Q–Q plot).  If a variable is count-based, we might consider if a Poisson or binomial model is reasonable.  As Potts et al. note, *“distribution types correspond to data types”*: continuous random variables often follow Normal/Exponential families, while discrete ones follow Binomial/Poisson families.  By examining histograms, boxplots, and scatterplots during EDA, we gain intuition about which models or assumptions might be appropriate.

In summary, EDA ties directly to random variables: we compute descriptive statistics and draw graphs that **characterize each variable’s (and pairs of variables’) distribution**.  For each numeric variable X, we estimate its sample mean, variance, and plot its sample distribution.  For each pair of numeric variables (X,Y), we might plot them on a scatter to explore dependence.  For categorical variables, we tabulate counts.  Through EDA, we treat the observed data as a sample from the random variables’ distributions, helping us decide on summary measures and any needed transformations.

## Practical Implications

Recognizing variable types has real impacts on analysis and modeling:

* **Summarization:** For continuous variables we use arithmetic mean/standard deviation; for discrete/ordinal data we use medians or mode as needed. Categorical variables are summarized by counts or proportions.  For example, it makes little sense to compute the mean of a nominal category, but we would compute the proportion in each category.
* **Visualization:** The plots we choose depend on type.  Continuous data → histograms, boxplots.  Categorical data → bar charts.  Bivariate plots also depend on types (e.g. boxplots for a numeric by category, scatterplots for numeric vs numeric).
* **Modeling assumptions:** Parametric models assume certain distributions.  If a variable is continuous and roughly bell-shaped, a Normal-based model might fit; for counts or binary outcomes, we use Poisson or logistic models respectively.  Using the wrong assumption can lead to poor fits (e.g. linear regression on a binary target is inappropriate).
* **Data preparation:** Discrete categorical variables often need encoding (dummy variables) before modeling; continuous variables might be normalized or binned depending on context.  Outlier detection and missing-value treatment can also differ by type.
* **EDA steps:** Early in data analysis, we always check variable types.  Pandas’ `.describe()` handles numeric vs object differently.  We might drop or group rare categories for discrete variables, or consider discretizing continuous variables if needed for certain models.

Understanding random variables underpins all these steps: knowing that *X* is discrete vs continuous tells us which statistical summaries and plots make sense, and alerts us to choose appropriate statistical models.

## Takeaway Points

* A **random variable** models the outcome of a random process by assigning a numeric value to each outcome. Its *distribution* describes how often different values occur.
* **Discrete vs. continuous:** Discrete random variables have countable values (e.g. number of events), while continuous ones vary over intervals (e.g. measurements).  The type determines how we compute probabilities (PMF vs PDF).
* In data, **each column is a random variable**.  Numeric columns (age, weight, etc.) are treated as continuous or integer random variables; categorical columns (gender, class) are discrete.  We summarize and plot them accordingly.
* **Sample vs population:** Our data are a sample from a larger population distribution.  We use the sample distribution (empirical data) to infer the population’s random-variable distribution.  Observed variables may have underlying latent factors, but EDA focuses on the observed.
* **EDA links to probability:** Descriptive statistics (mean, variance, etc.) and graphics (histogram, bar chart, scatter) are all ways to characterize random variables’ distributions.  We also check if data follow common distributions (normal, Poisson, etc.) depending on variable type.
* **Practical impact:** Correctly identifying variable types ensures we use the right summaries, plots, and models.  For example, we don’t compute a mean of a purely categorical label, and we apply logistic regression for binary outcomes, not linear regression.

By grounding data columns in the concept of random variables, we make our EDA and subsequent modeling statistically sound. This perspective helps clarify why we choose particular summaries and plots, and what assumptions are reasonable for each feature in our data.


# Probability Distributions and EDA

**Learning Objectives:** After this lesson, students will be able to:

* **Define** what a probability distribution is and why it matters in modeling.
* **Differentiate** between discrete and continuous distributions.
* **Describe** common discrete distributions (Bernoulli, Binomial, Poisson) and **cite examples** (e.g. coin flips, event counts).
* **Describe** common continuous distributions (Normal, Exponential, Uniform) with real-world contexts (e.g. heights, waiting times, uniform randomness).
* **Estimate** a variable’s distribution from data using EDA plots.
* **Use** visual diagnostics (histogram, kernel density, ECDF, Q–Q plot) to assess distribution shape.
* **Relate** distributional assumptions to modeling tasks (e.g. normal residuals in regression, Poisson for count data).
* **Apply** transformations (log, Box–Cox) to correct skew and meet assumptions.
* **Demonstrate** examples in Python (with `scipy.stats`, `seaborn`, `statsmodels`, `matplotlib`) on tabular, text, and image data.

## Probability Distributions: Definition and Motivation

A **probability distribution** is a mathematical function that describes the probabilities of different possible values of a random variable.  Equivalently, it’s an *idealized* version of a frequency distribution: it tells us how likely each outcome is in a random process (e.g. a coin flip, die roll, measurement).  For example, a fair coin toss has a probability distribution giving 0.5 for heads and 0.5 for tails.  Distributions are fundamental because they encode our assumptions about how data are generated.  When we model data or perform statistical tests, we often assume the data follow a particular distribution (or approximately do so).  This allows us to compute probabilities, confidence intervals, and predictive inferences.

Probability distributions come in two main types:

* **Discrete distributions** apply when a variable can take on only specific, separate values (often integers or categories).  They are described by a *probability mass function* (PMF) assigning each possible value a probability.  For example, the number of heads in 10 coin flips or the number of phone calls in an hour are discrete.  By definition, a discrete distribution only assigns nonzero probability to the *possible* outcomes of the variable.  For instance, a die-roll distribution would never give probability to “2.5” since that outcome is impossible.  The probabilities of all allowed values sum to 1.

* **Continuous distributions** apply when a variable can take on any value in a continuum (often real numbers).  They are described by a *probability density function* (PDF).  For a continuous random variable, the probability of any *exact* value is effectively zero; instead, probability is defined over intervals.  In other words, a continuous variable can assume any value in its range, and only the probability that it falls into a range (e.g. between a and b) is positive.  Common examples include heights of people, weights of objects, or waiting times for an event.

In short, **discrete vs. continuous** differ in how we treat probability: discrete variables have a PMF over individual values, whereas continuous variables have a PDF over ranges.  We will now look at specific, widely used distributions in each category.

## Common Discrete Distributions

### Bernoulli Distribution

A *Bernoulli* random variable takes only two values (0 or 1), often called “failure” or “success.”  It models a single binary trial, like flipping a (possibly biased) coin, answering yes/no, or whether a customer buys a product.  The Bernoulli distribution is characterized by one parameter $p$, the probability of success (1).  Formally, a random variable $X\sim\mathrm{Bernoulli}(p)$ has

$$
P(X=1) = p,\quad P(X=0)=1-p.
$$

It is the simplest discrete distribution. For example, if a coin has probability 0.7 of coming up heads, then flipping it once yields a Bernoulli(0.7) variable: 70% chance of 1 (heads) and 30% chance of 0 (tails).  The mean of a Bernoulli is $p$ and the variance is $p(1-p)$.  Bernoulli trials underlie many binary decision scenarios in data science (e.g., click/no-click, defect/no-defect).

```python
import numpy as np
from scipy.stats import bernoulli
p = 0.3
data = bernoulli.rvs(p, size=1000)  # 0/1 samples
```

### Binomial Distribution

The *Binomial* distribution counts the number of successes in a fixed number of independent Bernoulli trials.  If $X\sim \mathrm{Binomial}(n,p)$, then $X$ is the number of successes in $n$ trials with success probability $p$ each.  Its PMF is

$$
P(X=k) = \binom{n}{k} p^k (1-p)^{n-k},\quad k=0,1,\dots,n.
$$

In probability theory, the binomial distribution is defined as the discrete distribution of “the number of successes in a sequence of $n$ independent yes–no experiments” each with success probability $p$.  For example, the number of heads in 10 fair coin flips is Binomial($n=10$, $p=0.5$).  Common real-world examples include the number of defective items in a batch of $n$, or the count of people who clicked on an ad out of $n$ views.  The mean is $np$ and variance $np(1-p)$.

```python
from scipy.stats import binom
n, p = 10, 0.4
counts = binom.rvs(n, p, size=1000)  # number of successes in 10 trials
```

### Poisson Distribution

The *Poisson* distribution models counts of events occurring independently in a fixed interval of time or space, given a constant average rate $\lambda$.  It is a discrete distribution on $ \{0,1,2,\dots\}$.  Formally, $X\sim \mathrm{Poisson}(\lambda)$ has

$$
P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!},\quad k=0,1,2,\dots
$$

.  It describes the number of occurrences of random events in a fixed period when events happen independently.  For example, the number of phone calls a call center receives in an hour, or the number of typos on a page, are often modeled as Poisson.  As one source notes, “the Poisson distribution is most commonly used to model the number of random occurrences of some phenomenon in a specified unit of space or time” (e.g., calls per minute, flaws per meter, accidents per day).  The Poisson mean and variance are both $\lambda$.

```python
from scipy.stats import poisson
lam = 3.2
counts = poisson.rvs(lam, size=1000)  # counts per interval
```

## Common Continuous Distributions

&#x20;*Figure: Example normal (Gaussian) probability density functions. The red curve is the standard normal (mean=0, std=1); other curves vary mean or variance.*
A **normal (Gaussian) distribution** is a continuous bell-shaped distribution for real-valued variables.  It is defined by two parameters $\mu$ (mean) and $\sigma^2$ (variance), with PDF

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
$$

A normal distribution is symmetric around its mean and is often called a *bell curve*.  Many measurements (heights, exam scores, measurement errors) are approximately normal because of the Central Limit Theorem: averages of many independent factors tend to be normal.  In statistics, normality is often assumed because it simplifies analysis (e.g. least squares theory).

```python
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
data = np.random.normal(loc=0, scale=1, size=1000)
sns.histplot(data, kde=True)
plt.title("Histogram of normal data")
```

The **exponential distribution** is a continuous distribution modeling waiting times between independent Poisson events.  If $X\sim\mathrm{Exp}(\lambda)$, its PDF is $f(x)=\lambda e^{-\lambda x}$ for $x\ge0$.  Equivalently, it is the *distance between events in a Poisson process with rate $\lambda$*.  For example, if customers arrive on average 2 per hour, the time until the next customer is Exp(λ=2).  The memoryless property (future independent of past) is unique to the exponential among continuous distributions. The mean of Exp($\lambda$) is $1/\lambda$, so higher event rates imply shorter expected waiting times.

The **uniform distribution** on an interval $[a,b]$ assigns equal probability to any value in that range.  By definition, a continuous uniform distribution has constant PDF $f(x)=1/(b-a)$ for $x\in[a,b]$.  It models a random outcome with no bias over an interval.  For example, selecting a random time uniformly between 1pm and 2pm (no peak times), or a random number between 0 and 1, are uniform.  Every sub-interval of equal length in $[a,b]$ is equally likely.

```python
from scipy.stats import norm, expon, uniform
# Example PDFs evaluated at some points
x = np.linspace(-5,5,100)
print("Normal PDF at 0:", norm.pdf(0))
print("Exp PDF at 0:", expon.pdf(0, scale=1/2))
print("Uniform PDF on [0,1] at 0.5:", uniform.pdf(0.5, loc=0, scale=1))
```

## Estimating Distributions with EDA

In practice, we rarely *know* the true distribution of data.  Instead we **explore** the data to guess its distributional shape.  Basic EDA tools include histograms, density plots, empirical CDFs, and Q–Q plots.

* **Histogram:** A histogram partitions data into bins and shows frequencies or densities.  It is a nonparametric estimate of the distribution.  For example, plotting a histogram of `income` or `age` reveals skew or symmetry.
* **Kernel Density Estimate (Density Plot):** A smoothed version of the histogram, often drawn as a continuous curve.  Seaborn’s `kdeplot` or `histplot(kde=True)` is commonly used.
* **Empirical CDF (ECDF):** This plot shows the fraction of data ≤ $x$ at each $x$.  It is a step function (jumps of $1/n$) defined as $\hat F_n(x)=\frac{1}{n}\#\{x_i\le x\}$.  Unlike a histogram, the ECDF uses every data point and converges to the true CDF as $n\to\infty$.
* **Q–Q Plot:** A quantile–quantile plot compares the quantiles of the data to those of a theoretical distribution (e.g. normal).  If the data come from that distribution, the points should lie roughly on the line $y=x$.  Q–Q plots are powerful diagnostics: for example, a straight line in a normal Q–Q plot indicates normality, whereas systematic deviations (curvature, S-shape) indicate skew or heavy tails.

```python
import statsmodels.api as sm
data = np.random.exponential(scale=1.0, size=100)
sm.qqplot(data, line='45'); plt.title("Q-Q plot vs Exponential")
plt.show()
```

In summary, these visual tools allow us to *assess* distribution shape.  They suggest candidate distributions (e.g. data look roughly bell-shaped, or right-skewed like an exponential).  They also help identify outliers or multimodality.

## Distributional Assumptions in Modeling

Statistical models often **assume** specific data distributions: e.g. linear regression assumes the residuals are roughly normal, and generalized linear models (GLMs) assume distributions for the response (binomial for binary, Poisson for counts, etc.).  Checking assumptions is crucial:

* **Normal residuals in regression:** Many parametric tests and linear models assume that errors (residuals) are normally distributed.  A normal Q–Q plot of residuals can reveal if this assumption holds.  For example, if residuals vs normal quantiles lie on a straight line, the assumption is plausible; if not, the model may be mis-specified.  As UVA’s StatLab notes, a Q–Q plot is exactly a way “to help us assess if a set of data plausibly came from some theoretical distribution such as a normal or exponential”.

* **Poisson for count data:** When modeling counts of rare events (e.g. calls, accidents), a Poisson regression is often used under the assumption that counts follow a Poisson distribution.  Recall that for a Poisson process with known average rate, the number of events in each interval is Poisson.  Thus if we have count data (e.g. word occurrences in documents, or number of defect items), we should consider Poisson (or related) models rather than, say, normal models.  Generalized linear models let us specify these distributions directly.

If the EDA reveals that the assumed distribution is badly violated (e.g. heavy skew, outliers), we either choose a different model or *transform* the data.

## Transformations for Skewed Data

When a variable is highly skewed or has a heavy tail, analysts often transform it to better meet modeling assumptions (especially normality).  Common transformations include taking the log, square-root, or using a Box–Cox power transform.  For example, taking $\log(X)$ can turn a right-skewed distribution (like income) into something more symmetric.

* **Log transform:** If $X>0$ and is log-normal-like, then $Y=\log(X)$ may be closer to normal.  This is common for variables like income or population, which span orders of magnitude.
* **Box–Cox transformation:**  This is a parametric family of power transforms that seeks an exponent $\lambda$ making the data as normal as possible.  As one source explains, “A Box–Cox transformation is a transformation of non-normal dependent variables into a normal shape.  Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying Box–Cox means that you are able to run a broader number of tests”.  (The special case $\lambda=0$ corresponds to the log transform.)  In practice, statistical software can estimate the optimal $\lambda$ that best approximates normality.

```python
from scipy import stats
y = np.random.chisquare(df=4, size=100)  # right-skewed data
y_pos = np.where(y<=0, np.min(y[y>0])/2, y)  # ensure positivity
y_bc, lam = stats.boxcox(y_pos)  # optimal λ ~ 0 for this example
print("Estimated lambda:", lam)
```

Applying such transformations can stabilize variance and make residuals more normal, improving model validity.  Always back-transform predictions if needed to interpret them in the original scale.

## Examples with Python

Below are illustrative examples using Python libraries (`numpy`, `scipy.stats`, `seaborn`, `statsmodels`, `matplotlib`) for different data types.

* **Tabular Data (e.g. income, age, counts):** Suppose *income* is log-normally distributed.  One could simulate `income = np.random.lognormal(mean=10, sigma=0.5, size=1000)` and then plot its histogram.  The histogram will be right-skewed.  Taking `np.log(income)` will produce an (approximate) normal distribution.  You can overlay a normal PDF to check fit.  For *age*, if the population is roughly uniform or symmetric around middle age, a histogram might look flat or bell-shaped.  For *counts*, simulate e.g. `events = np.random.poisson(lam=5, size=500)` and plot.  The count histogram should resemble a Poisson shape (peaked near 5, with a long tail).

```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Simulate skewed income and transform
income = np.random.lognormal(mean=10, sigma=1.0, size=1000)
sns.histplot(income, bins=30, kde=True)
plt.title("Histogram of simulated income"); plt.xlabel("Income")
plt.show()

sns.histplot(np.log(income), bins=30, kde=True)
plt.title("Histogram of log-income (should look more normal)"); plt.xlabel("Log(Income)")
plt.show()
```

* **Text Data:** If we count word occurrences in documents, those counts are non-negative integers.  Often a Poisson or negative-binomial model is reasonable.  E.g. simulate `word_counts = np.random.poisson(lam=3, size=200)` to get counts per document.  A histogram or bar plot will show the discrete distribution.  For continuous text features (like sentiment scores normalized to $[-1,1]$), we might assume a normal or logistic distribution.  For instance, `sentiments = np.random.normal(loc=0, scale=0.2, size=300)` and plot a density; it should be symmetric about 0.

```python
# Poisson word counts
word_counts = np.random.poisson(2.5, size=300)
sns.histplot(word_counts, bins=range(0,10), kde=False)
plt.title("Histogram of simulated word counts"); plt.xlabel("Count")
plt.show()

# Normally distributed sentiment scores
sentiment = np.random.normal(0, 0.5, size=300)
sns.kdeplot(sentiment); plt.title("Density of sentiment scores"); plt.axvline(0, color='grey'); plt.show()
```

* **Image Data:** For a grayscale image, pixel intensities range (e.g. 0–255).  The distribution of pixel values often reveals features of the image (e.g. whether it’s dark or bright).  For example, consider the grayscale flower image below.  Its pixel values are not uniform; many are mid-range.  Plotting a histogram of the intensities shows two modes (reflecting the dark background and bright petals).  Such histograms are used in image processing (e.g. to adjust contrast).

  &#x20;*Figure: Example grayscale image of a flower (each pixel 0–255 intensity). The histogram below shows the frequency of each intensity value.*

  &#x20;*Figure: Histogram of the pixel intensities from the above image. The distribution is bimodal, indicating dark (left peak) vs. bright (right peak) regions.*

  ```python
  from skimage import data
  image = data.camera()  # example grayscale image
  plt.hist(image.ravel(), bins=256, color='blue', alpha=0.7)
  plt.title("Pixel intensity histogram"); plt.xlabel("Intensity"); plt.ylabel("Count")
  plt.show()
  ```

## Takeaway Points

* A **probability distribution** assigns probabilities to values of a random variable.  Distributions are essential for understanding and modeling uncertainty.
* **Discrete vs. continuous:** Discrete distributions (e.g. Bernoulli, Binomial, Poisson) assign positive probability only to specific values, whereas continuous distributions (e.g. Normal, Exponential, Uniform) describe probabilities over intervals.
* **Common distributions:**

  * *Bernoulli(p)* models a single yes/no trial.
  * *Binomial(n,p)* counts successes in $n$ trials.
  * *Poisson(λ)* models counts of random events in a fixed time/space.
  * *Normal(μ,σ²)* is the familiar bell curve for real-valued data.
  * *Exponential(λ)* models waiting times between Poisson events.
  * *Uniform(a,b)* assigns equal probability on $[a,b]$.
* **EDA for distributions:** Use **histograms** and **density plots** to visualize shape; **ECDFs** to see data percentiles; **Q–Q plots** to compare to a theoretical distribution.  These plots suggest which distribution fits or if data are skewed/multimodal.
* **Modeling assumptions:** Many models assume specific distributions (e.g. normality of residuals, Poisson counts).  Always check these assumptions visually or with tests.  A Q–Q plot of residuals is a quick check of normality.  If assumptions fail, consider alternatives or **transformations**.
* **Transformations:** For skewed data, a **log** or **Box–Cox** transform can make the data more normal-like.  This aids regression and ANOVA assumptions.  Always interpret results on the original scale if using transformed data.
* **Python tools:** Libraries like `scipy.stats`, `seaborn`, and `statsmodels` make it easy to simulate distributions, fit models, and create diagnostic plots.  Practice by generating data from known distributions and comparing to theory.

These concepts bridge probability theory and practical data analysis. Understanding distributions and how to assess them through EDA is critical for building valid statistical models and correctly interpreting data.
