

# Comprehensive NumPy Lecture Notes (Beginner to Intermediate)

## Introduction to NumPy

NumPy (Numerical Python) is a fundamental **open-source Python library for numerical computing**, widely used in scientific computing, data analysis, and engineering ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=NumPy%20,Learn)) ([NumPy Tutorial – Python Library | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-tutorial/#:~:text=NumPy%20is%20a%20powerful%20library,advanced%20divided%20in%2010%20sections)). It introduces an efficient N-dimensional array object (`ndarray`) that can store large **homogeneous** datasets (all elements of the same type) and provides a huge collection of mathematical functions to operate on these arrays quickly ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=NumPy%20,Learn)). In contrast to Python lists, which can hold mixed types and are optimized for general use, NumPy arrays are **optimized for numerical operations on large volumes of data**. This means NumPy operations can often **use less memory and run much faster** than equivalent Python code using lists ([NumPy Tutorial – Python Library | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-tutorial/#:~:text=NumPy%20is%20a%20powerful%20library,advanced%20divided%20in%2010%20sections)). NumPy shines especially when working with large quantities of data of the same type (e.g. large arrays of numbers) by exploiting contiguous memory layout and vectorized C/Fortran operations ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=Depending%20on%20the%20characteristics%20of,be%20processed%20on%20the%20CPU)) ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=,the%20same%20type%20of%20data)). 

**Why use NumPy?** Python’s built-in lists are flexible and great for general purposes, but NumPy arrays provide **significant performance and memory advantages** for numerical data. NumPy operations are executed in compiled C code under the hood and **avoid the overhead of Python loops**, enabling vectorized operations that are **orders of magnitude faster** than pure Python loops for large data ([1.4.2. Numerical operations on arrays — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/operations.html#broadcasting#:~:text=These%20operations%20are%20of%20course,did%20them%20in%20pure%20python)) ([1.4.2. Numerical operations on arrays — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/operations.html#broadcasting#:~:text=)). For example, adding 1 to each element of an array of 10,000 numbers with NumPy is dozens of times faster than using a Python list comprehension ([1.4.2. Numerical operations on arrays — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/operations.html#broadcasting#:~:text=)). Moreover, NumPy’s restrictions (fixed size, homogeneous dtype) let it store data more compactly in memory and avoid copying data when possible ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=Depending%20on%20the%20characteristics%20of,be%20processed%20on%20the%20CPU)) ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=,the%20same%20type%20of%20data)). In summary, NumPy provides a **memory-efficient container for multi-dimensional data** and high-level functions to operate on it, making tasks in data science and machine learning **concise and fast**.

**Getting started:** To use NumPy, first install it (e.g. via `pip install numpy`) and import it in your Python script or notebook. By convention, NumPy is imported under the alias `np` for brevity: 

```python
import numpy as np
``` 

This allows you to access NumPy functions with the `np.` prefix ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=After%20installing%20NumPy%2C%20it%20may,imported%20into%20Python%20code%20like)). Once imported, you can create NumPy arrays and perform various operations as discussed below.

## Basic NumPy Concepts and Operations

NumPy’s core object is the **ndarray**, a multi-dimensional array of elements (all of the same type). Let’s explore how to create and manipulate arrays, and use NumPy’s functions:

### Creating NumPy Arrays

- **From Python lists/tuples:** You can create a NumPy array by passing a Python list or nested list to `np.array()`. For example: 

  ```python
  data_list = [1, 2, 3, 4]
  arr = np.array(data_list)
  print(arr)        # Output: [1 2 3 4]
  print(arr.dtype)  # Data type of elements, e.g., int64
  ``` 

  This converts the list into a 1D NumPy array. Nested lists will create multi-dimensional arrays (e.g. a list of lists becomes a 2D array) ([NumPy Tutorial – Python Library | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-tutorial/#:~:text=The%20numpy%20array%20also%20called,table%20with%20rows%20and%20columns)) ([NumPy Tutorial – Python Library | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-tutorial/#:~:text=NumPy%20arrays%20are%20created%20using,arrays%20by%20further%20nesting%20lists)).

- **Using built-in array creation functions:** NumPy provides convenient functions to create arrays of certain shapes filled with constant or sequential values:
  - `np.zeros(shape)` – create an array filled with 0.  
  - `np.ones(shape)` – create an array filled with 1.  
  - `np.full(shape, fill_value)` – create an array filled with a specified value.
  - `np.arange(start, stop, step)` – create a 1D array with a range of values (like Python’s `range()` but as an array). For example, `np.arange(0, 10, 2)` gives array `[0, 2, 4, 6, 8]`.
  - `np.linspace(start, stop, num)` – create an array of `num` evenly spaced values between start and stop (inclusive).
  - `np.eye(N)` or `np.identity(N)` – create an NxN identity matrix (1s on the diagonal, 0s elsewhere).
  - `np.random.rand(d0, d1, ...)` – create an array of given shape with random samples from a uniform distribution on [0,1). (NumPy’s random module contains many methods for random sampling.)

  **Example:** 

  ```python
  a = np.zeros((2, 3))         # 2x3 array of zeros
  b = np.ones((3, 3))          # 3x3 array of ones
  c = np.arange(5)             # 1D array: [0 1 2 3 4]
  d = np.linspace(0, 1, 5)     # 1D array: [0.   0.25 0.5  0.75 1.  ]
  e = np.eye(3)                # 3x3 identity matrix
  print(a); print(d)
  ``` 

  Output:
  ```
  [[0. 0. 0.]
   [0. 0. 0.]]
  [0.   0.25 0.5  0.75 1.  ]
  ```

- **Data types (dtype):** When creating an array, NumPy will automatically choose a data type for the elements (e.g. `int64`, `float64`). You can check the data type with `arr.dtype`. You can also specify a dtype explicitly, for example `np.array([1,2,3], dtype=np.float32)`. Common NumPy dtypes include `int64`, `float64`, `bool`, etc. Using a lower precision type (like float32 instead of float64) can save memory, but might affect numerical precision.

**Array attributes:** Once you have an ndarray, you should know its important attributes:
- `arr.shape` – a tuple indicating the size of each dimension (e.g. a 2D array of 3 rows and 4 columns has shape `(3, 4)`).  
- `arr.ndim` – number of dimensions (axes) of the array. (1D ⇒ ndim=1, 2D ⇒ ndim=2, etc.)  
- `arr.size` – total number of elements in the array (product of the shape dimensions).  
- `arr.dtype` – data type of the elements.  
- `arr.itemsize` – size in bytes of each element.  
- `arr.nbytes` – total bytes consumed by the array (`nbytes = size * itemsize`).  

Example: 

```python
x = np.array([[1, 2, 3],
              [4, 5, 6]])
print(x.shape)    # (2, 3) -> 2 rows, 3 columns
print(x.ndim)     # 2 (it's a 2D array)
print(x.size)     # 6 (2*3 elements)
print(x.dtype)    # e.g. int64
print(x.nbytes)   # total bytes (6 * 8 bytes for int64 = 48)
``` 

### Indexing and Slicing Arrays

**Indexing** in NumPy works similarly to Python lists, but with extended capabilities for multi-dimensional data:
- **1D arrays:** Use a zero-based index to access elements. e.g. `arr[0]` for the first element, `arr[-1]` for the last element. 
- **2D arrays:** Use a tuple of indices `arr[row_index, col_index]` to access a single element. For example, given `M = np.array([[10,11,12],[13,14,15]])`, `M[0,2]` returns `12` (element at first row, third column). You can also index one dimension at a time: `M[0]` returns the first row as a 1D array `[10 11 12]`.

**Slicing** allows you to extract subarrays. The syntax is `start:stop:step` for each axis:
- For 1D array `v`, `v[1:5]` gives a slice from index 1 up to (but not including) index 5. For example, if `v = np.array([0,1,2,3,4,5])`, then `v[1:5]` yields `[1 2 3 4]`. You can omit start or stop to go to the beginning or end respectively (e.g. `v[:3]` gives first 3 elements). Negative indices count from the end (e.g. `v[-3:]` gives the last 3 elements).
- For multi-dimensional arrays, you can slice each dimension. For instance, `M[:2, 1:]` will take the first two rows and all columns from index 1 onward. If `M` is the 2x3 array above, `M[:2, 1:]` yields `[[11,12],[14,15]]`. You separate slices for different axes with commas.

One **important difference** from Python lists: **slicing a NumPy array returns a *view*, not a copy** ([1.4.1. The NumPy array object — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/array_object.html#copies-and-views#:~:text=A%20slicing%20operation%20creates%20a,You%20can)). This means the extracted subarray uses the same memory as the original array. Modifying the slice will modify the original array as well. For example:
```python
a = np.array([9,8,7,6])
b = a[1:3]      # b is a view of a's slice [8,7]
b[0] = 100      # modifying b will affect a
print(a)        # a is now [9, 100, 7, 6]
``` 
In this example, `a` changed because `b` is just a view on `a` (no new array was created). If you need an actual copy, you can use `arr.copy()` to explicitly copy the data.

For 2D arrays, slicing can extract submatrices. For example:
```python
M = np.array([[1,2,3],
              [4,5,6],
              [7,8,9]])
sub = M[0:2, 1:3]   # take first 2 rows and cols 1-2 (exclusive of 3rd index)
print(sub)         # [[2 3]
                   #  [5 6]]
``` 
Again, `sub` is a view of `M`. If we do `sub[0,0] = 99`, then `M[0,1]` becomes 99.

**Stepping and reversing:** The slice syntax allows a step. For instance, `arr[::2]` takes every second element. You can easily **reverse an array** by using a step of -1: `arr[::-1]` gives the array reversed. For a 2D array, you can reverse along a specific axis: e.g. `M[::-1, :]` reverses the rows (up-down flip), while `M[:, ::-1]` reverses the columns (left-right flip).

**Boolean masking:** NumPy allows you to use boolean arrays for indexing (advanced indexing). If `mask` is a boolean array of the same shape as `arr`, then `arr[mask]` will select all elements where the mask is True. Typically, you create a mask by applying a condition on the array. For example:
```python
x = np.array([10, -3, 5, -1, 0])
negatives = x < 0         # array([False, True, False, True, False])
print(x[negatives])       # array([-3, -1]) – elements where condition is True
# We can even directly do:
x[x < 0] = 0              # Set all negative values to 0
print(x)                  # array([10,  0,  5,  0,  0])
```
Here `x < 0` produces a boolean array which we use to filter or modify `x`. Boolean indexing is very powerful for extracting or modifying subsets of your data without loops.

**Fancy indexing (indexing with arrays of ints):** You can also index using an array of indices. For example, `y = np.array([10,11,12,13])`; then `idx = [0, 3]`; `y[idx]` will return `[10, 13]`. For a 2D array, you can provide index arrays for each axis: e.g. `M[[0,2], [1,2]]` would pick the elements at (0,1) and (2,2). (Fancy indexing always returns a copy, not a view, since it may gather elements from different places in memory.)

### Reshaping and Combining Arrays

NumPy makes it easy to **change the shape** of an array without changing its data:
- **Reshape:** `arr.reshape(new_shape)` will return a new view (if possible) of `arr` with the shape `new_shape` (which should have the same total number of elements). For example, a 1D array of length 6 can be reshaped to a 2x3 or 3x2 array: 
  ```python
  v = np.arange(6)         # [0,1,2,3,4,5]
  M = v.reshape(2, 3)      # 2x3 array: [[0,1,2],[3,4,5]]
  ``` 
  You can use `-1` for one dimension in reshape to let NumPy infer it. For instance `np.arange(10).reshape(2, -1)` will reshape into 2 rows and auto-calculate the number of columns (here 5 columns).

- **Flatten:** To convert a multi-dimensional array to a 1D array, use `arr.ravel()` or `arr.flatten()`. `ravel()` returns a view if possible (otherwise a copy), whereas `flatten()` always returns a new copy. Example: `M.ravel()` on the 2x3 `M` above would give `[0 1 2 3 4 5]`.

- **Transpose:** For a 2D array, `arr.T` gives the transpose (switches rows and columns). For higher dimensions, `arr.transpose()` can reorder axes in any order.

- **Concatenation:** You can concatenate arrays along an existing axis using `np.concatenate`. For example, if `a = [1,2,3]` and `b = [4,5,6]`, then `np.concatenate((a,b))` yields `[1 2 3 4 5 6]`. For 2D arrays, you can concatenate along rows or columns by specifying `axis` (axis=0 for vertical stacking (adding more rows), axis=1 for horizontal stacking (adding more columns)). NumPy also provides convenience functions: `np.vstack((a,b))` to stack arrays vertically (row-wise), and `np.hstack((a,b))` to stack horizontally (column-wise).

  **Example:** 
  ```python
  A = np.array([[1,2],[3,4]])   # shape (2,2)
  B = np.array([[5,6],[7,8]])   # shape (2,2)
  v = np.array([9, 10])         # shape (2,)
  print(np.vstack((A, B)))
  # [[1 2]
  #  [3 4]
  #  [5 6]
  #  [7 8]]
  print(np.hstack((A, v.reshape(2,1))))
  # [[ 1  2  9]
  #  [ 3  4 10]]
  ```
  In the second print, we reshaped `v` to (2,1) so it can be hstacked as a third column of A.

### Arithmetic Operations and Universal Functions

NumPy allows **element-wise arithmetic** on arrays with ease. If you perform standard arithmetic operators (`+`, `-`, `*`, `/`, etc.) on arrays, it applies **element-wise** (provided shapes are compatible, see *Broadcasting* below). For example:
```python
x = np.array([1,2,3])
y = np.array([10,20,30])
print(x + y)      # array([11,22,33])
print(x * y)      # array([10,40,90])
print(x ** 2)     # array([1,4,9])
print(y - 5)      # array([ 5,15,25])
```
These operations produce new arrays as results. Note that `*` is not matrix multiplication (for matrix multiply, use `np.dot` or the `@` operator as explained later) ([1.4.2. Numerical operations on arrays — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/operations.html#broadcasting#:~:text=Array%20multiplication%20is%20not%20matrix,multiplication)) ([1.4.2. Numerical operations on arrays — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/operations.html#broadcasting#:~:text=Matrix%20multiplication%3A)).

NumPy also provides **universal functions (ufuncs)** which are vectorized functions that apply to each element of an array. These include mathematical functions like `np.sqrt`, `np.exp`, `np.sin`, `np.log`, etc. Using ufuncs, you can compute complex element-wise transformations without Python loops. For example:
```python
angles = np.array([0, 30, 60, 90]) * np.pi/180.0  # in radians
print(np.sin(angles))
# Output: [0.      0.5     0.8660254  1.      ]  (sine of each angle)
```
You can combine element-wise operations and ufuncs to compute many values at once, which is typically much faster than looping in pure Python.

**Comparisons and boolean logic:** Element-wise comparison operators (`<, >, ==, !=, <=, >=`) return boolean arrays. For example, `arr > 0` gives a boolean array of the same shape as `arr` indicating which elements are positive. You can combine boolean arrays with logical operators: `np.logical_and(cond1, cond2)`, `np.logical_or`, etc., or use the bitwise operators `&` (and), `|` (or) on boolean arrays (ensure to use parentheses due to operator precedence).

### Aggregations and Statistics

NumPy makes it easy to compute aggregate statistics across an array:
- `arr.sum()` – sum of all elements (or use `np.sum(arr)`).
- `arr.mean()` – mean of elements.
- `arr.min()`, `arr.max()` – minimum/maximum element.
- `arr.prod()` – product of all elements.
- `arr.std()`, `arr.var()` – standard deviation and variance.
- `arr.argmin()`, `arr.argmax()` – index of minimum/maximum element.

**Axis parameter:** By default these compute the aggregate over the entire array. For multi-dimensional arrays, you can aggregate *along an axis*. For example, given a 2D array:
```python
M = np.array([[1,2,3],
              [10,20,30]])
print(M.sum())          # sum of all elements -> 66
print(M.sum(axis=0))    # sum down each column -> [11 22 33]
print(M.sum(axis=1))    # sum across each row -> [ 6 60]
```
Here `axis=0` means *collapse rows, compute along columns* (producing one result per column), and `axis=1` means *collapse columns, compute along each row*. Many numpy functions accept this `axis` argument (e.g. mean, min, etc.) to perform the operation by row or by column.

### Broadcasting 

One of NumPy’s most powerful features is **broadcasting**, which allows arithmetic operations on arrays of different shapes by automatically **stretching** the smaller array to match the larger one’s shape ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=Broadcasting%20simplifies%20mathematical%20operations%20on,explicitly%20copying%20or%20reshaping%20data)) ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=It%20automatically%20adjusts%20the%20smaller,scalar%20to%20a%202D%20Array)). Broadcasting rules apply when performing element-wise operations (like addition or multiplication) on arrays that do not have the same shape, but have compatible dimensions.

**How broadcasting works:** When operating on two arrays, NumPy compares their shapes element by element from the **last dimension backwards**:
1. If the arrays differ in number of dimensions, the smaller array is **paded with ones** on its leading (left) side until the dimension counts match ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=1,1)).
2. Then for each dimension, the sizes are compatible if they are equal **or** if **one of them is 1** ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=3,1)).
3. If a dimension in one array is 1 and the other is greater than 1, NumPy will **broadcast** (replicate) the array along that dimension to match the size of the other array ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=Broadcasting%20simplifies%20mathematical%20operations%20on,explicitly%20copying%20or%20reshaping%20data)) ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=It%20automatically%20adjusts%20the%20smaller,scalar%20to%20a%202D%20Array)).
4. If any dimension sizes conflict (one is not 1 and not equal to the other), a broadcasting error (`ValueError`) is thrown.

Broadcasting can **eliminate the need for explicit loops or manual resizing** of arrays, making code more concise and often faster ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=Broadcasting%20simplifies%20mathematical%20operations%20on,explicitly%20copying%20or%20reshaping%20data)). 

**Example 1 (scalar and array):** Adding a scalar to an array:  
```python
A = np.array([1,2,3])
print(A + 5)  
# Output: [6 7 8]
``` 
Here, the scalar `5` is conceptually stretched to the shape of `A` (as `[5,5,5]`) and then added elementwise ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=Broadcasting%20simplifies%20mathematical%20operations%20on,explicitly%20copying%20or%20reshaping%20data)). NumPy does this internally without actually allocating a new array for the scalar.

**Example 2 (different shapes):**  
```python
M = np.array([[1,2,3],
              [4,5,6]])        # shape (2,3)
v = np.array([10, 20, 30])     # shape (3,)
result = M + v
print(result)
# Output:
# [[11 22 33]
#  [14 25 36]]
``` 
Here `M` has shape (2,3) and `v` has shape (3,). When adding, NumPy sees that `v` is 1-dimensional. It pads `v`’s shape to (1,3) to match M’s 2D shape, then on the first dimension, 1 vs 2 means `v` is replicated to shape (2,3) ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=It%20automatically%20adjusts%20the%20smaller,scalar%20to%20a%202D%20Array)). Effectively, `v` is added to each row of `M`. This is much more efficient than looping to add `v` to each row manually.

**Example 3:** If you have `a` of shape (4,1) and `b` of shape (3,), you can add them because after padding `a` to (4,1) and `b` to (1,3), the 1 can expand. The result will be shape (4,3). For instance:
```python
a = np.array([[1],[2],[3],[4]])   # shape (4,1)
b = np.array([10,20,30])          # shape (3,)
print(a + b)
# Output is a 4x3 matrix:
# [[11 21 31]
#  [12 22 32]
#  [13 23 33]
#  [14 24 34]]
```
Here `a` is effectively repeated horizontally and `b` repeated vertically.

Broadcasting is very powerful. It lets you vectorize computations that involve arrays of different sizes without writing loops. It’s used behind the scenes in many NumPy operations. **Note:** When NumPy broadcasts, it doesn’t actually copy the data in memory for the repeated entries – it works with strides (multiple indices referring to the same data) to be memory-efficient. But one must be cautious: broadcasting a large array with a smaller one conceptually can lead to high memory usage if you attempt to materialize the broadcasted result. In general, though, broadcasting makes code cleaner and does *not* incur heavy memory cost unless you explicitly create the expanded arrays.

### Random Number Generation

NumPy includes a random module (`np.random`) that can generate random numbers efficiently:
- `np.random.rand(d0, d1, ...)` – generates an array of given shape with random samples from a uniform distribution [0,1).
- `np.random.randn(d0, d1, ...)` – samples from a standard normal distribution (mean 0, std 1).
- `np.random.randint(low, high, size)` – random integers from `low` (inclusive) to `high` (exclusive).
- `np.random.choice(arr, size)` – randomly choose elements from an array.
- To reproduce results, use `np.random.seed(seed_value)` before generating random numbers.

**Example:**  
```python
rng = np.random.default_rng(42)        # using new random Generator (NumPy 1.17+)
# For simplicity, using legacy np.random for demonstration:
np.random.seed(0)
rand_arr = np.random.rand(3, 2)        # 3x2 array of random floats in [0,1)
rand_ints = np.random.randint(5, 15, size=4)  # 1D array of 4 random ints between 5 and 14
print(rand_arr, rand_ints, sep="\n")
``` 
Output (will vary due to randomness, but with seed=0 it's deterministic):
```
[[0.5488135  0.71518937]
 [0.60276338 0.54488318]
 [0.4236548  0.64589411]]
[ 5 12 14 13]
```

## NumPy Tips, Tricks, and Best Practices

NumPy is a powerful tool—below are key practices to use it effectively:

- **Use vectorized operations and avoid Python loops:** Looping in Python over array elements is slow; instead, use NumPy’s vectorized operations that operate on whole arrays at once. For example, to compute the element-wise square of an array, prefer `arr ** 2` over a `for` loop. Internally, NumPy uses optimized C code, so vectorized ops are much faster ([1.4.2. Numerical operations on arrays — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/operations.html#broadcasting#:~:text=)). A quick timing comparison: squaring 1000 elements took ~403 µs with a Python list comprehension, but only ~12.7 µs with a NumPy array ([1.4.1. The NumPy array object — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/array_object.html#copies-and-views#:~:text=In%20)) ([1.4.1. The NumPy array object — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/array_object.html#copies-and-views#:~:text=In%20)) – a **30x speedup**.

- **Take advantage of broadcasting:** Write code assuming NumPy will broadcast smaller arrays to fit larger ones, rather than manually replicating data. For instance, if you need to add a constant vector to each row of a matrix, just do `matrix + vector` (shapes permitting) instead of using a loop. Broadcasting will handle it, making your code concise and fast ([NumPy Array Broadcasting | GeeksforGeeks](https://www.geeksforgeeks.org/numpy-array-broadcasting/#:~:text=Broadcasting%20simplifies%20mathematical%20operations%20on,explicitly%20copying%20or%20reshaping%20data)). Always ensure the dimensions align with broadcasting rules (add dummy dimensions with `np.newaxis` if needed).

- **Use slicing and views to avoid unnecessary copies:** As noted, slicing gives you a view on the data without copying. This is memory efficient. If you need to work on a subarray without altering the original, either use `.copy()` or be mindful not to modify the view. **Avoid large intermediate copies** – for example, instead of `c = a + b; d = c * 2` (which makes `c` as an intermediate array), you can do `d = (a + b) * 2` in one expression. NumPy also offers an `out` parameter in many functions to write results directly into an existing array to avoid allocating new arrays.

- **Be careful with mutable operations:** Some operations, like `arr.sort()` or `np.fill_diagonal`, operate in-place and modify the array directly, while most others (like `np.sort(arr)`) return a new array. Know which is which to avoid unintended side-effects or extra memory usage.

- **Use boolean masks and fancy indexing for conditionals:** Instead of looping with if-else, use boolean arrays. For example, to **filter** data: `filtered = data[data > 0]` gets all positive values. To **replace** based on condition: `data[data < 0] = 0` is much faster and cleaner than looping. For conditional selection, `np.where` is useful: e.g. `result = np.where(cond_array, x_array, y_array)` picks elements from `x_array` where `cond` is True, otherwise from `y_array`.

- **Understand copying behavior:** Slicing produces views (no copy) ([1.4.1. The NumPy array object — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/array_object.html#copies-and-views#:~:text=A%20slicing%20operation%20creates%20a,You%20can)), but **fancy indexing or arithmetic operations produce copies**. For example, `sub = arr[[0,2]]` or `sub = arr * 2` will allocate new memory for `sub`. If you modify `sub`, the original `arr` is unaffected in those cases. Knowing this helps manage memory and avoid confusion. If you truly need a separate copy of an array (e.g. to safely modify it), use `arr.copy()`.

- **Choosing data types:** By default, NumPy might use 64-bit types (for ints and floats). If you have very large arrays and the precision of 32-bit floats or ints is sufficient, consider using `dtype=np.float32` or `np.int32` to halve memory usage. This can be important when working with large image data (often 8-bit or 16-bit per pixel) or large datasets. However, keep in mind precision: e.g., summing a large array of float32 may accumulate more rounding error than float64.

- **Use NumPy’s linear algebra library instead of manual algorithms:** NumPy (via `np.linalg`) provides efficient implementations for things like matrix multiplication, inversion, decompositions, etc. Use these instead of writing your own unless for learning purposes. For example, use `np.dot(A, B)` or `A @ B` for matrix multiplication, `np.linalg.inv(A)` for inverse, `np.linalg.eig` for eigenvalues, etc. These are highly optimized (often using BLAS/LAPACK).

- **Vectorize with `np.vectorize` if needed (but be cautious):** If you have a Python function that isn’t naturally vectorized, you can use `np.vectorize(func)` to apply it element-wise to an array. However, note that `np.vectorize` is essentially a convenience that still loops under the hood (just in C). It does not provide speed like true ufuncs. For performance, it's better to find a way to express the operation with numpy routines.

- **Set print options for large arrays:** When dealing with large arrays, NumPy by default truncates printing. You can adjust this with `np.set_printoptions` (e.g., `np.set_printoptions(threshold=1000)` to increase the print threshold, or use `suppress=True` to avoid scientific notation for small numbers). This is just for display convenience.

By following these practices – vectorizing operations, leveraging broadcasting, and being mindful of memory – you can write **efficient NumPy code** that fully utilizes its capabilities.

## Advanced NumPy Examples and Techniques

Beyond the basics, NumPy can accomplish some non-obvious tasks very elegantly. Here are a few examples that demonstrate more complex uses:

- **Outer operations and mesh-grids:** Suppose you have two 1D arrays and want to compute some function on every pair of elements (like a table of all pairwise sums or products). You can use broadcasting to achieve this without loops. For instance, to compute an addition table for values 0 to 4:
  ```python
  x = np.arange(5)              # [0,1,2,3,4]
  outer_sum = x[:, np.newaxis] + x[np.newaxis, :]
  print(outer_sum)
  ``` 
  This produces a 5x5 matrix where element (i,j) is `i+j`:
  ```
  [[0 1 2 3 4]
   [1 2 3 4 5]
   [2 3 4 5 6]
   [3 4 5 6 7]
   [4 5 6 7 8]]
  ```
  Here `x[:, None]` reshaped `x` to (5,1) and `x[None, :]` to (1,5), then broadcasting gave a (5,5) result. NumPy also offers `np.meshgrid` to directly generate grids of indices or values. For example, `X, Y = np.meshgrid(x, x)` would produce coordinate matrices for X and Y values over a grid.

- **Pairwise distances (vectorization example):** You have two sets of points and want to compute all pairwise distances between them. Instead of a double loop, you can do:
  ```python
  points1 = np.random.rand(5, 2)  # 5 points in 2D
  points2 = np.random.rand(3, 2)  # 3 other points in 2D
  # Compute squared Euclidean distances:
  diff = points1[:, np.newaxis, :] - points2[np.newaxis, :, :]  # shape (5,3,2)
  dist_sq = np.sum(diff**2, axis=2)   # shape (5,3), each [i,j] is ||points1[i]-points2[j]||^2
  distances = np.sqrt(dist_sq)
  print(distances)
  ```
  The result is a 5x3 array of distances. The trick was introducing new axes to broadcast the subtraction across all pairs. This vectorized approach uses memory for the intermediate `diff`, but is far faster than explicit loops for moderately sized datasets. (For very large sets, one might use chunking or more advanced techniques.)

- **Using `np.einsum` for tensor operations:** The Einstein summation (`np.einsum`) is a powerful function for expressing operations involving multiple indices in a compact way. For example, matrix multiplication of `A` (shape MxN) and `B` (NxP) can be done by `np.einsum('ik,kj->ij', A, B)`. This isn’t necessary for built-in operations (since you can use `A @ B`), but `einsum` can do things like summing over certain axes or computing tensor contractions in one go. For instance, to compute the sum of outer products of two matrices’ rows, you could use einsum instead of nested loops.

- **Memory layout and stride tricks:** NumPy arrays have strides that determine how the index is translated to memory address. Advanced users can use `np.lib.stride_tricks.as_strided` or `np.lib.stride_tricks.sliding_window_view` to create views that represent a moving window on data without copying. For example, to get a rolling window of length 3 on a 1D array:
  ```python
  from numpy.lib.stride_tricks import sliding_window_view
  data = np.array([1,2,3,4,5])
  windows = sliding_window_view(data, window_shape=3)
  print(windows)
  # Output:
  # array([[1, 2, 3],
  #        [2, 3, 4],
  #        [3, 4, 5]])
  ```
  This gives a view of shape (3,3) where each row is a window of the original data. No copies are made; the rows are overlapping views of `data`. This kind of trick is useful in signal processing or time-series analysis (e.g., computing moving averages).

- **Structured arrays:** NumPy supports compound dtypes (like C structs) where each element of an array can have multiple named fields (e.g., an array of records). This is an advanced feature useful for certain applications (like reading binary data with mixed types). For instance, `dt = np.dtype([('x', np.int32), ('y', np.float64)])` defines a dtype with an int and a float. An array of this dtype can be created, and you can access `arr['x']` or `arr['y']` as if the array has columns. This is beyond the scope of most beginner workflows but good to be aware of for specialized use cases.

- **Integrating with other libraries:** NumPy arrays are the foundation for pandas (data frames), SciPy (scientific computing routines), and many machine learning libraries. Many of these libraries either accept or return NumPy arrays. Understanding NumPy allows you to seamlessly move data into those libraries (e.g., converting an image array to a Tensor for TensorFlow/PyTorch, or manipulating data in NumPy then creating a pandas DataFrame).

These advanced usages show that NumPy is not just about basic arrays – it’s a toolkit that, with some creativity, can handle complex tasks. As you progress, you’ll discover more patterns and functions (like `np.where`, `np.take`, `np.put`, `np.argmax` with axis for multi-dimensional argmax, etc.) that further unlock NumPy’s power.

## Examples: NumPy in Machine Learning 

NumPy alone (without higher-level libraries) can be used to implement many basic machine learning algorithms. Here we demonstrate a couple of mini-examples to illustrate how NumPy can handle common ML computations.

### Example 1: Linear Regression from Scratch (Least Squares)

Suppose we have a simple linear model $y = w x + b$ and some training data points. We want to find the best-fit line through the points (linear regression). This can be done analytically using the Normal Equation or by using NumPy’s linear algebra solver.

**Generate some synthetic data** for a linear relation $y = 3x - 2$ with noise: 

```python
import numpy as np
# True parameters: w=3, b=-2
np.random.seed(0)
X = np.linspace(0, 10, 20)               # 20 points between 0 and 10
y_true = 3 * X - 2
y = y_true + np.random.normal(scale=3, size=X.shape)   # add noise

# Design matrix for linear model (add a column of 1s for intercept term)
X_b = np.column_stack([X, np.ones_like(X)])  # shape (20,2): [x, 1]
# Solve for [w, b] using least squares: (X_b^T X_b)^{-1} X_b^T y
w_b, *_ = np.linalg.lstsq(X_b, y, rcond=None)  # returns solution for linear coefficients
w, b = w_b
print(f"Fitted w = {w:.2f}, b = {b:.2f}")
```

This code constructs a matrix `X_b` that has a column of `x` values and a column of 1s (for the intercept). Using `np.linalg.lstsq`, it solves the linear least squares problem. The output might be, for example:
```
Fitted w = 2.95, b = -1.54
``` 
which is close to the true 3 and -2 (with some error due to noise). We could also solve explicitly with normal equation: 
```python
theta = np.linalg.inv(X_b.T @ X_b) @ (X_b.T @ y)
``` 
which should give the same result.

**Using gradient descent:** Alternatively, we could use NumPy to perform gradient descent to find $w$ and $b$. This involves iteratively updating $w$ and $b$ in the direction of the negative gradient of the mean squared error. The vectorized operations in NumPy make this relatively straightforward:
```python
# Gradient descent for linear regression
w, b = 0.0, 0.0    # initial guess
lr = 0.01          # learning rate
for epoch in range(1000):
    y_pred = w * X + b
    error = y_pred - y
    # Compute gradients
    dw = (2/len(X)) * np.dot(error, X)    # derivative w.r.t. w
    db = (2/len(X)) * np.sum(error)      # derivative w.r.t. b
    # Update parameters
    w -= lr * dw
    b -= lr * db
# After loop, w and b should be close to optimal
print(w, b)
``` 
This loop uses NumPy operations for computing predictions and gradients (note the use of `np.dot` and `np.sum` to aggregate). Gradient descent will converge to similar values as the least squares solution if learning rate and iterations are set properly.

### Example 2: k-Means Clustering

**k-Means** is an unsupervised learning algorithm that partitions data into *k* clusters. Using NumPy, we can implement k-means fairly directly:

1. Initialize k cluster centers (randomly choose points from the data or random values).
2. Assign each data point to the nearest cluster center (compute distances and take argmin).
3. Update each cluster center to be the mean of points assigned to it.
4. Repeat until convergence.

Here’s a sketch of how one might implement a single iteration of k-means using NumPy’s vectorization:

```python
# Assume data is an (N, d) array of N points in d dimensions
# centers is a (k, d) array of current cluster centers
# Step 2: Assign points to nearest cluster
diff = data[:, np.newaxis, :] - centers[np.newaxis, :, :]   # shape (N, k, d)
distances = np.linalg.norm(diff, axis=2)   # shape (N, k), Euclidean distance from each center
cluster_assignments = np.argmin(distances, axis=1)  # shape (N,), index of nearest center for each point

# Step 3: Update centers as mean of assigned points
new_centers = np.array([
    data[cluster_assignments == j].mean(axis=0) 
    for j in range(k)
])
```

In the code above, the distance calculation is fully vectorized: we take advantage of broadcasting to subtract each center from each data point ([Understanding Broadcasting in NumPy | by Dagang Wei - Medium](https://medium.com/@weidagang/understanding-broadcasting-in-numpy-c44dceae42ea#:~:text=Understanding%20Broadcasting%20in%20NumPy%20,larger%20ones%20during%20arithmetic%20operations)), then compute the norm (distance) across the last axis. The result is an `N x k` array of distances, and `argmin` finds the index of the closest center for each point. Then we use a Python comprehension to compute the new centers; one could also vectorize this update using `np.where` or grouping by cluster indices, but for clarity a loop over `k` (which is typically small) is fine. This algorithm would loop over the assignment-update steps until convergence. Despite the inner workings, all heavy computations (distance calc and mean) use NumPy, making it efficient.

### Example 3: Principal Component Analysis (PCA)

PCA is a dimensionality reduction technique. With NumPy, implementing PCA involves linear algebra operations:
1. Subtract the mean from the data (so it’s zero-centered).
2. Compute the covariance matrix.
3. Compute eigenvalues and eigenvectors of the covariance matrix.
4. The principal components are the eigenvectors corresponding to the largest eigenvalues.

Using NumPy:
```python
# Assume data is (N, d)
X = data - data.mean(axis=0)                  # Center the data
cov_mat = np.cov(X, rowvar=False)             # Compute covariance matrix of shape (d, d)
eig_vals, eig_vecs = np.linalg.eigh(cov_mat)  # Eigen decomposition
# Sort eigenvectors by eigenvalues
order = np.argsort(eig_vals)[::-1]
eig_vecs = eig_vecs[:, order]
eig_vals = eig_vals[order]
# Project data onto the first principal component:
pc1 = eig_vecs[:, 0]                          # principal component (d-dimensional unit vector)
X_proj = X @ pc1                              # project data onto PC1 (result shape (N,))
```

Here, `np.cov` was used for convenience to get the covariance matrix. We use `np.linalg.eigh` (since covariance is symmetric) to get eigenvalues and eigenvectors ([NumPy: Create a 2d array with 1 on the border and 0 inside - w3resource](https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-8.php#:~:text=x%20%3D%20np,NumPy%20array%20filled%20with%20ones)). Sorting ensures we have the principal components in order of significance. We then projected the data onto the first principal component as an example.

This is a demonstration of using NumPy’s linear algebra in an ML context. In practice, one might use higher-level libraries (like scikit-learn) for PCA, but it’s powerful to see that **with just NumPy, a complex algorithm like PCA can be implemented in a few lines** leveraging vectorized operations and linear algebra routines.

## Examples: NumPy for Image Processing

Images can be thought of as just NumPy arrays! For a grayscale image, it’s a 2D array of pixel intensities. For a color image, it’s a 3D array with shape (height, width, 3) typically, where the last dimension corresponds to color channels (Red, Green, Blue). Because of NumPy’s efficiency in handling arrays, many image processing tasks can be done with just NumPy (and perhaps a little bit of plotting to visualize results).

Let’s consider some basic image processing examples using NumPy:

- **Loading an image into a NumPy array:** You can use libraries like imageio or matplotlib to read an image file into a NumPy array. For example, using matplotlib: `img = plt.imread('myphoto.png')`. The resulting `img` might be an array of shape `(H, W, 3)` (or 4 if there’s an alpha channel) with `dtype('uint8')` or `float32` depending on how it’s read ([1.4.4. Advanced operations — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/advanced_operations.html#images#:~:text=)) ([1.4.4. Advanced operations — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/advanced_operations.html#images#:~:text=)). Each element is a pixel value (0–255 for uint8). If using an image with intensity values, you might get floats between 0 and 1.

- **Color to grayscale conversion:** A common operation is converting an RGB image to grayscale. This can be done by a weighted sum of the R, G, B channels. For instance:
  ```python
  gray = np.dot(img[...,:3], [0.299, 0.587, 0.114])
  ```
  This single line takes the `img` array (… means all rows and cols, and `:3` takes the first 3 channels if there is an alpha channel) and applies the dot product with the weight vector, effectively computing `0.299*R + 0.587*G + 0.114*B` for each pixel, resulting in a 2D `gray` array. These particular weights account for human perception (we are more sensitive to green, etc.).

- **Image slicing (cropping and flipping):** Because images are arrays, you can **crop** by slicing. For example, `center = img[100:400, 200:500]` would take a sub-image from y=100 to 399 and x=200 to 499. To **flip** an image horizontally: `flipped = img[:, ::-1]` (this slices all rows, and columns with step -1, reversing the column order). To flip vertically: `img[::-1, :]`. These operations do not require any special image library, just NumPy indexing.

- **Image filtering (convolution):** Many image processing operations involve **convolution** with a kernel (small matrix). We can implement a simple filter with NumPy. For example, a *blur* filter or an *edge-detection* filter. We’ll demonstrate a simple edge detection using a Sobel-like operator.

  Consider a very basic edge detector in the horizontal direction: kernel = [[-1, 0, 1]] (detect vertical edges by subtracting left pixel from right pixel) and vertical direction kernel = [[-1],[0],[1]]^T (subtract top from bottom). We can apply these to an image by convolution. Below, we manually convolve a small synthetic image to detect edges:

   *Example: A simple binary image (left) and the result of an edge detection filter (right). The white square’s borders are detected as bright lines on the right.* 

  In the above figure, we created a 100x100 image with a white square in the middle (left image). We then applied horizontal and vertical edge filters using NumPy (taking differences between neighboring pixels) and combined them to get the edge magnitude, shown on the right as white lines where the edges were. This demonstrates using convolution operations with NumPy to detect edges.

  **How to implement this with NumPy:** Suppose `img` is a 2D array for simplicity (grayscale). One approach:
  ```python
  # Pad the image to handle borders
  padded = np.pad(img, pad_width=1, mode='constant', constant_values=0)
  # Horizontal and vertical differences
  Gx = padded[1:-1, 2:] - padded[1:-1, :-2]   # difference between right and left neighbors
  Gy = padded[2:, 1:-1] - padded[:-2, 1:-1]   # difference between bottom and top neighbors
  # Edge magnitude
  edges = np.sqrt(Gx**2 + Gy**2)
  edges = np.clip(edges, 0, 255)  # if we want to cap values for display
  ```
  Here we used `np.pad` to add a 1-pixel border of zeros around the image for simplicity in computing differences at the edges. The expressions for `Gx` and `Gy` use slicing to get shifted versions of the image (effectively convolving with [-1,0,1] and its transpose). The result `edges` is a 2D array highlighting where intensity changes were high. We could further normalize or convert dtype for proper display. This is a manual convolution example – in practice one might use libraries or `scipy.ndimage`, but it’s illuminating to see it done with pure NumPy.

- **Other operations:** With NumPy it’s straightforward to implement other image manipulations:
  - Adjust brightness: simply add or multiply the array by a scalar (with care to clip or wrap around if uint8).
  - Apply a mask: e.g., set all pixels below a certain intensity to 0 using boolean indexing (`img[img < 128] = 0`).
  - Channel operations: Split an image into R, G, B channels with `R = img[...,0]` etc., or stack separate channels into one with `np.dstack`.

In summary, images are just arrays! You can apply all the slicing, arithmetic, and linear algebra operations from NumPy to image data. Many advanced image processing techniques ultimately boil down to matrix and vector operations, which NumPy handles with ease. 

*(Using matplotlib to display images, one would do `plt.imshow(img, cmap='gray')` for grayscale or without cmap for color. Here we embedded an example output above to illustrate the result of edge detection.)*

## Common NumPy Coding Questions (Q&A)

Finally, let's go through some classic NumPy coding questions and their solutions. These are typical problems or “gotchas” that help reinforce understanding:

- **Q1: How can I add a border of zeros around an existing NumPy array?**  
  **A1:** Use the `np.pad` function ([How to add a border around a NumPy array? | GeeksforGeeks](https://www.geeksforgeeks.org/how-to-add-a-border-around-a-numpy-array/#:~:text=Sometimes%20we%20need%20to%20add,%E2%80%980%E2%80%99%20around%20the%20identity%20matrix)) ([How to add a border around a NumPy array? | GeeksforGeeks](https://www.geeksforgeeks.org/how-to-add-a-border-around-a-numpy-array/#:~:text=%60)). For example, to add a border of 1 zero-width around a 2D array:
  ```python
  arr = np.array([[1,2],[3,4]])
  padded = np.pad(arr, pad_width=1, mode='constant', constant_values=0)
  print(padded)
  ```
  Output:
  ```
  [[0 0 0 0]
   [0 1 2 0]
   [0 3 4 0]
   [0 0 0 0]]
  ```
  Here `pad_width=1` adds one layer of 0s on all sides ([How to add a border around a NumPy array? | GeeksforGeeks](https://www.geeksforgeeks.org/how-to-add-a-border-around-a-numpy-array/#:~:text=%60)). (NumPy’s pad is very flexible; you can pad different widths on edges, use different modes like reflection, etc.)

- **Q2: How to create a 5x5 array with 1s on the border and 0s inside?**  
  **A2:** Easiest is to start with an array of ones and then set the interior to 0 using slicing ([NumPy: Create a 2d array with 1 on the border and 0 inside - w3resource](https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-8.php#:~:text=x%20%3D%20np,NumPy%20array%20filled%20with%20ones)):
  ```python
  arr = np.ones((5,5), dtype=int)
  arr[1:-1, 1:-1] = 0
  print(arr)
  ```
  Output:
  ```
  [[1 1 1 1 1]
   [1 0 0 0 1]
   [1 0 0 0 1]
   [1 0 0 0 1]
   [1 1 1 1 1]]
  ```
  The slice `1:-1` selects all indices except the first and last in that dimension, so `arr[1:-1, 1:-1]` targets the inner submatrix and sets it to 0 ([NumPy: Create a 2d array with 1 on the border and 0 inside - w3resource](https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-8.php#:~:text=x%20%3D%20np,NumPy%20array%20filled%20with%20ones)).

- **Q3: How do I find the common elements between two arrays?**  
  **A3:** NumPy has an intersection function for 1D arrays: `np.intersect1d`. For example:
  ```python
  a = np.array([1, 2, 3, 4, 5])
  b = np.array([3, 4, 5, 6, 7])
  common = np.intersect1d(a, b)
  print(common)   # [3 4 5]
  ``` 
  This will return the **sorted** unique common values ([Mastering NumPy: 100 Exercises with Solutions](https://www.w3resource.com/python-exercises/numpy/numpy_100_exercises_with_solutions.php#:~:text=Get%20the%20common%20items%20between,two%20arrays)). Under the hood it might sort, so if order matters you might need a different approach, but for just finding common items, `intersect1d` is straightforward.

- **Q4: How can I get the index of the maximum value in a 1D array?**  
  **A4:** Use `np.argmax`. Similarly `np.argmin` for the min index. Example:
  ```python
  arr = np.array([3, 7, 1, 10, 4])
  idx_max = np.argmax(arr)
  idx_min = np.argmin(arr)
  print(idx_max, idx_min)   # 3  (index of 10) and 2 (index of 1)
  ``` 
  In this example, the largest value 10 is at index 3 ([Mastering NumPy: 100 Exercises with Solutions](https://www.w3resource.com/python-exercises/numpy/numpy_100_exercises_with_solutions.php#:~:text=Find%20the%20index%20of%20the,value%20in%20a%201D%20array)). If the array has multiple equal maxima, argmax returns the first occurrence. For multi-dimensional arrays, `argmax` returns a flat index; you can get multi-index via `np.unravel_index` if needed.

- **Q5: How do I find the array element closest to a given scalar value?**  
  **A5:** There’s no direct NumPy function for this, but you can do it in one line using vectorized operations:
  ```python
  import numpy as np
  arr = np.array([10, 22, 37, 41, 55])
  target = 30
  closest_index = np.abs(arr - target).argmin()
  closest_value = arr[closest_index]
  print(closest_value)   # 37
  ``` 
  Explanation: `np.abs(arr - target)` computes the absolute difference between each element and the target value. The smallest difference indicates the closest value. We then take `argmin()` of that difference array to get the index of the smallest difference, and use it to index back into `arr`. In this example, 37 is closest to 30 (difference 7). 

- **Q6: Does slicing a NumPy array allocate a new array? (View vs Copy)**  
  **A6:** **Slicing yields a view, not a copy** ([1.4.1. The NumPy array object — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/array_object.html#copies-and-views#:~:text=A%20slicing%20operation%20creates%20a,You%20can)). The data is not copied in memory when you slice; the slice is just a different view into the same buffer. For instance:
  ```python
  A = np.arange(10)
  B = A[2:6]     # slice view of A
  B[:] = 0       # modify all elements in B
  print(A)       # A is modified in indices 2..5
  ``` 
  After this code, `A` will have zeros in positions 2 through 5, because `B` is referencing that part of `A`. This behavior is by design for efficiency. In contrast, **fancy indexing or arithmetic operations produce a copy**. For example `C = A[[2,3,4]]` or `C = A * 2` will result in `C` that does not share memory with `A`. Understanding this distinction is important to avoid unintended side effects or performance issues ([1.4.1. The NumPy array object — Scipy lecture notes](https://scipy-lectures.org/intro/numpy/array_object.html#copies-and-views#:~:text=A%20slicing%20operation%20creates%20a,You%20can)).

Each of these Q&A covers a common NumPy scenario. Mastering these patterns will help you write effective NumPy code and debug issues (like unexpected modifications due to views, etc.).

## Practice Exercises

To solidify your understanding, here are **50 practice questions** categorized by difficulty. Attempting these will help reinforce the NumPy concepts covered:

### Easy (Basics)

1. **Array Creation:** Create a 1D NumPy array containing the numbers 0 through 9.
2. Create a 3x3 NumPy array filled with zeros.
3. Create a 2x3 NumPy array filled with ones.
4. Create a 4x4 identity matrix using NumPy.
5. Generate an array of 5 random integers between 0 and 100.
6. Reshape a 1D array of length 6 into a 2x3 2D array.
7. Concatenate two 1D arrays `[1, 2, 3]` and `[4, 5, 6]` into one array.
8. Given a 2D array, extract the second row (index 1) as a 1D array.
9. Given a 2D array, extract the last column as a 1D array.
10. Reverse a 1D array (i.e., make the last element first and vice versa).
11. Compute the sum of all elements in a NumPy array.
12. Compute the average (mean) of a NumPy array.
13. Find the index of the maximum value in a 1D array.
14. Determine the data type (`dtype`) of the elements of an array.
15. Convert an array of floats `[1.5, 2.3, 3.9]` to an array of integers (by truncating/flooring each element).

### Intermediate (Core Skills)

1. Add a 1D array `[1, 2, 3]` to each row of a 3x3 2D array (broadcasting the 1D array across the rows).
2. Given an array, use boolean indexing to find all elements greater than a certain value (e.g. 10).
3. Replace all negative values in an array with 0 (in-place).
4. Swap the first and second rows of a 2D array.
5. Swap the first and second columns of a 2D array.
6. Flatten a 2D array into a 1D array.
7. Given two same-length 1D arrays, stack them as rows to make a 2D array.
8. Create a 5x5 matrix where each row is `[0, 1, 2, 3, 4]`.
9. Compute the dot product of two 1D arrays (vectors).
10. Compute the matrix product of a 2x3 matrix with a 3x2 matrix.
11. Solve the system of linear equations `Ax = b` for x, given A (e.g., a 2x2 matrix) and b (length 2 array).
12. Find all unique values in an array.
13. Find the common values between two arrays.
14. Sort a NumPy array in ascending order.
15. Find the indices of the three largest values in a 1D array.
16. Calculate the Euclidean distance between two points given as NumPy arrays.
17. Given a 2D array, subtract the mean of each row from that row (zero-center each row).
18. Extract the diagonal elements of a square matrix.
19. Given a 2D array, find the indices (row, col) of the minimum value in the array.
20. Compute the mean of each column in a 2D array.

### Hard (Advanced Applications)

1. **2D Convolution:** Implement a function to apply a 3x3 filter/kernel to a 2D array (image) using only NumPy (no library convolution). For example, implement a blur or edge-detection filter via convolution.
2. **PCA Implementation:** Given a 2D dataset (as a 2D array where each row is a data sample), perform Principal Component Analysis: compute the covariance matrix and its eigenvalues and eigenvectors using NumPy, then project the data onto the top principal components.
3. **k-Means Clustering:** Implement the k-means algorithm using NumPy. Given an array of data points and an initial set of k centroids, perform iterations of assigning points to the nearest centroid and recomputing centroids.
4. **Gradient Descent (Linear Regression):** Using only NumPy, implement gradient descent to fit a linear regression model. Start with random `w` and `b`, and iteratively update them using the gradient of mean squared error.
5. **Logistic Regression:** Implement a simple logistic regression classifier using NumPy. Use the sigmoid function and gradient descent to learn the parameters on a small binary classification dataset.
6. **Conway’s Game of Life:** Given a 2D array of 0s and 1s representing a Game of Life grid, compute the next generation of the grid using NumPy (i.e., count neighbors for each cell and apply the rules, all in a vectorized manner).
7. **SVD Compression:** Use `np.linalg.svd` to compute the Singular Value Decomposition of a given matrix or image. Reconstruct the matrix using only the first *k* singular values and vectors, and compare with the original (this is a basic form of image compression).
8. Compute the **moving average** of a 1D array with window size N using NumPy (i.e., produce an array of the averages of each consecutive N-length window of the input).
9. Perform matrix multiplication of two matrices using NumPy **broadcasting and sum** (i.e., implement your own matrix multiply without using `@` or `np.dot`, by exploiting broadcasting).
10. Given two polynomials represented by coefficient arrays (e.g., `[1, 2, 3]` for $1 + 2x + 3x^2$), compute the coefficients of their product polynomial using NumPy.
11. Implement **Gaussian elimination** to solve a linear system (transform the augmented matrix to reduced row echelon form) using NumPy for array operations.
12. Compute the **determinant** of a matrix using NumPy (without using `np.linalg.det` – for instance, using a recursive approach or row reduction).
13. Given an array of 2D points, find the pair of points with the smallest Euclidean distance between them (a brute-force solution is $O(N^2)$ – use NumPy to vectorize that pairwise distance computation).
14. Implement a simple 2-layer neural network (one hidden layer) training on the XOR problem using NumPy. (Hint: use random initial weights, forward propagate, compute loss, backpropagate gradients, and update weights – all with numpy operations).
15. Given a high-resolution image represented as a 2D NumPy array, downsample it by a factor of 2 in each dimension by averaging every 2x2 block of pixels (i.e., produce a smaller image where each pixel is the average of a 2x2 block from the original).


