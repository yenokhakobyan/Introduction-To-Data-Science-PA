
# 📘 **Section 1 — The Essence of Exploratory Data Analysis (EDA)**  
*Graduate‑Level Lecturer Notes — expanded edition*

---

## 0  Pre‑class Orientation  

> **Must read**  
> * J. W. Tukey (1962) — *The Future of Data Analysis*  
> * J. W. Tukey (1977) — *Exploratory Data Analysis* (Chs 1–2)  
> **Optional enrichment**  
> * Hoaglin et al. (1983) — *Understanding Robust and Exploratory Data Analysis*  
> * Cleveland (1993) — *Visualizing Data* (esp. ch. 1)  
> * Wickham & Grolemund (2017) — *R for Data Science*, EDA chapters  

**Hands‑on prep:** clone the public *Iris* and *Ames Housing* CSVs; run `df.info()` and inspect unique counts.

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Define* EDA and justify its position in CRISP‑DM / OSEMN workflows. | Remember |
| **2** | *Contrast* EDA with confirmatory data analysis (CDA) in goals, logic, and output. | Understand |
| **3** | *Compute* classical & robust summaries (mean, median, IQR, MAD, skew, kurtosis) and interpret them. | Apply |
| **4** | *Detect* data‑quality issues (missing, outliers, miscoding) via plots & stats. | Analyse |
| **5** | *Design* a reproducible EDA pipeline (narrative + code) for an unseen dataset. | Create |

---

## 2  What *Is* EDA?  

EDA is a disciplined, **model‑agnostic interrogation** of data whose aims are to

1. **Describe** central tendency, spread, and shape.  
2. **Diagnose** data‑quality defects (missingness, implausible values).  
3. **Discover** structure: clusters, trends, nonlinear relations.  
4. **Generate** hypotheses and transformation strategies.  
5. **Validate** modelling assumptions *before* statistical inference.

Formally, given a sample $\mathbf{X}=\{x_i\}_{i=1}^n$, EDA seeks a mapping  

$$
\mathcal{E}:\mathbf{X}\;\longrightarrow\;\bigl(\text{graphics},\;\text{statistics},\;\text{domain insights}\bigr)
$$

that **maximises insight** while **minimising assumptions**.

> **Tukey principle:** *“Let the data surprise you before the model confines you.”*

---

## 3  Historical & Philosophical Context  

| Year | Milestone | Significance for EDA |
|------|-----------|----------------------|
| **1936** | Fisher publishes Iris data & linear discriminant analysis. | Provides a canonical EDA demo dataset. |
| **1962** | Tukey’s *Future of Data Analysis* lecture. | Re‑frames statistics as analytic craft, not pure math. |
| **1965–73** | Bell Labs develops PRIM‑9 & interactive graphics. | Shows computers as visual exploration engines. |
| **1977** | Tukey’s *Exploratory Data Analysis* book. | Codifies boxplot, stem‑and‑leaf, five‑number summary. |
| **1980s** | Robust statistics (Huber, Hampel) enter mainstream. | Supplies resistant estimators crucial for EDA. |
| **1990s–** | Grammar‑of‑Graphics (Wilkinson) → ggplot2, seaborn. | Standardises declarative plotting for reproducible EDA. |

**Philosophy:** EDA is *inductive* (data ➔ questions); CDA is *deductive* (questions ➔ tests).

---

## 4  EDA ⇄ CDA — Complementarity  

| Dimension | **Exploratory** | **Confirmatory** |
|-----------|-----------------|------------------|
| Logic | Inductive, data‑driven | Deductive, hypothesis‑driven |
| Primary verbs | *Look, reveal, question* | *Test, estimate, decide* |
| Assumptions | Minimal (robust stats) | Explicit distributional forms |
| Typical outputs | Clean data, visuals, hypothesis log | $p$‑values, CIs, model fit metrics |
| Error focus | Type III / IV (wrong question) | Type I / II (false +/–) |
| Cycle | Begin project; iterate after residual checks | Follows EDA or preregistration |

A rigorous workflow oscillates: **EDA → provisional model → residual EDA → refined model → CDA**.

---

## 5  Quantitative Toolkit  

For numeric vector $\{x_i\}$:

| Measure | Formula | Robust? | Use case |
|---------|---------|---------|----------|
| Mean $\bar{x}$ | $\tfrac1n\sum x_i$ | ✗ | Symmetric distributions |
| Median $\tilde{x}$ | 50‑th percentile | ✔ | Skewed / heavy‑tail |
| Trimmed mean $\bar{x}_{(\alpha)}$ | Mean of middle $(1-2\alpha)n$ obs. | ✔ | Compromise robustness + efficiency |
| Variance $s^{2}$ | $\tfrac1{n-1}\sum(x_i-\bar{x})^{2}$ | ✗ | Spread (interval / ratio) |
| Skewness $\gamma_1$ | $\tfrac{\frac1n\sum(x_i-\bar{x})^{3}}{s^{3}}$ | – | Asymmetry gauge |
| Excess Kurtosis $\gamma_2$ | $\tfrac{\frac1n\sum(x_i-\bar{x})^{4}}{s^{4}}-3$ | – | Tail heaviness |

**Outlier rule:** flag if  
$$
x < Q_1 - 1.5\,\text{IQR}\quad\text{or}\quad x > Q_3 + 1.5\,\text{IQR}.
$$

---

## 6  Graphical Toolkit  

| Intent | Plot | Python Snippet |
|--------|------|----------------|
| Univariate shape | Histogram + KDE | `sns.histplot(x, kde=True)` |
| Robust summary | Box / Violin | `sns.boxplot(x)` |
| Normality check | Q–Q plot | `stats.probplot(x, plot=plt)` |
| Bivariate relation | Scatter / Hexbin | `sns.scatterplot(x,y,alpha=.4)` |
| Multivariate scan | Pair‑plot | `sns.pairplot(df, corner=True)` |

> **Design mantra:** *One plot → one clear question → one take‑away.*

---

## 7  Worked Mini‑Example — *Iris*  

```python
import seaborn as sns, pandas as pd, numpy as np, scipy.stats as st, matplotlib.pyplot as plt
iris = sns.load_dataset("iris")

# Structure
display(iris.info())

# Robust & classical summaries
num = iris.select_dtypes(float)
summ = num.agg(['count','mean','median','std','mad','skew','kurt']).T.round(3)
display(summ)

# Pairwise exploration
sns.pairplot(iris, hue="species", corner=True, diag_kind="kde")
plt.tight_layout()
```

**Insights**

* Petal dims highly collinear ($r{>}0.95$) → consider PCA or drop one.  
* `sepal_width` skewed left; median < mean.  
* Species clusters separable in petal sub‑space → LDA seems promising.

---

## 8  Why Start with EDA?  

| Risk avoided by EDA | Illustration |
|--------------------|--------------|
| **Garbage‑in model** | Null‑filled `last_login` leads to useless predictor. |
| **Hidden imbalance** | 90 % “no‑churn” → accuracy illusion. |
| **Type mismatch** | ZIP codes averaged (!) if stored numeric. |
| **Assumption breach** | Heavy‑tailed residuals violate OLS errors. |
| **Resource waste** | Hours training model that fails basic sanity checks. |

> *“EDA is the cheapest insurance against model disaster.”*

## ❓ Review Questions (with Answers)

### Q1. What does EDA stand for, and what is its primary purpose?

**Answer**: EDA stands for Exploratory Data Analysis. Its purpose is to understand data by summarizing its main characteristics, often using visualization.

---

### Q2. Who introduced the concept of EDA and in what year?

**Answer**: John W. Tukey introduced the concept. He published "The Future of Data Analysis" in 1962 and later the book *Exploratory Data Analysis* in 1977.

---

### Q3. What is a key philosophical difference between EDA and classical statistics?

**Answer**: EDA is about exploration and discovering patterns in data without rigid assumptions, while classical statistics focuses on confirmatory analysis and hypothesis testing.

---

### Q4. In what ways can EDA help improve the modeling phase of a data science project?

**Answer**: It helps identify issues such as missing values, outliers, or incorrect data types and guides feature selection and transformation.

---

### Q5. Consider this code: `iris.describe()`. What information does this provide?

**Answer**: It returns descriptive statistics for numeric columns, including count, mean, std deviation, min, quartiles, and max.

---

### Q6. What are the dangers of skipping EDA?

**Answer**: Skipping EDA can lead to using incorrect models, drawing false conclusions, or missing critical insights due to unclean or misunderstood data.

---

### Q7. Fill in the blank: EDA is to \_\_\_\_, as Classical Statistics is to \_\_\_\_.

**Answer**: EDA is to exploration, as Classical Statistics is to confirmation.

---

### Q8. Which of the following is NOT a goal of EDA?

a) Generating hypotheses
b) Confirming statistical significance
c) Understanding distributions
d) Spotting anomalies

**Answer**: **b)** Confirming statistical significance — that's part of inferential statistics, not EDA.

---

### Q9. Suppose you are given a dataset with hundreds of features. What should you do first?

**Answer**: Perform EDA to examine the structure, types, distributions, and relationships of the features. This can guide dimensionality reduction or feature engineering.

---

### Q10. Describe a real-world example where EDA might reveal an issue before modeling begins.

**Answer**: In a medical dataset, EDA might reveal that a “0” in the blood pressure column was used to indicate missing data, which could distort averages or model assumptions if not discovered early.

---


## 11  Key Take‑aways  

* **EDA is mandatory groundwork**; no shortcut exists.  
* Combine **robust** and **classical** summaries, guided by domain context.  
* Document every finding & cleaning step — EDA’s narrative is part of the deliverable.  

> *“Approximate answers to the right question beat precise answers to the wrong one.”* — **Tukey**

---



# 🧱 **Section 2 — Data Types & Data Structures in Exploratory Data Analysis**  

---

## 0   Pre-class Preparation  

> **Read / Skim**  
> * Tukey (1977) §2 “Types of variables”  
> * Wickham (2014) — *Tidy Data* (JSS)  
> * VanderPlas (2016) §3-4 — pandas dtypes and reshaping  
> * ISO/IEC 11404 — General-purpose Datatypes (for the curious)

---

## 1   Learning Outcomes  

| # | Outcome (Bloom level) |
|---|-----------------------|
| **1** | *Identify* and correctly label variable scales: nominal, ordinal, interval, ratio, binary (Remember). |
| **2** | *Differentiate* between **rectangular**, **relational**, **hierarchical**, and **tidy-long** data structures (Understand). |
| **3** | *Evaluate* how variable type and structure constrain legitimate statistics and plots (Analyse). |
| **4** | *Transform* data between wide ↔ long, string ↔ categorical, etc., with justified EDA goals (Apply/Create). |
| **5** | *Diagnose* and resolve dtype problems that silently corrupt analysis (e.g., numeric stored as object) (Evaluate). |

---

## 2   Why Data *Type* and *Structure* Matter  

> **Rule of analysis:** *The semantic level (what the numbers mean) dictates the mathematical level (what operations are valid).*

* **Type → statistic:** Variance is defined for *interval/ratio* variables but not for *nominal* ones.  
* **Type → plot:** A violin plot on a binary variable collapses to two spikes; misleading.  
* **Structure → inference frame:** A panel (time × units) violates i.i.d. assumptions; independence tests must account for clustering.  
* **Memory & speed:** `category` vs `object` can cut RAM ×10 and accelerate group-bys.

Failing to respect these constraints leads to *Type III* errors — answering the wrong question with great precision.

---

## 3   Variable–Scale Taxonomy  

> We adopt Stevens’ (1946) four measurement scales, plus binary as a practical subset.

| Scale | Mathematical Permitted | Example | Typical Summary | Canonical Plot |
|-------|-----------------------|---------|-----------------|----------------|
| **Nominal** | =, ≠ | Country, Gene ID | Mode, count | Bar / Mosaic |
| **Ordinal** | <, > | Likert (1–5) | Median, IQR | Ordered bar, Box |
| **Interval** | +, – | Temperature (°C) | Mean, SD | Histogram, KDE |
| **Ratio** | ×, ÷ (true 0) | Income, Length | CV, Geometric mean | Density, Violin |
| **Binary** | ∈{0,1} | Churn Yes/No | Proportion, Log-odds | Bar, ROC |

### ⚠ Edge Cases  
* **Zip codes** look numeric but are nominal.  
* **ISO dates** are ordinal *and* interval (differences have meaning) → use `datetime64`.  
* **Ratings 1–5**: treat as ordinal for EDA; treat as interval only with justification.

---

## 4   Data-Structure Spectrum  

| Structure | Form | JSON/Pandas Analogue | EDA Considerations |
|-----------|------|----------------------|--------------------|
| **Rectangular** | Single flat table | `DataFrame` | Assume one observation per row; check duplicates & primary key. |
| **Relational** | Multiple tables linked by keys | Merged `DataFrame`s / SQL | Must ensure referential integrity before joins; guard against *fan-out* duplicates. |
| **Hierarchical / Nested** | Lists-of-lists, arrays-of-structs | `dict` ↔ `DataFrame.explode()` | Aggregation levels (e.g., customer → orders → items) matter for variance formulas. |
| **Panel / Time-Series** | Wide (units × time) or Long (stacked) | MultiIndex `DataFrame` | Autocorrelation & seasonality diagnostics supplant simple histograms. |
| **Tidy-Long** | Each variable = column; each observation = row; each value = cell | `pd.melt`, `pivot_longer` | Maximises grammar-of-graphics compatibility; eases faceting & group summarise. |

> **Best practice:** *Perform EDA in tidy-long form; store in relational form; pivot wide only for modelling that demands matrices.*

---

## 5   Practical dtype Management in `pandas`  

```python
import pandas as pd, numpy as np

df = pd.read_csv("ames.csv")

# Detect suspicious types
suspect = df.select_dtypes("object").nunique()
print(suspect[suspect < 30].head())

# Convert high-cardinality strings to category
for col in ["Neighborhood", "MS_Zoning"]:
    df[col] = df[col].astype("category")

# Binary cast: 'Yes'/'No' → bool
df["CentralAir"] = df["CentralAir"].map({"Y": True, "N": False})

# Ordinal encoding with explicit order
qual_order = ["Po","Fa","TA","Gd","Ex"]
df["KitchenQual"] = pd.Categorical(df["KitchenQual"], ordered=True, categories=qual_order)

# Date parsing
df["SaleDate"] = pd.to_datetime(df["YrSold"].astype(str) + "-01-01")
```

🗒 **Notes**  
* `category` preserves labels yet stores as integer codes — ideal for group-by speed.  
* Boolean columns plot as 0/1 on numeric axes unless explicit maps are used in Seaborn (“hue”).  
* `pd.to_datetime` must succeed *before* resampling time-series.

---

## 6   Visual Grammar by Type  

| Goal | Variable Type(s) | Recommended Plot | Code Hint |
|------|-----------------|------------------|-----------|
| Distribution shape | Continuous | `histplot`, `kdeplot`, `rugplot` | `sns.histplot(x, bins='auto', kde=True)` |
| Category frequency | Nominal/Binary | `countplot`, Mosaic | `sns.countplot(y=var, order=...)` |
| Ordinal dispersion | Ordinal + Continuous | Ordered `boxplot`/`violin` | `sns.boxplot(x=ord, y=metric, order=ord_levels)` |
| Correlation | Continuous × Continuous | Scatter/Hexbin, `sns.lmplot` | `sns.scatterplot(alpha=.4)` |
| Categorical correlation | Nominal × Nominal | Stacked bar, Chi-square mosaic | `pd.crosstab` + `.plot(kind='bar', stacked=True)` |
| Time trend | DateTime × Metric | `lineplot`, Seasonal-decomp | `sns.lineplot(x='date', y='sales')` |

### 👉 Design Principle  
*“Map variable semantics to *aesthetic channels* that preserve scale information”* — e.g., colour for nominal, y-position for ratio.

---

## 7   Mini Case — Reshaping for Insight  

**Task:** Compare monthly SalePrice trends across *three* top neighborhoods.

```python
top3 = df["Neighborhood"].value_counts().head(3).index
tmp = (df[df["Neighborhood"].isin(top3)]
       .groupby(["SaleDate","Neighborhood"])["SalePrice"].median()
       .reset_index())

sns.lineplot(data=tmp, x="SaleDate", y="SalePrice", hue="Neighborhood")
plt.title("Median SalePrice Over Time")
```

*If data were in “wide” form (dates as index, neighborhoods as columns), `melt` would be required first.*

---

## 9   Review & Self-Test (answers hidden below fold)  

<details><summary>Click to reveal A-1 … A-10</summary>

| # | Question | Key Answer |
|---|----------|------------|
| 1 | Name two statistics valid for ordinal but *not* nominal data. | Median, IQR. |
| 2 | Why is coefficient of variation undefined for temperature °C? | CV requires a true zero → ratio scale; °C is interval. |
| 3 | Identify the dtype bug: `df['Age'].mean()` returns `nan`. | Age stored as object w/ non-numeric tokens (“Unknown”) → coerce errors. |
| 4 | Which plot best shows bivariate association between two binaries? | 2×2 heat-map of joint frequencies (or stacked bar). |
| 5 | In pandas, what’s the memory win of `category` over `object` for a 50-level factor? | Roughly 8 bytes per row → 8 × n savings (factor code stored as int8). |
| 6 | Explain how panel structure violates i.i.d. | Rows share temporal autocorrelation within entity; errors correlated. |
| 7 | Ordinal encoding: why might `LabelEncoder` be dangerous? | Imposes arbitrary numeric spacing; trees fine, linear models misled. |
| 8 | Why use `MultiIndex` for cross-section × time? | Enables hierarchical group-by (`level=`), preserves panel alignment. |
| 9 | What pandas method converts wide → tidy-long? | `pd.melt` (or `wide_to_long`). |
| 10 | When merging tables, how to detect one-to-many explosion? | After join, compare row-count versus left table; or inspect duplicate keys. |

</details>

---

## 10   Key Take-aways  

* **Type drives tool:** The statistical **verbs** you may apply depend on the data’s **noun**.  
* **Structure drives assumption:** Choose reshaping that aligns with the *independence structure* you intend to assume.  
* **Start with semantics, end with storage:** Declare meaning first (nominal, ordinal) then cast dtype (`category`, `int8`, etc.).  
* **Document every coercion**; silent dtype conversion is a primary source of analytic error.

> *“Data has a deep grammar; analysis respects that grammar or pays a correction penalty later.”*

---

## ❓ Review Questions (with Answers and Explanations)

### Q1. What is the difference between continuous and discrete numeric data?

**Answer**: Continuous data can take any value within a range (e.g., weight), while discrete data takes specific integer values (e.g., number of pets).

* ✏️ *Explanation*: This distinction affects the use of histograms vs bar charts.

---

### Q2. Is it appropriate to compute the mean of a categorical variable? Why or why not?

**Answer**: No. Categorical variables represent labels or groups without numeric meaning. Averaging them is meaningless.

* ✏️ *Example*: Taking the mean of “Red”, “Blue”, “Green” is invalid.

---

### Q3. Which type of data is most appropriate for a violin plot?

**Answer**: A continuous variable grouped by a categorical or ordinal variable.

* ✏️ *Explanation*: Violin plots show distribution within each group.

---

### Q4. What data type is the “Yes/No” field typically considered as?

**Answer**: Binary (a special case of categorical).

* ✏️ *Note*: It may be stored as boolean or 0/1 in software.

---

### Q5. You have an ‘education level’ column: "High School", "College", "Graduate". What kind of variable is this?

**Answer**: Ordinal — there is a natural order.

* ✏️ *Important*: We must define this order explicitly in `pandas`.

---

### Q6. How does `pandas` differentiate between categorical and ordinal data?

**Answer**: Using `pd.Categorical(..., ordered=True)` to define ordinal levels.

* ✏️ *Without ordering*, it's just a nominal category.

---

### Q7. Which of the following visualizations best suits a categorical variable?

a) Histogram
b) Scatterplot
c) Bar chart
d) Line plot

**Answer**: **c)** Bar chart

---

### Q8. Suppose you have a column "Rating" with values from 1 to 5. Should this be treated as numeric or ordinal?

**Answer**: Ordinal — though it's numeric, the values represent ranks or levels, not distances.

* ✏️ *Note*: Use boxplots, not means with standard deviation bars.

---

### Q9. You find a column stored as `object` in pandas, but it contains "0" and "1" as values. What's your first step?

**Answer**: Convert to `int` or `bool`, depending on context.
python
df['col'] = df['col'].astype(int)
---

### Q10. What’s the risk of treating ordinal data as continuous in modeling?

**Answer**: The model assumes equal spacing between values (e.g., 1 and 2, 2 and 3), which may not be true. This can distort predictions.



# 📊 **Section 3 — Descriptive Statistics: Location, Spread & Shape**  

---

## 0  Pre-class Orientation  

> **Core reading**  
> * Hoaglin, Mosteller & Tukey (1983) — *Understanding Robust and Exploratory Data Analysis*, ch. 2–4  
> * Wilcox (2017) — *Introduction to Robust Estimation & Hypothesis Testing*, ch. 1–2  
> **Optional**  
> * Freedman, Pisani & Purves (2007) — *Statistics*, ch. 4–5 (spread & outliers)  
> * VanderPlas (2016) — *Python Data Science Handbook*, §3.4 (aggregation)  

**Hands-on prep:** Compute `df.describe()` on *Ames Housing* and note which statistics are missing for skewed variables (e.g. `SalePrice`).

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Compute* and interpret classical & robust location metrics (mean, geometric mean, median, trimmed mean). | Apply |
| **2** | *Quantify* variability using variance, SD, IQR, MAD, coefficient of variation (CV). | Apply |
| **3** | *Assess* distribution shape via skewness & kurtosis and link to transformation choices. | Analyse |
| **4** | *Explain* robustness, breakdown point, and efficiency trade-offs for each statistic. | Understand |
| **5** | *Select* appropriate statistics & plots given scale, skew, and outlier profile of a variable. | Evaluate |

---

## 2  Why Summarise?  

Large tables are cognitively opaque. **Descriptive statistics** compress high-dimensional data into **informative scalars** that:

* Provide *context* for domain experts (e.g., “typical house sells for \$180 k”).  
* Flag data-quality problems rapidly (negative ages, implausible ranges).  
* Guide model design (log-transform when CV ≫ 1, choose Poisson vs Normal).  
* Enable fair comparisons across groups (location & spread side-by-side).  

> **Rule:** *Summaries are not substitutes for plots, but partners.*  
> Always pair numbers with a graphic so shape ≠ lost.

---

## 3  Measures of **Location**  

### 3.1 Arithmetic Mean  

$$
\bar{x}=\frac1n\sum_{i=1}^{n}x_i,\qquad
\operatorname{SE}(\bar{x})=\frac{s}{\sqrt{n}}
$$

*Maximum likelihood* estimator of μ under Normality; **0 % breakdown point** (one extreme value can move it arbitrarily).

---

### 3.2 Median (50-th percentile)  

$$
\tilde{x}=x_{(k)},\quad k=\left\lceil\frac{n}{2}\right\rceil
$$

Breakdown = 50 %; unbiased for symmetric distributions; L1-optimal.

---

### 3.3 Trimmed & Winsorised Means  

* α-trim: discard α % on each tail, average remainder.  
* α-winsor: replace tail values with nearest kept quantiles, then average.

Balance **robustness vs efficiency**; common α = 0.10 or 0.20.

```python
from scipy.stats import trim_mean, mstats
trim = trim_mean(x, 0.1)
wins = mstats.winsorize(x, limits=0.1).mean()
```

---

### 3.4 Geometric & Harmonic Means  

For **positive ratio-scale** data.

$$
\text{g-mean}= \bigl(\prod x_i\bigr)^{1/n},\qquad
\text{h-mean}= \frac{n}{\sum 1/x_i}
$$

Useful for growth rates, log-normal data, and performance benchmarks.

---

### 3.5 Mode  

Most frequent value; trivial for continuous data (estimate via KDE argmax).

---

## 4  Measures of **Spread**  

| Statistic | Formula | Robust? | Notes |
|-----------|---------|---------|-------|
| **Range** | $\max - \min$ | ✗ | Sensitive, quick sanity check |
| **Variance** $s^{2}$ | $\displaystyle \tfrac{1}{n-1}\sum (x_i-\bar{x})^{2}$ | ✗ | Additive; units² |
| **Std. Dev.** $s$ | $\sqrt{s^{2}}$ | ✗ | Same units; compare to mean |
| **Coeff. of Var.** $\text{CV}=s/\bar{x}$ | — | ✗ | Scale-free; requires ratio scale |
| **IQR** | $Q_{0.75}-Q_{0.25}$ | ✔ | 25 % breakdown; box-plot hinge |
| **MAD** | $\displaystyle \operatorname{median}\!\bigl(\lvert x_i-\tilde{x}\rvert\bigr)$ | ✔ | 50 % breakdown; × 1.4826 ≈ σ (Normal) |
| **Gini Mean Abs. Diff.** | $\displaystyle \frac{1}{n(n-1)}\sum_{i<j}\lvert x_i-x_j\rvert$ | – | Inequality metric |


### 4.1 Chebyshev & Empirical Rule  

* Any distribution: ≥ $1-1/k^{2}$ of data within $k$ SDs (Chebyshev).  
* Normal: ≈ 68–95–99.7 % within 1–2–3 SDs.  
Large deviations from these heuristics signal skew/heavy tails.

---

## 5  Shape Diagnostics  

| Metric | Definition | Interpretation |
|--------|------------|----------------|
| **Skewness** $\gamma_1$ | $\tfrac{\tfrac1n\sum(x-\bar x)^3}{s^3}$ | + ve = right-tail, – ve = left-tail |
| **Excess Kurtosis** $\gamma_2$ | $\tfrac{\tfrac1n\sum(x-\bar x)^4}{s^4}-3$ | > 0 heavy-tailed; < 0 light-tailed |

Rules of thumb: |γ₁| > 1 → strongly skewed; γ₂ > 3 → leptokurtic (fat tails).

---

## 6  Robustness Table  

| Metric | Breakdown % | Relative Efficiency (Normal) |
|--------|-------------|------------------------------|
| Mean | 0 | 100 % |
| 10 % Trimmed Mean | 10 | 95 % |
| Median | 50 | 64 % |
| MAD (σ-scaled) | 50 | 37 % |
| IQR/1.349 | 25 | 74 % |

> **Trade-off:** higher breakdown ⇒ lower efficiency under strict Normality.

---

## 7  Python Workflow Example — *Ames Housing SalePrice*  

```python
import pandas as pd, seaborn as sns, numpy as np, scipy.stats as st
df = pd.read_csv("AmesHousing.csv")

y = df["SalePrice"].dropna()

# Location
loc = {
    "mean": y.mean(),
    "median": y.median(),
    "trim10": st.trim_mean(y, 0.1),
    "g_mean": np.exp(np.log(y).mean())
}

# Spread
spread = {
    "sd": y.std(),
    "IQR": y.quantile(.75)-y.quantile(.25),
    "MAD": (y - y.median()).abs().median()*1.4826,
    "CV": y.std()/y.mean()
}

print(pd.Series(loc).round(1))
print(pd.Series(spread).round(1))

# Visual
sns.boxenplot(x=y, whis=1.5)
sns.despine(); plt.title("SalePrice: log-axis"); plt.xscale("log")
```

**Findings**

* CV ≈ 0.30 ⇒ moderate relative dispersion.  
* γ₁ ≈ 1.5 ⇒ strong right skew; log-transform advisable (as plotted).  
* Trimmed mean (log-scale) aligns with median ⇒ robust to luxury-home outliers.

---

## 8  Decision Heuristics  

| Symptom | Preferred Summary | Rationale |
|---------|------------------|-----------|
| Heavy right-tail (γ₁ ≫ 0) | Median, log-mean | Resistant & interpretable |
| Many 0’s + positives | Geometric mean on shifted data | Handles multiplicative scale |
| Discrete counts (0–10) | Mode, median, IQR | Mean less intuitive |
| Ratio comparison (two groups) | CV, fold-change of medians | Scale-free |

---



## 11  Key Take-aways  

* **Location + Spread + Shape** = minimal sufficient narrative for a single numeric feature.  
* Robust statistics (median, IQR, MAD) guard against outliers at small efficiency cost.  
* Shape metrics inform transformation and model family choice.  
* Always **visualise** alongside numbers to avoid summary fallacies.

---
## ❓ Review Questions (with Explanations)

### Q1. What’s the key difference between mean and median?

**Answer**: The mean averages all values; the median is the middle value. The median is less affected by extreme values (outliers).

---

### Q2. If a dataset contains significant outliers, which measure is more appropriate: mean or median?

**Answer**: Median — it’s robust and not distorted by extremes.

---

### Q3. True or False: The standard deviation is always greater than the mean.

**Answer**: False. There’s no fixed rule. It depends on the dataset.

---

### Q4. What does the IQR tell us?

**Answer**: It shows the spread of the middle 50% of the data — the difference between the 75th and 25th percentiles.

---

### Q5. Why is the standard deviation not considered robust?

**Answer**: Because it squares deviations from the mean — so outliers have a disproportionate impact.

---

### Q6. How is MAD (Median Absolute Deviation) calculated?

**Answer**: It’s the median of the absolute differences between each value and the dataset’s median.

---

### Q7. You observe a mean of 60 and a median of 40. What does this tell you?

**Answer**: The data is likely **right-skewed** — a few large values are pulling the mean up.

---

### Q8. What plot type best visualizes central tendency and spread together?

**Answer**: A boxplot — it shows median, quartiles, and outliers.

---

### Q9. What’s a trimmed mean useful for?

**Answer**: It reduces the effect of extreme values while still using most of the data — a compromise between mean and median.

---

### Q10. How does the variance relate to standard deviation?

**Answer**: The standard deviation is the **square root** of the variance.

---

## 🧠 Optional Exercise

Download the Titanic dataset from Seaborn:
python
titanic = sns.load_dataset("titanic")
Choose one continuous feature (e.g., `age` or `fare`) and:

1. Compute mean, median, std, IQR, and MAD.
2. Plot a histogram and a boxplot.
3. Briefly interpret: Is the variable skewed? Are there outliers? What’s a better summary: mean or median?

---

# 📊 **Section 4 — Visualising Univariate Distributions**  

---

## 0  Pre-class Orientation  

> **Read / Skim**  
> * Cleveland (1993) — *Visualizing Data*, ch. 2 (univariate displays)  
> * Scott (1992) — *Multivariate Density Estimation*, §3.1 (bin-width rules)  
> * Wand & Jones (1995) — *Kernel Smoothing*, ch. 2 (KDE foundations)  
> **IDE setup**: Ensure `seaborn >= 0.13`, `matplotlib >= 3.8` and enable retina rendering:  
> `from matplotlib import rcParams; rcParams['figure.dpi']=150`

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Choose* the appropriate univariate plot (histogram, KDE, box, violin, ECDF) given data scale & sample size. | Evaluate |
| **2** | *Interpret* modality, skewness, kurtosis and outliers directly from graphics. | Analyse |
| **3** | *Explain* the mathematical basis of histograms (binning) and KDE (kernel-bandwidth). | Understand |
| **4** | *Apply* data-driven rules (Freedman–Diaconis, Scott) to select histogram bins; apply plug-in methods for KDE bandwidth. | Apply |
| **5** | *Integrate* visual and numeric evidence (mean, IQR, MAD) into a coherent distribution narrative. | Create |

---

## 2  Why Visualise?  

* Numbers **compress**; plots **reveal**.  
* Descriptive stats can *match* across datasets while shapes diverge (Anscombe’s quartet).  
* Graphics expose model assumptions (Normality, homoscedasticity) **before** fitting.  
* Stakeholders digest pictures faster than tables.

> **Tukey:** “The simple graph has brought more information to the data analyst’s mind than any other device.”

---

## 3  Simulated Playground  

```python
import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt, scipy.stats as st
rng = np.random.default_rng(42)

normal  = rng.normal (loc=50, scale=10, size=1_000)
skewed  = rng.exponential(scale=10, size=1_000)
bimodal = np.concatenate([rng.normal(40,5,600), rng.normal(60,4,400)])
df = pd.DataFrame({'normal':normal, 'skewed':skewed, 'bimodal':bimodal})
```

---

## 4  Histograms — Frequency Binning  

### 4.1 Anatomy  

* Divide range into **bins** $(b_j,b_{j+1})$.  
* Height = count / width (density=True) or raw count.  
* Visualises *empirical density* $\hat f(x)=\frac{\text{count}_j}{n\,h}$, where $h$ = bin width.

```python
sns.histplot(df['normal'],  bins='fd', stat='density', color='#4271AE')
plt.axvline(df['normal'].mean(), ls='--', lw=1, c='k'); plt.title("Normal • Freedman–Diaconis bins")
```

### 4.2 Bin-width Rules  

| Rule | Formula | Use Case |
|------|---------|----------|
| **Sturges** | $k=⌈\log_2 n+1⌉$ | n < 200 |
| **Rice** | $k=⌈2n^{1/3}⌉$ | quick default |
| **Scott** | $h=3.5s n^{-1/3}$ | near-Normal |
| **Freedman–Diaconis** | $h=2·\text{IQR} n^{-1/3}$ | skewed / heavy-tail |

> **Guideline:** display histograms with `stat='density'` when comparing groups of unequal size.

---

## 5  Kernel Density Estimate (KDE)  

A smooth estimator:

$$
\hat f_h(x)=\frac1{nh}\sum_{i=1}^n K\!\left(\frac{x-x_i}{h}\right),
$$
where $K$ is a kernel (often Gaussian) and $h$ is bandwidth.

### 5.1 Bandwidth Selection  

| Method | Idea | `seaborn` call |
|--------|------|----------------|
| Silverman | $h=0.9\min(s,\text{IQR}/1.34)n^{-1/5}$ | default |
| Scott     | $h=1.06 s n^{-1/5}$                  | `bw_method='scott'` |
| Plug-in   | Minimise MISE                         | `bw_adjust` param |

```python
sns.kdeplot(df['bimodal'], bw_method='scott', fill=True, alpha=.3)
```

### 5.2 Pros & Cons  

| ✔ reveals multimodality | ✘ sensitive to bandwidth |
|-------------------------|--------------------------|
| ✔ smooth visual appeal  | ✘ less intuitive area≈1  |
| ✔ easy overlay compare  | ✘ misleading w/ n < 50   |

---

## 6  Box, Violin, Boxen  

| Plot | Shows | Sample-size sweet spot | `seaborn` |
|------|-------|------------------------|-----------|
| **Box** | median, IQR, Tukey outliers | 20 – 20 000 | `boxplot` |
| **Boxen** | quantile strips (≈ letter-value) | n > 1 000 | `boxenplot` |
| **Violin** | KDE + quartiles | n > 100 | `violinplot` |

```python
sns.violinplot(y=df['skewed'], inner='quartile', color='#EE854A')
plt.title("Skewed • Violin with quartiles")
```

> **Interpretation tip:** In violin plots, width is density; mirror halves are symmetric by design.

---

## 7  Additional Univariate Displays  

| Plot | Strength | Code hint |
|------|----------|-----------|
| **Rug / Strip** | Raw datapoints, n < 200 | `sns.stripplot` |
| **ECDF** | Exact cumulative distribution; good for small n | `sns.ecdfplot` |
| **Ridgeline** | Many groups, density stacked | `sns.kdeplot(..., multiple='stack')` |
| **Dot plot** | Categorical counts, discrete numeric | `sns.countplot` |

---

## 8  Case Study — *Titanic* Age  

```python
t = sns.load_dataset("titanic").dropna(subset=['age'])
age = t['age']

fig, ax = plt.subplots(1,3, figsize=(12,3))
sns.histplot(age, bins='fd', ax=ax[0]);      ax[0].set_title('Histogram')
sns.boxplot(  x=age, ax=ax[1]);              ax[1].set_title('Box')
sns.kdeplot(   age, fill=True, ax=ax[2]);    ax[2].set_title('KDE')
plt.tight_layout()
```

**Visual diagnosis**

* Right-skew visible in histogram + KDE.  
* Boxplot median ≈ 28; whisker high tail; several 70 + outliers.  
* Suggest log or square-root transform if modelling *duration-like* outcomes.

Cross-check numeric skew:

```python
from scipy.stats import skew
skew(age, bias=False)   # ≈ 0.4 moderate
```

---

## 9  Choosing the Right Plot — Decision Grid  

| Data size | Goal | Recommended | Why |
|-----------|------|-------------|-----|
| n < 50 | Inspect every point | Strip + box | Avoid over-smoothed KDE |
| 50 ≤ n ≤ 500 | Outliers & centre | Box + histogram (`bins='fd'`) | Robust + frequency |
| n > 500 | Shape & tails | Boxen + KDE | Density smooth; letter-values show deep quantiles |
| Discrete integers (0–10) | Mass at few points | Bar/count plot | KDE inappropriate |
| Heavy skew (γ₁>1) | Compare groups | Violin on log scale | Removes long-tail compression |

---

## 10  Active-Learning Prompt  

> **Task:** For variable `SalePrice` (Ames Housing) plot **histogram**, **KDE**, **boxen** on original and log10 scale.  
> 1. Which scale better reveals modality?  
> 2. Does log transform change outlier identification?  
> 3. Summarise in ≤ 3 sentences which plot convinced you most and why.

(8 min breakout; reconvene.)

---

## ❓ Review Questions (with Explanations)

### Q1. What does the length of the box in a boxplot represent?

**Answer**: The interquartile range (IQR) — the spread of the middle 50% of the data.

---

### Q2. If your histogram is very jagged, what might be the issue?

**Answer**: The bin size is too small. Increase the number of observations per bin or reduce the number of bins.

---

### Q3. What does KDE stand for, and what does it estimate?

**Answer**: Kernel Density Estimate. It estimates the probability density function of a continuous variable.

---

### Q4. Which of the following best identifies outliers?

a) Histogram
b) KDE plot
c) Boxplot
d) Mean

**Answer**: **c)** Boxplot

---

### Q5. What would a bimodal KDE plot suggest?

**Answer**: The data may contain two subgroups (e.g., two populations with different means).

---

### Q6. What is the main disadvantage of KDE plots on small samples?

**Answer**: They can produce misleading shapes due to over-smoothing or under-smoothing.

---

### Q7. True or False: A histogram can detect skewness in the data.

**Answer**: True — skewness is often visible in the shape of the bars.

---

### Q8. Which plot type shows the median explicitly?

**Answer**: Boxplot

---

### Q9. If a variable has many repeated values (e.g., 1s and 2s), which plot works better: KDE or histogram?

**Answer**: Histogram — KDE assumes a continuous distribution and may be misleading.

---

### Q10. In a KDE plot, what does “bandwidth” control?

**Answer**: The amount of smoothing applied. Larger bandwidth = smoother curve; smaller bandwidth = more jagged.

---

## 12  Key Take-aways  

* **Histograms** expose raw frequency; **bin width matters** — use data-driven rules.  
* **KDE** yields smooth shape; choose bandwidth carefully and avoid when n is tiny.  
* **Box / Violin / Boxen** condense centre, spread, tails; great for group contrasts.  
* Combine numeric (skew, IQR) and visual evidence to narrate distribution behaviour.



# 🔗 **Section 5 — Exploring Relationships Between Variables**  

---

## 0  Pre-class Orientation  

> **Read / Skim**  
> * Cleveland (1993) — *Visualizing Data*, ch. 3–4 (scatterplots & conditioning)  
> * Friendly (2002) — “Corrgrams: Exploratory displays for correlation matrices” (JCGS)  
> * Schabenberger & Pierce (2002) — *Contemporary Statistical Models for the Plant and Soil Sciences*, §2.3 (interaction plots)  
> **Code prep**  
> * Install `statsmodels`, `pingouin` (partial & partial-correlation helpers).  
> * For big-N demos consider `datashader` or set seaborn theme:  
> ```python
> import seaborn as sns; sns.set_theme(context='talk', style='whitegrid')
> ```

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Select* the appropriate graphic for numeric–numeric, numeric–categorical, categorical–categorical relationships. | Evaluate |
| **2** | *Compute* and interpret Pearson, Spearman, and point-biserial correlations, including 95 % CIs. | Apply |
| **3** | *Diagnose* multicollinearity and confounding visually (scatterplot matrix, coloured residuals). | Analyse |
| **4** | *Explain* pitfalls such as over-plotting, lurking variables, and Simpson’s paradox. | Understand |
| **5** | *Integrate* graphical & numerical evidence into feature-selection or hypothesis narratives. | Create |

---

## 2  Why Explore Relationships?  

* **Feature engineering** — uncover interactions, non-linear transformations.  
* **Model diagnostics** — spot heteroscedasticity or non-additive effects before regression.  
* **Scientific discovery** — test substantive theory (e.g., height–weight allometry).  
* **Data validation** — find impossible joint combinations (e.g., infant age > 20 yrs).  

> **Principle:** *Univariate EDA is necessary; bivariate/multivariate EDA is decisive.*

---

## 3  Typology of Variable Pairs  

| X           | Y           | Goal                        | Recommended Plots (Seaborn alias)                                                          |
|-------------|-------------|-----------------------------|--------------------------------------------------------------------------------------------|
| Numeric     | Numeric     | Trend, linearity, outliers  | Scatter (`scatterplot`), Lowess (`regplot lowess=True`), Hexbin/Contour (`hexbin`, `kde`)  |
| Categorical | Numeric     | Group differences           | Box / Violin (`boxplot`, `violinplot`), Strip + Swarm (`stripplot`, `swarmplot`)            |
| Categorical | Categorical | Association / dependence    | Stacked bar (`countplot`, `hue=`), Mosaic, Heat-map of crosstab (`heatmap`)                |

---

## 4  Numeric ⊗ Numeric  

### 4.1 Scatterplot With Enhancements  

```python
iris = sns.load_dataset("iris")
sns.scatterplot(x='sepal_length', y='petal_length',
                hue='species', style='species',
                data=iris, alpha=.7)
sns.regplot(x='sepal_length', y='petal_length',
            data=iris, scatter=False, color='k', ci=None)
plt.title("Iris • Sepal vs Petal Length")
```

**Visual cues**

* Cluster separation ⇒ classification signal.  
* Linear overlay suggests Pearson correlation appropriate.

### 4.2 Pearson & Spearman  

$$
r_{xy}=\frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum (x_i-\bar x)^2\sum(y_i-\bar y)^2}}, \qquad
\rho_s=\operatorname{corr}(\operatorname{rank}(x),\operatorname{rank}(y)).
$$

```python
from scipy.stats import pearsonr, spearmanr
pearsonr(iris['sepal_length'], iris['petal_length'])
spearmanr(iris['sepal_length'], iris['petal_length'])
```

### 4.3 Big-N Over-plotting  

```python
import datashader as ds, datashader.transfer_functions as tf
import pandas as pd, numpy as np
N = 1_000_000
df = pd.DataFrame({'x': np.random.randn(N),
                   'y': np.random.randn(N)+np.random.randn(N)*0.2})
canvas = ds.Canvas(plot_width=450, plot_height=400)
agg = canvas.points(df, 'x', 'y')
tf.shade(agg, cmap='viridis')
```

*Hexbin alternative:*

```python
plt.hexbin(df['x'], df['y'], gridsize=60, cmap='viridis',
           mincnt=5); plt.colorbar(label='count')
```

---

## 5  Numeric ⊗ Categorical  

### 5.1 Grouped Box & Violin  

```python
titanic = sns.load_dataset("titanic").dropna(subset=['fare','class'])
sns.boxenplot(x='class', y='fare', data=titanic, showfliers=False)
sns.stripplot(x='class', y='fare', data=titanic, color='k', alpha=.3, jitter=.2)
plt.yscale('log'); plt.title("Titanic Fare • log-scale")
```

*Interpretation*: median fare rises sharply by class; heavy right-tail within each box suggests fare heterogeneity.

### 5.2 Effect Size & Assumption Check  

*Visual →* numeric:

```python
import pingouin as pg
pg.anova(dv='fare', between='class', data=titanic)
```

Heteroscedastic spread visible → consider Welch ANOVA or non-parametric Kruskal–Wallis.

---

## 6  Categorical ⊗ Categorical  

### 6.1 Mosaic / Stacked Bars  

```python
ct = pd.crosstab(titanic['sex'], titanic['survived'])
ct_norm = ct.div(ct.sum(axis=1), axis=0)

ct_norm.plot(kind='bar', stacked=True, colormap='Accent')
plt.ylabel("Proportion"), plt.title("Survival Proportion by Sex")
```

### 6.2 $\chi^2$ Test of Independence  

```python
from scipy.stats import chi2_contingency
chi2, p, dof, exp = chi2_contingency(ct)
print(f"χ²={chi2:.2f}, p={p:.3g}")
```

Large χ² / small p ⇒ evidence of association.

---

## 7  Correlation Matrix & Corrgram  

```python
num_cols = iris.select_dtypes(float)
corr = num_cols.corr(method='pearson')
sns.heatmap(corr, annot=True, cmap='vlag', center=0)
plt.title("Iris • Pearson Corrgram")
```

**Friendly’s corrgram** idea: sort variables via hierarchical clustering to reveal block-structure.

---

## 8  Pairplot & Conditional Plots  

```python
sns.pairplot(iris, hue='species', corner=True, diag_kind='kde',
             plot_kws=dict(alpha=.5, edgecolor='none'))
```

*Conditioning*:

```python
sns.lmplot(x='age', y='fare', hue='sex', data=titanic,
           scatter_kws=dict(alpha=.4))
```

Conflicting slopes between strata vs pooled ⇒ **Simpson’s paradox**.

---

## 9  Multicollinearity Diagnostics  

| Tool | Insight | Python |
|------|---------|--------|
| Pearson | pairwise $|r|>0.8$ suspect | `.corr()` |
| Variance Inflation Factor (VIF) | $>5$ problematic | `statsmodels.stats.outliers_influence.variance_inflation_factor` |
| Condition Number | large ⇒ near-collinearity | `np.linalg.cond(X)` |

**Mitigation**: drop redundant predictor, PCA, or ridge regression.

---

## 10  Practical Heuristics  

| Symptom | Remedy |
|---------|--------|
| Over-plotting | α-blending, hexbin, datashader |
| Hidden subgroup trend | Colour/shape by categorical, facet grids |
| Non-linear pattern | Transform (log, sqrt), add polynomial term, use Spearman |
| Unequal group sizes | Plot **proportions**, not raw counts |
| Spurious high $r$ | Inspect scatter; rule out leverage points |

---

## 11  Active-Learning Prompt  

> **Task:** Using *Ames Housing*, explore `GrLivArea` vs `SalePrice`.  
> 1. Plot scatter on linear and log-log scales with a Lowess smoother.  
> 2. Compute Pearson & Spearman correlations before and after log transform.  
> 3. Does transformation improve linearity & homoscedasticity? Summarise.


---

## ❓ Review Questions (with Explanations)

### Q1. What type of plot best shows the relationship between two numeric variables?

**Answer**: Scatterplot or regression plot

---

### Q2. You want to compare `fare` distributions across `class` levels in Titanic data. What plot do you choose?

**Answer**: Boxplot or Violin Plot

---

### Q3. What does a correlation coefficient of –0.85 indicate?

**Answer**: A strong negative linear relationship between two variables.

---

### Q4. True or False: A correlation of 0 means the two variables are independent.

**Answer**: False — correlation only measures linear relationships. There could be a non-linear relationship.

---

### Q5. What kind of plot would you use to display counts of combinations of two categorical variables?

**Answer**: Grouped bar chart or a heatmap of a crosstab

---

### Q6. What problem can occur with scatterplots on large datasets?

**Answer**: Overplotting — too many overlapping points. Use hexbin or transparency.

---

### Q7. When comparing distributions between groups, why might violin plots be preferable to boxplots?

**Answer**: Violin plots show the **full distribution**, not just median and quartiles.

---

### Q8. What does it mean if a KDE in a pairplot shows multiple peaks?

**Answer**: The variable may be multimodal — likely indicating subgroups.

---

### Q9. What is Simpson’s Paradox?

**Answer**: A trend in aggregate data reverses when data is divided into groups — due to a lurking variable.

---

### Q10. What’s the risk of relying solely on `.corr()` for insights?

**Answer**: It ignores:

* Categorical relationships
* Non-linear patterns
* Confounding variables

---

## 13  Key Take-aways  

* **Choose graphics by pair-type**: scatter/hex (num-num), box/violin (cat-num), mosaic/heat-map (cat-cat).  
* Augment visuals with **appropriate correlation statistics** (Pearson, Spearman, χ²).  
* **Stratify** to avoid Simpson’s paradox; inspect residual plots for non-linearity.  
* Address **multicollinearity** early to prevent unstable models.

---

## 14  Section 6 tackles **Data-Quality Diagnostics**


---

## 0  Pre-class Orientation  

> **Read / Skim**  
> * Karr & Sanil (2008) — *Data Quality: A Statistical Perspective* (Technometrics)  
> * van der Laan (2017) — “Missing Data: Broken Records” (chapter in *Targeted Learning*)  
> * Wickham (2014) — *Tidy Data* §5 (inconsistencies & cleaning)  
> **Tool install**  
> * `pip install missingno pyjanitor great_expectations`  
> Enable pandas display helpers:  
> ```python
> pd.set_option('display.max_rows',   120)  # audit-wide
> pd.set_option('display.float_format', '{:.3f}'.format)
> ```

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Detect* patterns of missingness, type mis-assignments, duplicates, and outliers in a DataFrame. | Analyse |
| **2** | *Differentiate* MCAR, MAR, MNAR and articulate modelling consequences. | Understand |
| **3** | *Apply* robust and model-based outlier detection (IQR, MAD, Isolation Forest). | Apply |
| **4** | *Design* a cleaning pipeline that preserves information while minimising bias. | Create |
| **5** | *Document* data-quality issues using validation frameworks (Great Expectations) for reproducibility. | Evaluate |

---

## 2  Why Data-Quality Matters  

* **Validity** — flawed inputs propagate to flawed inference (“garbage in → garbage out”).  
* **Trust** — stakeholders must believe the dataset and, by extension, your conclusions.  
* **Cost** — 80 % of analysis time is cleaning; early detection prevents downstream re-work.  
* **Ethics** — unrepresentative or wrong data can harm people (e.g., biased credit scores).

> **Statistical imperative:** *Model assumptions rarely hold when data-quality assumptions fail.*

---

## 3  Taxonomy of Data-Quality Defects  

| Category | Manifestation | Typical Cause | First-line Tools |
|----------|---------------|---------------|------------------|
| **Missingness** | NaN, blank, “9999” | Non-entry, sensor loss | `.isna()`, *missingno* |
| **Type Drift** | “12.5” as object | Mixed formats | `.to_numeric(errors='coerce')` |
| **Inconsistency** | “NY”, “New York”, “new york” | Human entry | `.str.lower()`, fuzzy-match |
| **Duplicates** | Repeated rows / keys | ETL join error | `.duplicated()` |
| **Outliers** | |x − μ| > 3σ; leverage points | Entry slip, rare cases | Boxplot, z/MAD-score |
| **Anomalies** | Impossible combos (e.g., age = −4) | Logic error | Cross-field rules |
| **Encoding** | Mojibake, � | Wrong file encoding | `errors='replace'` |

---

## 4  Missing-Data Mechanics  

| Mechanism | Definition | Consequence | Remedies |
|-----------|------------|-------------|----------|
| **MCAR** | Missing Completely At Random; probability independent of data. | List-wise deletion unbiased. | Drop rows; simple impute. |
| **MAR** | Missing At Random given observed vars. | Bias if ignored. | Multiple imputation, model-based. |
| **MNAR** | Missing Not At Random (informative). | Serious bias. | Sensitivity analysis; explicit modelling. |

*Test heuristics:* Little’s MCAR test (`pingouin.mcar`). Visual missingness patterns (`missingno.matrix`, `.heatmap`).

```python
import missingno as msno
msno.dendrogram(df)      # clusters variables by co-missingness
```

---

## 5  Diagnostics in Python  

### 5.1 Null Report  

```python
audit = (df.isna()
           .mean()
           .mul(100)
           .round(1)
           .sort_values(ascending=False)
           .to_frame('pct_null'))
display(audit.head(10))
```

> **Rule of thumb:** columns ≥ 40 % missing require justification to keep.

### 5.2 Type Audit  

```python
suspect = df.select_dtypes('object').applymap(type).nunique()
display(suspect[suspect > 1])   # mixed types inside object cols
```

### 5.3 Duplicate & Key Integrity  

```python
duplicates = df.duplicated().sum()
key_dups   = df.duplicated(subset=['id']).sum()
```

### 5.4 Outlier Hunt  

* **IQR/MAD** (robust):  
  ```python
  med = y.median(); mad = (y-med).abs().median()
  mod_z = 0.6745*(y-med)/mad
  out = y[mod_z.abs()>3.5]
  ```

* **Isolation Forest** (model-based, multivariate):  
  ```python
  from sklearn.ensemble import IsolationForest
  iso = IsolationForest(contamination=0.01, random_state=0)
  df['anomaly'] = iso.fit_predict(df[num_cols])
  ```

Visual check: `sns.scatterplot(hue='anomaly', ...)`

---

## 6  Cleaning Strategies & Bias Trade-offs  

| Defect | Naïve Fix | Risk | Preferred Workflow |
|--------|-----------|------|--------------------|
| High null col | Drop column | Lose signal | Assess correlation w/ target; consider modelling missingness as category |
| Null numeric | Mean impute | Under-dispersion | Median / KNN / MICE |
| Duplicates | Drop all dup rows | Lose legitimate repeats | Verify business key uniqueness; keep last? |
| Outlier | Delete | Remove rare but real | Winsorise, log-transform, robust model |
| String type numeric | `astype(float)` | Coerce fails silently | `pd.to_numeric(errors='raise')` + test |

> **Best practice:** record every cleaning decision in a *data-prep log* (YAML/markdown) for auditability.

---

## 7  Validation Frameworks  

### 7.1 Great Expectations  

```python
import great_expectations as ge
gdf = ge.from_pandas(df)

gdf.expect_column_values_to_not_be_null('age')
gdf.expect_column_values_to_be_in_set('sex',['male','female'])
gdf.validate()
```

Outputs human-readable *data-quality report* (JSON / HTML).

### 7.2 pytest-based Unit Tests  

```python
def test_age_range():
    assert df['age'].between(0, 120).all()
```

CI integration prevents silent regressions in pipelines.

---

## 8  Context: When an Outlier Is a Data Point of Interest  

* Fraud detection — anomalous transactions drive the *signal*.  
* Extreme weather — rare events matter for climate models.  
* Medical extremes — early disease onset valuable for research.

**Guideline:** *Flag, label, model; rarely discard outright.*

---

## 9  Active-Learning Prompt  

> **Dataset:** *Ames Housing*  
> 1. Compute % missing for every column; plot top 15 with bar chart.  
> 2. Use `missingno.heatmap` to identify variable clusters of missingness.  
> 3. Apply Isolation Forest on `GrLivArea`, `SalePrice` and visualise anomalies.  
> 4. Recommend a cleaning & imputation plan (≤ 5 bullet points).

(10 min group exercise; class discussion.)

---

## 10  Self-Test (answers hidden)  

<details><summary>Reveal answers</summary>

| Q | Key Answer |
|---|------------|
| 1 | `.isnull().sum()` counts missing per column. |
| 2 | Blind row-dropping may bias if MAR/MNAR. |
| 3 | False — investigate first; outliers may be valid. |
| 4 | Convert with `pd.to_numeric(errors='coerce')`. |
| 5 | Inconsistent case; normalise via `.str.lower()`. |
| 6 | Mean-impute shrinks variance, biases correlations. |
| 7 | Library: *missingno*. |
| 8 | Detect duplicate rows by subset; e.g. primary key. |
| 9 | |z| > 3 or |modified Z| > 3.5 flags outlier. |
|10| Replace with sentinel “Unknown” or mode; possibly add missing-flag column. |
</details>

---

## 11  Key Take-aways  

* **Detect → diagnose → decide**: tri-phase approach to any defect.  
* Missing-data mechanism (MCAR/MAR/MNAR) dictates the *valid* remedy.  
* Robust statistics and anomaly models complement visual inspection.  
* Logging & validation frameworks turn ad-hoc cleaning into reproducible science.

---
## ❓ Review Questions (with Explanations)

### Q1. What does `df.isnull().sum()` do?

**Answer**: It counts the number of missing (null or NaN) values in each column.

---

### Q2. What is the risk of blindly dropping rows with missing values?

**Answer**: You may remove informative or systematically missing data, leading to biased results.

---

### Q3. True or False: All outliers should be removed from the dataset.

**Answer**: False — outliers may be valid and informative. Investigate before removal.

---

### Q4. If you see “25” as a string in a numeric column, what should you do?

**Answer**: Convert the column using `pd.to_numeric(..., errors='coerce')`.

---

### Q5. You discover three values in a column: “Yes”, “yes”, and “YES”. What’s the issue?

**Answer**: Case inconsistency. Normalize using `.str.lower()` or `.str.upper()`.

---

### Q6. What’s the danger in filling missing values with the mean?

**Answer**: It may reduce variance and distort statistical relationships.

---

### Q7. Which library provides visual summaries of missing data patterns?

**Answer**: `missingno`

---

### Q8. What does `df.duplicated(subset=[...])` help you identify?

**Answer**: Duplicate rows based on a subset of columns.

---

### Q9. How would you flag outliers based on z-score?

**Answer**: Compute z-scores and flag those with `|z| > 3`.

---

### Q10. What is the best strategy for handling missing values in a categorical column?

**Answer**: Often, replace with “Unknown” or the mode — but depends on the context.

---

# 🧪 **Section 7 — End-to-End EDA Case Study**  
*From raw Titanic data to reproducible insights*

---

## 0  Pre-class Orientation  

> **Read / Skim**  
> • Wickham & Grolemund (2017) — *R for Data Science*, ch. 25 “Case Study”  
> • Peng (2020) — *Exploratory Data Analysis with R*, ch. 12 “Case Study”  
> • van der Laan & Rose (2011) — *Targeted Learning*, §2.1 (causal caveats)  
>
> **Tooling checklist**  
> * seaborn ≥ 0.13 — ships the **Titanic** dataset  
> * missingno, pingouin, pyjanitor — for quality audit & effect size  
> * `titanic = sns.load_dataset("titanic").copy()`  

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Execute* a complete EDA pipeline on a messy real-world dataset. | Apply |
| **2** | *Integrate* univariate, bivariate, multivariate graphics with numeric diagnostics into a coherent story. | Create |
| **3** | *Justify* cleaning and imputation choices in terms of bias–variance trade-offs. | Evaluate |
| **4** | *Communicate* insights to technical and non-technical stakeholders using concise narrative & visuals. | Synthesise |
| **5** | *Document* the workflow for reproducibility (scripts, notebook, data-prep log). | Evaluate |

---

## 2  Case-Study Brief  

> **Research Question**  
> *Which passenger characteristics most strongly associate with survival on the RMS Titanic, and how trustworthy is the dataset for predictive modelling?*

---

## 3  Step 0 — Problem Framing  

1. **Unit of analysis**: individual passenger.  
2. **Target**: `survived` (binary).  
3. **Candidate predictors**: demographics (`age`, `sex`), socio-economic (`class`, `fare`), family context (`sibsp`, `parch`).  
4. **Stakeholder deliverable**: concise insight deck + cleaned CSV.

---

## 4  Step 1 — Data Ingestion & Structure Check  

```python
import seaborn as sns, pandas as pd, numpy as np, matplotlib.pyplot as plt
titanic = sns.load_dataset("titanic").copy()
display(titanic.shape)          # (891, 15)
titanic.info(memory_usage='deep')
```

*Observations*

* Mixed dtypes: float (`age`), int (`sibsp`), bool (`adult_male`), object (`embark_town`).  
* `deck` has 77 % nulls; `age` ≈ 20 % null.  
* Potential identifiers: `survived` unique? (No — target). No true unique row key → add if needed.

---

## 5  Step 2 — Data-Quality Audit  

### 5.1 Missingness

```python
import missingno as msno
msno.matrix(titanic, figsize=(9,4))
msno.heatmap(titanic)
```

* `deck` MCAR?— actually MAR: first-class cabins more likely to be recorded.  
* `age` missing patterns align with `class` + `sex` → MAR.

### 5.2 Duplicates & Type Drift

```python
titanic.duplicated().sum()      # 0 → good
titanic.select_dtypes('object').applymap(type).nunique().max()  # 1 → no mixed types
```

### 5.3 Outlier Scan

```python
sns.boxplot(x=titanic['fare']); plt.xscale('log')
```

* Fares ≥ £500: extreme but legitimate (ticket group bookings). Flag, don’t drop.

**Decision log**

| Action | Rationale |
|--------|-----------|
| Drop `deck` (77 % null) | Too sparse; collinear with `class`. |
| Impute `age` by **median within `sex × class` strata** | MAR; preserves distribution shape. |
| Add boolean `fare_outlier` (fare > 300) | Retain information without truncation. |
| Convert `embarked` missing → “Unknown” | Categorical sentinel. |

```python
# Stratified median impute
titanic['age'] = titanic.groupby(['sex','class'])['age']\
                        .transform(lambda s: s.fillna(s.median()))
```

---

## 6  Step 3 — Univariate Exploration (Post-Clean)  

| Variable | Numeric Summary | Graphic |
|----------|-----------------|---------|
| `age` | mean = 30.0, median = 28, IQR = 20–38 | Histogram (bins = FD) |
| `fare` | median = 14.5, MAD ≈ 12 | Log-histogram & boxen |
| `class` | 1st = 216, 2nd = 184, 3rd = 491 | Bar plot |

*Skew alerts*: `fare` γ₁ ≈ 4.8 ⇒ log-transform candidate.

```python
titanic['log_fare'] = np.log1p(titanic['fare'])
```

---

## 7  Step 4 — Bivariate Relationships  

| Pair | Plot | Numeric |
|------|------|---------|
| `sex` × `survived` | Stacked proportion bar | φ = 0.54 (large) |
| `class` × `survived` | Mosaic | χ² p < 1e-50 |
| `age` → `survived` | Violin by outcome | AUC ≈ 0.62 |
| `log_fare` → `survived` | Boxen by outcome | Cliff’s Δ ≈ 0.42 |

```python
sns.catplot(x='sex', hue='survived', kind='count',
            data=titanic, height=4, aspect=1)
```

*Women & children first* pattern visually obvious.

---

## 8  Step 5 — Multivariate Synthesis  

### 8.1 Correlation Heat-map (numeric only)

```python
num = titanic.select_dtypes(['float','int'])
sns.heatmap(num.corr(method='spearman'), cmap='vlag', center=0, annot=True)
```

* No pair above |ρ| = 0.55 → multicollinearity low.  
* `fare` correlates with `class` (encoded 1–3) at −0.55 (ordinal vs numeric).  

### 8.2 PairGrid Stratified

```python
g = sns.PairGrid(titanic,
                 vars=['age','log_fare','family_size'],
                 hue='survived')
g.map_diag(sns.kdeplot, fill=True)
g.map_offdiag(sns.scatterplot, alpha=.4)
g.add_legend()
```

Clusters: survivors cluster age < 40 + medium fare.

---

## 9  Step 6 — Feature Engineering  

| Feature | Formula | Motivation |
|---------|---------|------------|
| `family_size` | `sibsp + parch + 1` | Group context |
| `is_alone` | `family_size == 1` | Binary simplification |
| `class_num` | map {1st:1, 2nd:2, 3rd:3} | Ordinal numeric |

```python
titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1
titanic['is_alone'] = (titanic['family_size'] == 1)
```

`is_alone` vs survival bar plot shows solo travellers fared worse than small families (size 2–4).

---

## 10  Step 7 — Exploratory Baseline Model (optional)  

```python
import statsmodels.formula.api as smf
logit = smf.logit('survived ~ C(sex) + C(class) + age + np.log1p(fare) + is_alone',
                  data=titanic).fit()
logit.summary2().tables[1]
```

* Female odds ≈ 7× male.  
* 3rd-class odds ≈ 0.1 relative to 1st.  
* Age OR < 1 (older lower survival).  
* Good sanity check of EDA insights; **not** final model.

---

## 11  Step 8 — Insight Synthesis (Executive Slide)  

| Theme | Evidence | Business Take-away |
|-------|----------|--------------------|
| **Demography** | Female survival ≈ 74 % vs male ≈ 19 % | Life-boat priority adhered; gender critical predictor. |
| **Socio-economic** | 1st-class survival ≈ 63 %; 3rd ≈ 24 % | Cabin location / crew access matter more than fare alone. |
| **Age** | Survivors median = 28 vs non-survivors = 28 (!) but tails differ | Children (< 10) advantaged; elderly disadvantaged. |
| **Family context** | Solo travellers survival = 30 % vs small family 45 % | Travel party size interacts with rescue logistics. |

---

## 12  Narrative & Communication Tips  

* **Lead with a hook**: “Only 1 in 5 third-class men survived.”  
* **Use faceting** to compare survival across class × sex grids (heat-map).  
* **Annotate** bars with % to avoid misreading counts.  
* **Caption wisely**: every figure answers a question (avoid chart dumping).  
* **Acknowledge limitations**: observational, historical data; missing `deck` may bias cabin analyses.

---

## 13  Active-Learning Prompt  

> **Task:** Draft *two* “headline” slides for a C-level audience:  
> 1. Insight slide with a single persuasive visual.  
> 2. Risk slide discussing data limitations & next steps.  
> (15 min; peer feedback.)

---

## 14  Self-Test (answers hidden)  

<details><summary>Reveal answers</summary>

| Q | Key Answer |
|---|------------|
| 1 | Missing `age` (~20 %), `deck` (~77 %), `embarked` (2). |
| 2 | Gender, class, log-fare, family context. |
| 3 | Median robust to skew/outliers. |
| 4 | Grouped bar (`sex` × `survived`). |
| 5 | > 70 % null → low information, drop or external enrich. |
| 6 | Pair-plot shows clusters & linearity across variables simultaneously. |
| 7 | Fare positively correlated with 1st class and higher survival. |
| 8 | Combine `sibsp`+`parch` to capture social unit size effect. |
| 9 | Causality cannot be claimed; unobserved confounding. |
|10| Narrative turns raw analysis into compelling, actionable insight. |
</details>

---

## 15  Key Take-aways  

* End-to-end EDA unites **cleaning, visualisation, statistics, and narrative**.  
* **Decision logs** track every edit to maintain reproducibility.  
* Insights must tie back to the original research or business question.  
* EDA sets the stage for modelling; it does *not* finish the science.

---



# 🧾 **Section 8 — Best-Practice Playbook & EDA Reporting Templates**  
*From exploratory notebook to polished, shareable insight document*

---

## 0  Pre-class Orientation  

> **Read / Skim**  
> * Bryan (2018) — *Workflow: projects* (in *Happy Git & GitHub for Data Science*)  
> * Rule et al. (2019) — “Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks” (*PLoS Comp Bio*)  
> * Stodden et al. (2016) — *Enabling Reproducibility for Computational Research* (NAS report)  
>
> **Tools to install**  
> * `pandas-profiling` (now **ydata-profiling**) — one-click EDA HTML  
> * `great_expectations` — data-validation framework  
> * `nbstripout`, `jupytext`, `papermill` — notebook hygiene & parametrisation  

---

## 1  Learning Outcomes  

| # | Outcome | Bloom |
|---|---------|-------|
| **1** | *Design* a report structure that is clear, modular, and reproducible. | Create |
| **2** | *Apply* coding, narrative, and visual best-practices in notebooks / markdown docs. | Apply |
| **3** | *Differentiate* audiences and tailor EDA outputs accordingly (exec, analyst, peer reviewer). | Evaluate |
| **4** | *Integrate* automated EDA tools (profilers, validation) into the reporting workflow. | Analyse |
| **5** | *Critique* common anti-patterns (plot dumping, hidden state, magic numbers). | Evaluate |

---

## 2  Why Presentation Equals Impact  

Data work that is **opaque** or **irreproducible**  
= *scientific liability*.  

A professional EDA deliverable should be:

| Principle | Manifestation |
|-----------|---------------|
| **Readable** | Logical flow, headlines that scan. |
| **Reliable** | Deterministic execution (set seeds, pin package versions). |
| **Reproducible** | Single `make all` or notebook *Restart & Run All* reproduces every figure. |
| **Action-oriented** | Insights lead to decisions; next steps are explicit. |
| **Audience-aware** | Exec summary first; code details collapsible / appendix. |

> **Tagline:** *“If a colleague cannot rerun your notebook tomorrow and get the same story, it isn’t done.”*

---

## 3  Report Anatomy—Recommended Outline  

```markdown
# 📄 Project Title (One-sentence problem statement)

## 1. Executive Synopsis  ← write last, place first
- 3-5 bullet insights
- 1 headline figure (small multiple or annotated bar)
- Recommendation(s) & caveats

## 2. Data & Provenance
- Source, extraction date, licence
- Unit of analysis, key identifiers
- Version hash or DVC snapshot ID

## 3. Data-Quality Audit
- Missingness table + heat-map
- Type coercions & validation summary (Great Expectations)
- Decision log for drops / imputations

## 4. Univariate Exploration
- Bullet list (+ inline sparkline or histogram) per major feature
- Skew / transformation candidates flagged

## 5. Relationships & Drivers
### 5.1 Bivariate Highlights
### 5.2 Multivariate Diagnostics (heat-maps, pair-plots)
- Multicollinearity table (VIF)
- Simpson’s paradox checks

## 6. Feature Engineering
- New variables, rationale, code snippet
- Distribution check post-engineering

## 7. Key Findings & Business Implications
- Narrative paragraphs supported by 1–2 succinct visuals
- Risks / limitations

## 8. Repro Appendix
- `sessionInfo()` / `pip freeze`
- Folder tree, data dictionary
- Full code (optionally collapsed code-folds)
```

---

## 4  Best-Practice Checklist  

| Area | ✔ Do | ✖ Avoid |
|------|------|--------|
| **Objectives** | State *research / business question* upfront. | “Exploring for fun” with no goal. |
| **Code** | Modular functions, `src/` folder, `%load_ext autoreload`. | Repeating copy-paste cells; long global notebook state. |
| **Narrative** | Explain *why* each plot exists. | Dumping 15 unlabeled charts in a row. |
| **Version control** | Track notebook with `nbstripout` or `jupytext --pair py`. | Committing 25-MB checkpoints to Git. |
| **Repro** | Use `requirements.txt` or `environment.yml`; pin seeds. | Relying on implicit global RNG state. |
| **Visuals** | Title, axis labels, annotation of key numbers. | Microscopic fonts; rainbow colourmap. |
| **Decisions** | Log cleaning steps in YAML or in-notebook table. | Silent coercions / drops. |
| **Actionability** | Close with *next steps / recommendations*. | Ending with “TODO later”. |

---

## 5  Audience-Driven Variants  

| Audience | Deliverable | Tone / Length | Tips |
|----------|-------------|---------------|------|
| **C-suite** | 2-slide PDF (Executive Summary) | High-level | Lead with “Why it matters” KPI. |
| **Product Mgr** | Slide deck + appendix | Mid-level | Link insights to user stories. |
| **Data Team** | Parametrised Jupyter notebook (`.ipynb` or `.py`) | Detailed | Emphasise code clarity & reproducibility. |
| **Peer Review** | Quarto / R-Markdown article | Academic | Cite sources, include robustness checks. |

---

## 6  Automation Helpers  

| Tool | Strength | Quick Start |
|------|----------|-------------|
| **ydata-profiling** | 1-click HTML of missingness, correlations, sample rows | `df.profile_report().to_file("report.html")` |
| **Great Expectations** | Formal “tests” for data contracts | `great_expectations init` → checkpoints |
| **Sweave / Quarto** | Combine prose + code for PDF/HTML | `quarto render` |
| **Papermill** | Notebook parameterisation (e.g., different date ranges) | `papermill template.ipynb output.ipynb -p start '2024-01-01'` |

Automate *templated* EDA for recurring datasets (e.g., weekly ingestion).

---

## 7  Anti-Patterns & Red Flags  

| Smell | Why It Hurts |
|-------|--------------|
| **Hidden State**: running cells out of order changes output. | Breaks reproducibility; “it works on my laptop” syndrome. |
| **Magic Numbers**: `df.drop(df.columns[[0,3,7]], axis=1)` | Opaque; use names or constants. |
| **Chartjunk**: Gradients, 3-D pie charts. | Distracts, misleads perception. |
| **Wall-of-Text** Markdown. | Readers skim; emphasise with bullets & headings. |
| **Over-claiming** causal inference in EDA. | Violates exploratory scope; erodes credibility. |

---

## 8  Template Repository Skeleton  

```
project/
├── data/
│   ├── raw/         # untouched downloads
│   └── processed/   # after cleaning
├── notebooks/
│   ├── 01_eda.ipynb
│   └── 99_report.ipynb
├── src/
│   ├── cleaning.py
│   └── visuals.py
├── reports/
│   ├── eda_report.html
│   └── executive_summary.pdf
├── requirements.txt
└── README.md
```

*Use Makefiles or `nox` for automated “clean → analyse → report” pipelines.*

---

## 9  Active-Learning Prompt  

> **Scenario:** Your notebook contains 60 executed cells with mixed outputs.  
> **Task:**  
> 1. Refactor into **max 20 cells** (parameter, import, data load, QA, univariate, bivariate, findings, save).  
> 2. Strip ephemeral outputs (`nbstripout`).  
> 3. Add a 150-word executive synopsis at top.  
> *(15 min hands-on; share before/after.)*

---

## 10  Self-Test (answers hidden)  

<details><summary>Reveal answers</summary>

| Q | Key Answer |
|---|------------|
| 1 | Objective / question statement. |
| 2 | Transparency; reviewers judge bias trade-offs. |
| 3 | False — EDA can inform data quality audits, research discovery. |
| 4 | Bullet insights improve scan-ability and retention. |
| 5 | Missing *interpretation* and link to question. |
| 6 | Outliers may signal fraud / rare events; assess context first. |
| 7 | Findings, limitations, recommended next steps. |
| 8 | Modular code, functions, clear headings, parameterisation. |
</details>

---

## 11  Key Take-aways  

* **Narrative + Evidence** beats code dump; lead with *why*, not *how*.  
* Maintain **reproducibility hygiene**: deterministic execution, pinned environments, stripped outputs.  
* Craft deliverables for **specific audiences**; one size never fits all.  
* Transparency in decisions builds trust; document *what, why, how*.  

> *“Readable, repeatable, reusable — the three R’s of a professional EDA report.”*

---

## 12  Capstone Prompt  

Compile your Titanic (or chosen) analysis into **two artifacts**:

1. `eda_notebook.ipynb` — parametrised, clean, restart-run-all passes.  
2. `executive_summary.pdf` — 2 slides: *Insights* + *Risks / Next steps*.  

Push to GitHub with MIT licence; tag release **v1.0-eda**. Peer-review swap in next session.

---

## 13  Congratulations 🎉  

You have completed the structured EDA curriculum. Next pathway options:

* 🔮 **Predictive Modelling** — feature engineering → logistic regression → tree ensembles.  
* 🏗️ **Data Pipelines** — Airflow, dbt, Dagster for productionising EDA checks.  
* 📊 **Interactive Dashboards** — Streamlit, Plotly Dash, Observable notebooks.

Choose your adventure!


---

## 📊 Tabular Data EDA

| Notebook                                                                                     | What makes it worth studying?                                                                                   | Key take-aways you can borrow                                   |
| -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Titanic Classification** (GitHub) — full notebook plus modular `src/` folder ([GitHub][1]) | *Narrative first* (goal → audit → EDA → baseline model), tidy functions for imputation & feature engineering    | Clear decision log and final slide-style summary cell           |
| **House Prices EDA** (Kaggle) — classic Ames dataset walk-through ([Kaggle][2])              | Deep dive into skewed numeric features, categorical encoding demos, exhaustive but well-labelled plots          | Shows how to gatekeep feature leakage before modeling           |
| **Comprehensive Python EDA repo** (GitHub) ([GitHub][3])                                     | Dozens of standalone notebooks covering time-series, geospatial and tabular tasks; each with checklist markdown | Good boilerplate for “EDA playbook” scripts you can adapt       |
| **Iris Dataset EDA** (GitHub) ([GitHub][4])                                                  | Minimal but elegant—perfect for teaching pair-plots, class separation and robust summary stats                  | Illustrates concise storytelling in <30 cells                   |
| **EDA & House-Price Prediction** (Kaggle) ([Kaggle][5])                                      | Couples thorough EDA with step-wise feature selection and cross-validation; nice bridge to modeling             | Shows smooth hand-off from exploration into predictive workflow |

---

## 📝 Text Data EDA

| Notebook                                                                          | Highlights                                                                                                        | Why you might copy the pattern                                        |
| --------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **IMDB Movies EDA** (GitHub) ([GitHub][6])                                        | Mixes tabular movie metadata with **NLP-specific EDA** (n-gram clouds, TF-IDF distributions, topic sketches)      | Demonstrates dual treatment of structured & unstructured fields       |
| **Sentiment-Analysis IMDB** (GitHub) ([GitHub][7])                                | Shows cleaning pipeline (HTML strip, emoji handling) → exploratory word-frequency / length plots → model baseline | Great template for social-media or review corpora                     |
| **ContextLens (Reddit + Twitter)** (GitHub) ([GitHub][8])                         | End-to-end: raw crawl → text normalisation → EDA on sentiment score distributions and topic clusters              | Good example of fusing multiple noisy sources and logging assumptions |
| **General EDA notebooks repo** (collection includes text notebooks) ([GitHub][9]) | Variety of compact notebooks (IMDB, BBC News, Tweets) with consistent section headers                             | Useful for comparing different stylistic takes on similar tasks       |

*Patterns to notice*: token-length histograms, proportion of stop-words, rare-word cut-off curves, word-clouds only *after* numeric context.

---

## 🖼️ Image Data EDA

| Notebook                                                                  | Strengths                                                                                          | Re-usable ideas                                                     |
| ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **CIFAR-10 exploratory notebook** (GitHub) — Mark Popovich ([GitHub][10]) | Walks through dataset shapes, class balance, pixel-level summary stats, montage grids              | Shows how to build quick “look book” of random images per class     |
| **CIFAR-10 CNN notebook** (GitHub) — Vedanta Banerjee ([GitHub][11])      | Starts with EDA: RGB channel histograms, t-SNE of pre-CNN features, misclassification visual audit | Great example of **closing the loop**—EDA ⇄ model errors            |
| **Chest X-ray EDA v3** (Kaggle) ([Kaggle][12])                            | Focus on medical images: resolution checks, pathology label imbalance, DICOM meta-data heat-map    | Illustrates domain-specific QC (aspect ratio, pixel spacing)        |
| **NIH Chest X-ray EDA** (Kaggle) ([Kaggle][13])                           | Extensive use of seaborn + matplotlib grids, overlays bounding-box prevalence                      | Shows combining **image + tabular** clinical labels in one notebook |

*Look for*: per-channel mean/STD tables, class-imbalanced visualisations, augmentation preview grids (flip/rotate examples).

---

## 🚀 How to Use These Notebooks Effectively

1. **Clone, run, and read** the narrative markdown—note how authors justify each step.
2. **Lift and adapt** helper-functions (e.g., `display_random_images`, `text_stats`) into your own `utils.py`.
3. **Benchmark your EDA**: replicate one notebook on a new dataset, then add at least one extra analysis the original author missed.
4. **Compare styles**: notice how Kaggle notebooks lean toward presentation, while GitHub repos often separate logic into modules—choose what suits your workflow.

These examples collectively demonstrate *clean code, strong narrative, and reproducible structure* across three data modalities. Studying them will accelerate your ability to craft professional-grade EDA deliverables of your own.

[1]: https://github.com/Ruban2205/titanic-classification?utm_source=chatgpt.com "GitHub - Ruban2205/titanic-classification"
[2]: https://www.kaggle.com/code/dgawlik/house-prices-eda?utm_source=chatgpt.com "House Prices EDA - Kaggle"
[3]: https://github.com/drshahizan/Python_EDA?utm_source=chatgpt.com "drshahizan/Python_EDA - GitHub"
[4]: https://github.com/THAMIZH-ARASU/Exploratory-Data-Analysis-on-The-Iris-Data-Set?utm_source=chatgpt.com "Exploratory Data Analysis (EDA) of Iris Dataset - GitHub"
[5]: https://www.kaggle.com/code/siddheshpujari/eda-and-prediction-of-house-price?utm_source=chatgpt.com "EDA and Prediction of House Price - Kaggle"
[6]: https://github.com/navidyou/IMDB_Exploratory_Data_Analysis?utm_source=chatgpt.com "Exploratory Data Analysis on IMDB Movies Dataset - GitHub"
[7]: https://github.com/windi-wulandari/sentiment-analysis-IMDB?utm_source=chatgpt.com "windi-wulandari/sentiment-analysis-IMDB - GitHub"
[8]: https://github.com/KarthikUdyawar/ContextLens?utm_source=chatgpt.com "KarthikUdyawar/ContextLens: Unveiling Hidden Sentiments through ..."
[9]: https://github.com/Saba-Gul/Exploratory-Data-Analysis-and-Statistical-Analysis-Notebooks?utm_source=chatgpt.com "Saba-Gul/Exploratory-Data-Analysis-and-Statistical ... - GitHub"
[10]: https://github.com/MarkPopovich/cifar-10?utm_source=chatgpt.com "MarkPopovich/cifar-10 - GitHub"
[11]: https://github.com/vedantabanerjee/cifar.cnn?utm_source=chatgpt.com "CIFAR-10 Image Classification using Convolutional Neural ... - GitHub"
[12]: https://www.kaggle.com/code/tolgadincer/eda-chest-x-ray-images-dataset-v3?utm_source=chatgpt.com "EDA Chest X-ray Images dataset v3 - Kaggle"
[13]: https://www.kaggle.com/code/rerere/eda-nih-chest-x-rays?utm_source=chatgpt.com "EDA - NIH Chest X-rays"


## In Class examples 

1. https://www.kaggle.com/code/ash316/eda-to-prediction-dietanic
2. https://www.kaggle.com/code/deffro/eda-is-fun
3. https://www.kaggle.com/code/dgawlik/house-prices-eda
4. https://www.kaggle.com/code/mehakiftikhar/amazon-sales-dataset-eda


---

## 🟢 Easy (1 – 8)

| # | Question (choose **one** option)                                                      | a                   | b                                       | c                                       | d                      | **Ans.** | Explanation                                                      |
| - | ------------------------------------------------------------------------------------- | ------------------- | --------------------------------------- | --------------------------------------- | ---------------------- | -------- | ---------------------------------------------------------------- |
| 1 | What does `df.describe()` return for numeric columns?                                 | Counts only         | 5-number summary                        | Six summary statistics incl. mean & std | Pair-wise correlations | **c**    | `describe()` shows count, mean, std, min, 25 %, 50 %, 75 %, max. |
| 2 | Which Seaborn plot best visualises the distribution of a **single** numeric variable? | `relplot`           | `histplot`                              | `scatterplot`                           | `violinplot`           | **b**    | `histplot` (or `displot`) bins a single variable.                |
| 3 | In a box plot, what does the **line inside the box** represent?                       | Mean                | Median                                  | Mode                                    | IQR                    | **b**    | The central line is the median (50th percentile).                |
| 4 | A histogram shows a long right-hand tail.  The data are …                             | Left-skewed         | Symmetric                               | Right-skewed                            | Uniform                | **c**    | Tail to the right ⇒ positive/right skew.                         |
| 5 | Which method reveals missing-value counts **per column**?                             | `df.isnull().sum()` | `df.countna()`                          | `df.nunique()`                          | `df.value_counts()`    | **a**    | `isnull()` + `sum()` counts `True` per column.                   |
| 6 | In Pandas, `df.corr()` (default) computes which correlation?                          | Spearman            | Kendall                                 | Pearson                                 | Point-biserial         | **c**    | Pearson linear correlation is default.                           |
| 7 | Tukey’s “1.5 × IQR rule” is used to …                                                 | Pick histogram bins | Detect outliers                         | Impute nulls                            | Scale data             | **b**    | Values beyond Q1 – 1.5 × IQR or Q3 + 1.5 × IQR are flagged.      |
| 8 | `sns.pairplot(df, hue='species')` creates …                                           | One scatter plot    | Grid of pair-wise scatters & histograms | Heat-map                                | Boxplots by species    | **b**    | PairPlot = scatter-matrix + diagonal density.                    |

---

## 🟡 Medium (9 – 15)

| #  | Question                                                                                   | a                    | b                        | c                 | d                     | **Ans.** | Explanation                                                         |
| -- | ------------------------------------------------------------------------------------------ | -------------------- | ------------------------ | ----------------- | --------------------- | -------- | ------------------------------------------------------------------- |
| 9  | Code below returns what columns? <br>`df.select_dtypes(include='number').columns`          | Only object cols     | Only numeric cols        | All dtypes        | Index labels          | **b**    | Filters DataFrame to numeric dtypes.                                |
| 10 | You see a correlation heat-map with many bright red squares (r≈1). Likely issue?           | Multicollinearity    | Low variance             | MCAR nulls        | Heteroscedasticity    | **a**    | High inter-feature correlation ⇒ multicollinearity.                 |
| 11 | Which plot best shows relationship between **two dense numeric** variables (>100 k rows)?  | Simple scatter       | Hexbin plot              | Boxplot           | Pie chart             | **b**    | Hexbin aggregates points into hexagonal bins; avoids over-plotting. |
| 12 | When comparing distributions across categories, violin plots add value because they show … | Mean & SD only       | Full density shape       | Just quartiles    | Cumulative counts     | **b**    | Violin = mirrored KDE + quartiles.                                  |
| 13 | `df['log_income'] = np.log1p(df['income'])` is typically done to …                         | Make income negative | Reduce right skew        | Remove duplicates | Encode categories     | **b**    | Log transform stabilises variance, compresses high tail.            |
| 14 | MCAR, MAR, MNAR describe …                                                                 | Outlier types        | Missing-data mechanisms  | Join strategies   | Correlation levels    | **b**    | They classify patterns of missingness.                              |
| 15 | In a bar chart, starting the y-axis **above 0** risks …                                    | Simpson’s paradox    | Exaggerating differences | Hiding skew       | Inflating sample size | **b**    | Non-zero baseline distorts perceived magnitude.                     |

---

## 🔴 Hard (16 – 20)

| #  | Question                                                                                                                | a                                        | b                                                | c                             | d                          | **Ans.** | Explanation                                                             |
| -- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- | ------------------------------------------------ | ----------------------------- | -------------------------- | -------- | ----------------------------------------------------------------------- |
| 16 | Given `sns.boxplot(x='class', y='fare', data=titanic)`, the **whiskers** extend to …                                    | Max & min values                         | 5th & 95th percentiles                           | Q1–1.5 × IQR and Q3+1.5 × IQR | Mean ± 2 × SD              | **c**    | Tukey whiskers mark 1.5 × IQR beyond quartiles.                         |
| 17 | Simpson’s paradox occurs when …                                                                                         | Sampling is random                       | Aggregated trend reverses after stratification   | Histogram bins are unequal    | Skew = 0                   | **b**    | A lurking variable flips the direction of association when conditioned. |
| 18 | Code:  <br>`df.groupby('dept')['salary'].apply(lambda s: s.skew()).abs().gt(1)` returns?                                | Boolean Series of depts with strong skew | DataFrame of salaries                            | Z-scores                      | Null mask                  | **a**    | Computes abs(skew) by dept; >1 flagged `True`.                          |
| 19 | `from scipy.stats import jarque_bera; jarque_bera(df['residuals'])` tests …                                             | Equality of means                        | Normality (skew + kurtosis)                      | Homoscedasticity              | Autocorrelation            | **b**    | JB tests joint skewness & kurtosis vs Normal.                           |
| 20 | After PCA on scaled features you plot the first two PCs and see points forming a narrow diagonal line.  Interpretation? | PCs capture little variance              | Strong linear dependence among original features | Data are MCAR                 | Outliers dominate variance | **b**    | Tight diagonal → 1-dimensional structure; high collinearity originally. |

---
