

# Introduction to Data, Linear Algebra, and NumPy in AI & Machine Learning

## 1. Introduction to Data and Linear Algebra

**What is Data?** In simple terms, *data* refers to pieces of information, often represented as numbers when stored or processed by a computer. Data can come in many forms: text documents, images, audio recordings, tables of numbers, sensor readings, etc. All these forms ultimately can be encoded as numbers (for example, text characters can be encoded as numbers, image pixels as numeric intensities). In data science and machine learning, we typically organize data into structured collections like **tables**, **vectors**, or **matrices** for convenient processing.

**Why Linear Algebra?** *Linear algebra* is the branch of mathematics that deals with vectors (1D arrays of numbers), matrices (2D arrays of numbers), and linear transformations. It provides a powerful framework for handling and transforming data. In fact, linear algebra provides the mathematical **foundation** for many key data science and machine learning techniques ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=A%20solid%20understanding%20of%20linear,optimization%20and%20machine%20learning%20models)). This is because complex data (whether it’s images, text, or databases) is often represented in terms of vectors and matrices, and operations on data (like combining variables, transforming coordinates, or finding patterns) can be described with linear algebra operations.

Some reasons linear algebra is crucial for data handling and modeling:

- **Data Representation:** Most datasets can be viewed as matrices or vectors. For example, a dataset table with *m* rows (samples) and *n* columns (features) can be seen as an *m×n* matrix of numbers. Each data sample is a vector, and the whole dataset is a matrix. Linear algebra gives a language to work with these representations ([Linear Algebra Techniques in Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-techniques-in-data-science/#:~:text=1,data%20points%20based%20on%20linear)).
- **Transformations and Preprocessing:** Tasks like scaling features, rotating images, or mixing audio signals can be done through matrix operations. For instance, if we have an image represented as a matrix of pixels, applying a filter or transformation to the image can be expressed as multiplying by a transformation matrix.
- **Modeling Relationships:** Many models in machine learning are essentially linear algebra operations. A simple linear regression model, for example, can be written in matrix form. More complex models like neural networks also rely heavily on matrix operations (multiplying input vectors by weight matrices at each layer).

**Linear Algebra in Practice:** Linear algebra shows up everywhere in data science and AI:
- In **recommender systems**, large user-item datasets are represented as matrices, and linear algebra methods help generate personalized suggestions (such as how Netflix or Spotify make recommendations) ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=%2A%20Recommender%20Systems%20,relationships%20through%20linear%20algebra%20operations)).
- In **natural language processing (NLP)** for text data, words and documents are often represented as vectors. Methods like *word embeddings* (e.g. Word2Vec or GloVe) turn words into high-dimensional vectors, and linear algebra operations (dot products, matrix multiplications) are used to measure similarities or relationships between words ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=%2A%20NLP%20,dot%20products%20alongside%20matrix%20multiplication)).
- In **image processing and computer vision**, an image is essentially a matrix of pixel values. Techniques for image transformations, compression (like JPEG using linear transformations), and feature extraction (like edge detection filters) are built on linear algebra ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=includes%20both%20dot%20products%20alongside,as%20extracting%20features%20from%20datasets)).
- For **dimensionality reduction** (simplifying datasets by reducing features), methods like **Principal Component Analysis (PCA)** rely on linear algebra concepts of eigenvalues and eigenvectors to find new axes (principal components) that capture the most variance in the data. PCA transforms data into a smaller set of variables while preserving important information ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=PCA%20is%20a%20dimensionality%20reduction,feature%20extraction%20and%20noise%20reduction)).
- **Clustering and Classification** algorithms also use linear algebra under the hood. For example, *k*-means clustering can be accelerated by linear algebra operations, and Support Vector Machines use linear algebra to find optimal separating hyperplanes ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=images%20through%20various%20transformations%20and,reshaping%20data%20points%20ahead%20of)).

In short, a solid grasp of linear algebra helps you understand and work with data more effectively. It’s hard to imagine modern data science or AI without heavy use of vectors, matrices, and operations on them ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=A%20solid%20understanding%20of%20linear,optimization%20and%20machine%20learning%20models)). Throughout these notes, we will see how data can be represented with vectors/matrices and how linear algebra operations enable modeling and extracting insights from data.

## 2. Introduction to Artificial Intelligence and Machine Learning

**Artificial Intelligence (AI)** broadly refers to technologies that enable machines to mimic or simulate human intelligence. This includes abilities like learning from experience, understanding language, making decisions, and solving problems. For example, AI systems can learn to **recognize objects** in images or understand spoken **language**. A classic example is a self-driving car that can perceive its environment and make driving decisions without human intervention ([What Is Artificial Intelligence (AI)? | IBM](https://www.ibm.com/think/topics/artificial-intelligence#:~:text=Artificial%20intelligence%20,decision%20making%2C%20creativity%20and%20autonomy)). In essence, AI is about creating software and systems that exhibit *intelligent behavior* – performing tasks that would normally require human intelligence.

**Machine Learning (ML)** is a *subset of AI* that specifically focuses on algorithms and models which learn from data. Instead of being explicitly programmed step-by-step for a task, a machine learning system is "trained" on historical data and it **learns patterns** or rules from that data. It then uses those learned patterns to make predictions or decisions on new, unseen data. In other words, ML enables computers to improve their performance on a task with experience over time ([What Is Artificial Intelligence (AI)? | IBM](https://www.ibm.com/think/topics/artificial-intelligence#:~:text=Directly%20underneath%20AI%2C%20we%20have,explicitly%20programmed%20for%20specific%20tasks)). For example, rather than programming explicit rules to detect spam emails, we can feed a machine learning model many example emails (some labeled spam, some not) and the model can *learn* to distinguish them on its own.

**Real-Life Examples of AI/ML:**
- **Voice Assistants:** Virtual personal assistants like Amazon’s Alexa or Apple’s Siri use machine learning to understand spoken questions and provide answers. They employ speech recognition (converting voice to text) and natural language understanding to determine the user’s intent. It’s ML that powers tasks done by these voice assistants ([Machine Learning Examples, Applications & Use Cases | IBM](https://www.ibm.com/think/topics/machine-learning-use-cases#:~:text=It%E2%80%99s%20ML%20that%20powers%20the,the%20person%20has%20asked%20before)) – for instance, learning to recognize your speech patterns or recall context from previous queries.
- **Email Spam Filtering:** Services like Gmail use machine learning algorithms to automatically filter out spam and phishing emails. The system learns from millions of examples of spam vs. legitimate emails. Over time it improves its accuracy. In Gmail, ML models classify incoming emails into categories (Primary, Social, Promotions) and detect spam with high accuracy ([Machine Learning Examples, Applications & Use Cases | IBM](https://www.ibm.com/think/topics/machine-learning-use-cases#:~:text=ML%20algorithms%20in%20Google%E2%80%99s%20Gmail,categorize%20emails%20as%20they%20come)) without having a programmer manually code thousands of spam rules.
- **Recommender Systems:** When Netflix suggests a movie or Amazon recommends a product, that’s AI at work. The system learns from your past behavior and similar users’ data. Using ML, it finds patterns (like “users who like X also like Y”) to make personalized recommendations ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=%2A%20Recommender%20Systems%20,relationships%20through%20linear%20algebra%20operations)).
- **Facial Recognition and Vision:** Facebook’s ability to tag you in photos, or your phone unlocking using your face, are powered by ML models (often deep neural networks) trained on large datasets of images. They learn to recognize faces by processing images as numerical pixel data and identifying unique patterns for each person.
- **Healthcare and Diagnosis:** AI systems are being trained to read medical images (like X-rays or MRIs) to detect diseases, sometimes matching or exceeding human experts. For example, a machine learning model can learn to identify tumors in scans or predict diseases from patient data.
- **Customer Service Chatbots:** Many websites use AI chatbots that can answer common questions. These bots use natural language processing and learned responses to simulate a human-like conversation with customers.

---

# 🤖 Real-World Machine Learning Applications (with Images)

---

## 🗣 Voice Assistants: Alexa, Siri, Google Assistant

Virtual personal assistants like Amazon’s Alexa or Apple’s Siri use **machine learning** to understand spoken language and provide meaningful responses.  
They use:
- **Speech Recognition**: Converting your spoken words into text.
- **Natural Language Understanding (NLU)**: Interpreting the meaning and intent behind your query.

Over time, they **learn your speech patterns** and **personal preferences**, enabling smarter and more personalized interactions.

> *Example: Alexa learning to recognize your accent better after repeated use.*




---

## 📧 Email Spam Filtering: Smarter Inboxes

Email platforms like **Gmail** use machine learning algorithms to **detect spam and phishing** automatically.

- The system is trained on **millions of emails**.
- It learns to spot common spam patterns **without humans coding manual rules**.
- ML also powers **automatic email categorization** (Primary, Promotions, Social).

> *Example: Gmail detecting and moving a phishing email straight to Spam.*



---

## 🎬 Recommender Systems: Netflix, Amazon, YouTube

Recommendation engines use machine learning to **analyze user behavior** and **suggest personalized content**.

- Netflix recommends movies based on your viewing history.
- Amazon suggests products you might like.
- YouTube curates your home page videos.

These systems find **hidden patterns** like:
> *"Users who liked *Stranger Things* also liked *Dark*."*


---

## 🧑‍🦰 Facial Recognition and Computer Vision

Facial recognition systems, like:
- Facebook’s photo tagging
- Apple’s Face ID unlocking
- Surveillance and security applications

...use deep learning models trained on **massive image datasets** to learn:
- Pixel patterns unique to each face.
- Features like eyes, nose, mouth positioning.

> *Example: Your phone unlocking instantly when it recognizes your face.*


---

## 🏥 Healthcare and Medical Diagnosis

AI is **revolutionizing healthcare** by helping doctors:
- Analyze **X-rays**, **MRIs**, and **CT scans**.
- Detect diseases like cancer **at early stages**.
- Predict patient risks based on health data.

**Machine learning models** can sometimes match or exceed human doctors in specific diagnosis tasks!

> *Example: AI spotting a tumor that a radiologist missed.*



---

## 💬 Customer Service Chatbots

Modern websites use **AI-powered chatbots** that:
- Answer frequent customer questions.
- Provide **24/7 support**.
- Handle multiple users simultaneously without human fatigue.

They use **natural language processing (NLP)** to **understand** what the user asks and **generate appropriate responses**.

> *Example: A chatbot helping you track your order late at night.*



---

These examples show how AI and ML are already embedded in daily life – from how we get information (asking Siri), to how our email is managed, to the entertainment we consume. Key to all these applications is *data*: the algorithms learn from vast amounts of data (voice recordings, emails, user ratings, images, etc.), and underlying many of these systems are linear algebra computations (for example, the neural network in a voice assistant uses many matrix multiplications). 

**AI, ML, Data, and Linear Algebra Connection:** To tie it together, think of an ML model as a function that takes in data (often represented as vectors/matrices) and produces a prediction or decision. Training that model usually involves linear algebra – adjusting numeric *parameters* (often organized in vectors/matrices) by optimizing a cost function. Even making a prediction is often just a series of linear algebra operations on the input data (for instance, a linear regression prediction is essentially a dot product of a feature vector with a weight vector). Thus, understanding how data is represented (with numpy arrays) and how linear algebra operations work is foundational for anyone starting in AI and ML.


# 🌍 Main Branches, Job Titles, and Specializations in AI and Data Science (2025 Guide)

---

## 🧠 1. Main Branches of AI and Data Science

| Branch                      | Focus                                             |
|------------------------------|---------------------------------------------------|
| Machine Learning (ML)        | Algorithms that learn patterns from data         |
| Deep Learning (DL)           | Neural networks with many layers                 |
| Natural Language Processing (NLP) | Understanding human language         |
| Computer Vision (CV)         | Understanding images and videos                 |
| Reinforcement Learning (RL)  | Learning through trial and error (agents)        |
| Robotics & Control Systems   | AI controlling physical systems/machines        |
| Data Science (DS)            | Analyzing and interpreting complex data          |
| MLOps                         | Deploying and managing ML models in production   |
| AI Ethics and Responsible AI | Ensuring AI is safe, fair, and unbiased          |
| Generative AI                | Creating new data (text, images, audio, etc.)    |
| Causal Inference             | Understanding cause-effect relationships        |

---

## 🧳 2. Common Job Titles in AI and Data Science

| Title                                | Description                                     |
|--------------------------------------|-------------------------------------------------|
| **Data Scientist**                   | Analyzes and interprets complex datasets        |
| **Machine Learning Engineer**        | Builds, trains, and deploys ML models           |
| **Deep Learning Engineer**           | Specializes in training large neural networks   |
| **Data Analyst**                     | Summarizes and visualizes data insights         |
| **Research Scientist (AI/ML)**        | Develops new algorithms and advances the field  |
| **Computer Vision Engineer**         | Works on image and video understanding          |
| **NLP Engineer**                     | Builds models for text, chatbots, translation   |
| **MLOps Engineer**                   | Puts ML models into production reliably         |
| **AI Ethics Specialist**             | Ensures fairness, privacy, transparency in AI   |
| **Big Data Engineer**                | Handles massive datasets (Hadoop, Spark, etc.)  |
| **Business Intelligence (BI) Developer** | Turns data into business insights          |
| **Quantitative Analyst (Quant)**     | Applies AI/ML in finance and trading            |
| **Robotics Engineer**                | Combines AI with mechanical systems             |
| **Prompt Engineer (Generative AI)**  | Designs effective prompts for AI models         |
| **AI Product Manager**               | Leads AI product development teams              |
| **AI Consultant**                    | Advises companies on AI strategies and systems  |

---

## 🧩 3. Specializations in AI and Data Science

**1. Machine Learning Specializations:**
- Supervised learning (classification, regression)
- Unsupervised learning (clustering, dimensionality reduction)
- Self-supervised learning (pretraining without labels)
- Active learning (smartly querying for labels)

**2. Deep Learning Specializations:**
- CNNs (Convolutional Neural Networks) → vision tasks
- RNNs/LSTMs/GRUs (Recurrent Neural Networks) → sequence tasks
- Transformers → dominant architecture (NLP, vision, audio)
- Graph Neural Networks (GNNs) → graph-structured data
- Diffusion models (e.g., DALL·E 3, Stable Diffusion)

**3. Natural Language Processing (NLP) Specializations:**
- Language modeling (e.g., GPT, LLaMA)
- Text generation and summarization
- Translation (multilingual models)
- Sentiment analysis
- Information retrieval and search
- Speech-to-text (ASR) and text-to-speech (TTS)

**4. Computer Vision Specializations:**
- Object detection (YOLO, Faster R-CNN)
- Image segmentation (Mask R-CNN, U-Net)
- Face recognition
- Medical imaging
- Video understanding (action recognition)

**5. Reinforcement Learning Specializations:**
- Game AI (e.g., AlphaZero, OpenAI Five)
- Robotic control
- Autonomous vehicles
- Recommendation systems (multi-armed bandits)

**6. Data Science Specializations:**
- Predictive modeling
- Time series forecasting
- Experiment design (A/B testing)
- Business analytics
- Statistical modeling
- Causal inference and uplift modeling

**7. MLOps and AI Infrastructure Specializations:**
- Model deployment (Docker, Kubernetes, AWS/GCP)
- Model monitoring and drift detection
- Continuous Training (CT) pipelines
- Data versioning and lineage (DVC, MLflow)
- Scalable ML (distributed training)

**8. AI Ethics and Policy Specializations:**
- Fairness in AI (mitigating bias)
- Explainable AI (XAI)
- Privacy-preserving AI (e.g., federated learning)
- Regulation and compliance (GDPR, CCPA)
- Risk assessment for AI systems

**9. Generative AI Specializations:**
- Large Language Models (LLMs) engineering
- Diffusion models for image and audio generation
- Prompt engineering for generative models
- Fine-tuning and alignment of generative models
- Content authenticity and detection (deepfake detection)

**10. Robotics Specializations:**
- Control systems
- Motion planning
- Robot perception (vision and sensor fusion)
- Human-robot interaction

---

## 🧱 4. Learning Paths (Examples)

- **Aspiring Data Scientist:**  
  Python → Statistics → SQL → Machine Learning → Deep Learning → Project portfolio

- **Aspiring ML Engineer:**  
  Python → ML algorithms → Deep Learning → MLOps → System Design

- **Aspiring NLP Engineer:**  
  Python → Text processing → ML for NLP → Transformers → Large Language Models

- **Aspiring Computer Vision Engineer:**  
  Python → OpenCV → CNNs → Detection & Segmentation models → Vision Transformers

- **Aspiring AI Research Scientist:**  
  Math foundations (Linear Algebra, Probability) → Advanced ML/DL → Research papers → PhD (optional)

---

## 📚 5. Hot Skills (2025)

| Area                     | Technologies / Tools        |
|---------------------------|------------------------------|
| ML/DL Frameworks          | TensorFlow, PyTorch, JAX     |
| Data Science & Analytics  | Pandas, Scikit-learn, SQL    |
| Deployment & MLOps        | Docker, Kubernetes, MLflow  |
| Cloud Platforms           | AWS, GCP, Azure             |
| Big Data                  | Spark, Hadoop               |
| Visualization             | Matplotlib, Seaborn, Plotly |
| NLP Specific              | Hugging Face Transformers   |
| CV Specific               | OpenCV, Detectron2          |
| Generative AI             | Stable Diffusion, OpenAI APIs|
| Research                  | arXiv, Papers with Code     |

---

# 🎯 Final Advice

AI and Data Science are **broad and interdisciplinary fields**.  
It’s **impossible to master everything** — focus on **one path first** (e.g., NLP, Computer Vision, or MLOps) and build real projects!

Always remember:
- Master **math** (linear algebra, probability, optimization).
- Build **practical projects**.
- Learn **end-to-end workflows** (not just models but deployment too).
- Follow **new research** (it evolves fast — GPT-4 Turbo, Diffusion models, etc.)
- Stay **ethical and responsible** in applying AI.

🚀 **The future of AI needs creators, not just users!** 🚀


## 3. Representing Different Data Types in Python (using NumPy)

One of the most popular libraries in Python for numerical computing is **NumPy** (Numerical Python). NumPy provides the `ndarray` data structure, which is an **N-dimensional array** (think of it as a generalized matrix) to store and operate on numeric data efficiently. We will use NumPy to illustrate how different data types – images, sound, and tabular data – can be represented as arrays of numbers.

Before diving in, make sure to import numpy in your Python environment: 

```python
import numpy as np
```

### 3.1 Images as NumPy Arrays

An **image** is essentially a grid of pixel values. In a grayscale image, each pixel is a single number (representing intensity, e.g. 0 = black, 255 = white for 8-bit images). In a color image, each pixel is typically represented by 3 numbers (for Red, Green, and Blue channels). This means we can represent a grayscale image as a 2D array (matrix) of numbers, and a color image as a 3D array (height × width × 3 color channels).

- **Dimensions/Shape:** Suppose you have a color image of 640×480 pixels (width 640, height 480). In NumPy this could be an array of shape `(480, 640, 3)` where the last dimension size 3 corresponds to the R, G, B values. A grayscale image of the same size would be shape `(480, 640)` (just two dimensions).
- **Data Type:** Pixel values are often stored as unsigned 8-bit integers (np.uint8 type), ranging 0–255. Sometimes we normalize pixels to 0–1 floats for calculations, but the concept is the same array of numbers.

**Simple Example (Grayscale Image):** Imagine a tiny 2×3 pixel grayscale image. We can represent it with a 2x3 NumPy array of integers:

```python
# Create a 2x3 "image" with pixel intensity values
small_img = np.array([[0,   128, 255],    # 0=black, 255=white, 128=gray
                      [255, 128,   0]])
print("Image array:\n", small_img)
print("Shape:", small_img.shape)
```

This would output something like:

```
Image array:
 [[  0 128 255]
  [255 128   0]]
Shape: (2, 3)
```

Here we have 2 rows and 3 columns, corresponding to 2 pixel rows and 3 pixel columns. The values 0, 128, 255 are example intensities.

**Real-World Example (Loading and manipulating an image):** In practice, you can load image files and get a NumPy array. For example, using the PIL (Pillow) library to read an image:

```python
from PIL import Image
img = Image.open('/Users/yenokhakobyan/Introduction To Data Science PA/lecturer_notes/path_to_image/istockphoto-1454217037-612x612.jpg')        # Load an image file (e.g., JPEG)
img = img.resize((100, 100))           # Resize to 100x100 for this example
img_array = np.array(img)              # Convert to numpy array
print(img_array.shape, img_array.dtype)
```

If `example.jpg` is a color image, `img_array` will be of shape `(100, 100, 3)` and dtype `uint8`. We can then manipulate this array with NumPy. For instance, let's convert it to grayscale by averaging the color channels, and then invert the colors:

```python
gray = img_array.mean(axis=2)               # average across color channels -> shape (100, 100)
inverted = 255 - gray                       # invert grayscale (white->black, etc.)
print(inverted.shape, inverted.dtype)       # should be (100, 100) and uint8
```

Now `inverted` is a 2D array representing the inverted grayscale image. We could save or display this using image libraries, but as far as NumPy is concerned, it's just an array of numbers which we can slice, index, modify, etc., just like any matrix.

**Key takeaway:** Images = matrices of pixel numbers. NumPy makes it easy to load and manipulate these as arrays. You can do things like cropping (slicing the array), brightness adjustment (adding a constant to the array or multiplying), filtering (using matrix operations), and more using NumPy.

*Exercise:* Create a small 5×5 array to represent a simple image (maybe a diagonal line of 255s on a background of 0s). Print out the array and verify the positions of high values form the diagonal. This simulates creating a simple image pattern with numpy.

### 3.2 Sound (Audio) as NumPy Arrays

A **sound** or audio signal can be thought of as a wave of air pressure over time. When we record audio on a computer, we sample this wave at a fast rate (like 44,100 samples per second for CD quality audio). Each sample is just a number (representing the air pressure or voltage at a given time). So a mono audio clip can be represented as a 1D array of numbers (the samples). Stereo audio would be a 2D array with shape (N_samples, 2) since there are two channels (left and right) each with N_samples.

- **Dimensions/Shape:** For a 5-second mono audio clip sampled at 44.1 kHz, you'd have 5*44100 ≈ 220,500 samples, so an array shape `(220500,)`. If stereo, shape `(220500, 2)`.
- **Data Type:** Audio samples might be stored as 16-bit integers (np.int16 ranging -32768 to 32767) in raw form or as floats (e.g., np.float32 between -1.0 and 1.0) if normalized.

**Simple Example (Synthetic Audio):** Let's simulate a very short audio signal. Say we want a simple sine wave (pure tone). We can generate a sine wave with NumPy easily:

```python
# Generate a 440 Hz sine wave for 1 second at 8 kHz sampling
sr = 8000  # sample rate (8000 samples per second)
t = np.linspace(0, 1, sr, endpoint=False)        # time values from 0 to 1 second
frequency = 440  # A4 note frequency in Hz
audio_wave = 0.5 * np.sin(2 * np.pi * frequency * t)  # 0.5 amplitude for safety
print("Audio shape:", audio_wave.shape)
print("First 10 samples:", audio_wave[:10])
```

This code produces a NumPy array `audio_wave` of shape (8000,). The first 10 samples might look like: `[0.0, 0.169, 0.318, 0.430, 0.493, 0.493, 0.430, 0.318, 0.169, 0.0]` (values between -0.5 and 0.5 since we scaled amplitude). This is essentially a digital representation of a 440 Hz tone. You could imagine feeding this array to a sound player to hear the tone.

**Real-World Example (Loading an audio file):** In practice, libraries like `scipy.io.wavfile` or `soundfile` can read a WAV audio file into a NumPy array. For example:

```python
from scipy.io import wavfile
rate, data = wavfile.read('audio_clip.wav')
print("Sample rate:", rate)
print("Audio array shape:", data.shape)
print("Data type:", data.dtype)
```

If `audio_clip.wav` is a stereo clip, `data` could have shape `(N_samples, 2)`. If it’s mono, shape `(N_samples,)`. `rate` might be 44100 (for example). The `data` array will contain the waveform samples. You can manipulate this array: for instance, adjust volume by multiplying the array by a factor, or take a slice `[start:end]` to cut the clip, etc. After processing (which may involve linear algebra if doing fancy filters like convolution), you could write the array back to a WAV file or play it.

**Using NumPy on audio data:** Because an audio signal is just an array, we can use NumPy operations. Want to mix two signals? Just add their arrays (with proper alignment and dtype). Want to fade out? Multiply the tail of the array by a linearly decreasing vector. Want the frequency spectrum? Use Fourier transforms (NumPy provides `np.fft` for this). All these involve treating the sound as a sequence (vector) of numbers and using math on it.

*Exercise:* Generate two different sine wave arrays with different frequencies (e.g., 440 Hz and 880 Hz). Add them together to simulate a simple chord, and listen to it (if you have the capability to play audio). Plot a small portion of the resulting waveform using matplotlib to visualize the combined wave.

### 3.3 Tabular Data as NumPy Arrays

Tabular data is data that naturally fits into a table format – think Excel spreadsheets or SQL tables, where you have rows and each row has multiple columns (attributes). This is very common for structured data: each row could be a person with attributes like age, height, weight; or each row could be a day’s weather with columns for temperature, humidity, etc. We can represent such data as a 2D NumPy array where each row is an observation (sample) and each column is a feature.

- **Dimensions/Shape:** If you have *N* records (rows) and *M* features (columns) for each record, the array shape will be `(N, M)`. For example, a dataset of 100 patients with 4 measurements each can be a NumPy array of shape `(100, 4)`.
- **Data Type:** Typically numeric (int or float). If some columns are categorical (like strings), one usually converts those to numeric codes or uses a different structure (like pandas). NumPy arrays are homogeneous, meaning every element must be the same type.

**Simple Example:** Suppose we have a small table of student scores. Each student has [ID, Math score, English score]. We can create a 2D array for this:

```python
students = np.array([
    [1, 85, 78],
    [2, 90, 88],
    [3, 75, 85]
], dtype=np.int32)
print(students)
print("Shape:", students.shape)
```

Output:
```
[[ 1 85 78]
 [ 2 90 88]
 [ 3 75 85]]
Shape: (3, 3)
```

This represents 3 students (rows) and 3 columns (ID, Math, English). We can easily compute things like the average score per subject using NumPy:

```python
avg_math = students[:, 1].mean()      # mean of second column (Math scores)
avg_eng  = students[:, 2].mean()      # mean of third column (English scores)
print("Average Math score:", avg_math)
print("Average English score:", avg_eng)
```

We sliced `students[:, 1]` which means "all rows, column index 1", i.e., the Math scores. Similarly for English. NumPy makes these operations concise.

**Real-World Example (Loading CSV data):** You might have a CSV file with rows of data. While the pandas library is often used for tabular data, NumPy can also load simple numeric data. For instance, if `data.csv` contains:

```
height,weight,age
170,65,25
160,50,30
180,80,22
```

We can load it with NumPy (using `genfromtxt` or `loadtxt`):

```python
import numpy as np
data = np.loadtxt('data.csv', delimiter=',', skiprows=1)
print(data.shape)
print(data[:2])  # print first two rows
```

If the CSV has 3 columns and 3 data rows (as above), `data` will be shape `(3, 3)`. The first two rows printed might be `[[170.  65.  25.], [160.  50.  30.]]`. Notice by default `loadtxt` gave floats; we can specify `dtype=int` if we want integers.

Once the data is in a NumPy array, you can manipulate it:
- Compute column-wise statistics easily (e.g., `data.mean(axis=0)` gives mean of each column – average height, average weight, average age).
- Slice out a portion of data (e.g., `data[:2, :]` gives first 2 rows, perhaps to simulate a train/test split in ML).
- Combine with another dataset (concatenate arrays) if needed.

**Tabular Data and Linear Algebra:** If we consider a matrix **X** of shape `(N, M)` for tabular data (N samples, M features) and perhaps a vector **w** of length M representing some weights (like a model’s parameters), then a prediction for all N samples can be done by a matrix-vector multiply `X @ w` which yields an N-length vector of predictions. This is one reason linear algebra is so handy: it lets us express bulk operations on all our data in concise ways.

*Exercise:* Create a NumPy array with some made-up tabular data (e.g., a 4x3 array for 4 cars with [horsepower, weight, MPG]). Try to compute something like the correlation between two columns using NumPy (hint: you can compute mean of each, subtract, multiply and average). This will give practice in selecting and combining columns.

## 4. Key Linear Algebra Concepts in Machine Learning

Now that we've discussed representing data with arrays, let's cover some fundamental linear algebra concepts and see how they are used in machine learning. We’ll go through: **vectors**, **matrices**, **matrix multiplication**, **dot product**, and **eigenvalues/eigenvectors**. For each concept, we’ll give a brief description, show how to work with it in Python/NumPy, and explain its relevance to data or ML.

### 4.1 Vectors

**Description:** In mathematics, a **vector** is an ordered list of numbers. Geometrically, you can think of a vector as a point in space (like a coordinate) or an arrow from the origin to that point. For example, `[3, 5]` in 2D represents a point 3 units along the x-axis and 5 units along the y-axis. Vectors have a *magnitude* (length) and *direction*. In linear algebra, vectors are often written as column vectors (like a column of numbers), but in NumPy we usually use 1D arrays to represent them. 

In data science, a vector is a convenient way to represent a single data instance or a set of features. **Feature vector** is a term you’ll hear – it just means a list of features describing one sample. For example, if we have a patient with [height, weight, age], that’s a feature vector in 3-dimensional space. Vectors are used to represent words in NLP (word embeddings), pixel values of an image (flattened into one long vector), or a time-series of sensor readings, etc.

**Python/NumPy Example:** Creating and using vectors.

```python
# Create a vector (as a 1D numpy array)
v = np.array([2, 5, 1])
w = np.array([3, 4, 1])

print("Vector v:", v)
print("Vector w:", w)
print("Shape of v:", v.shape)
```

Output:
```
Vector v: [2 5 1]
Vector w: [3 4 1]
Shape of v: (3,)
```

Here `v` and `w` are 3-dimensional vectors. We can perform basic vector operations:
- **Addition/Subtraction:** Add corresponding elements.
- **Scalar Multiplication:** Multiply each element by a number.
- **Magnitude (Length):** `||v|| = sqrt(v_1^2 + v_2^2 + ... )`.
- **Dot Product:** (covered later, but it’s a key operation between two vectors).

Example of these operations:

```python
sum_vw = v + w
diff_vw = v - w
scaled_v = 2 * v
mag_v = np.linalg.norm(v)  # Euclidean norm (length) of v

print("v + w =", sum_vw)      # [5, 9, 2]
print("v - w =", diff_vw)     # [-1, 1, 0]
print("2 * v =", scaled_v)    # [4, 10, 2]
print("||v|| =", mag_v)       # sqrt(2^2 + 5^2 + 1^2) = sqrt(30) ≈ 5.477
```

NumPy will perform element-wise addition/subtraction automatically since `v` and `w` have the same shape. The norm (length) is computed with `np.linalg.norm`. You could also compute it manually: `np.sqrt((v**2).sum())`.

**Use in Data (Why vectors matter in ML):**
- **Tabular data:** Each data point is a vector of features. If you have 100 features, each example is a 100-dimensional vector.
- **Images:** You can flatten a 2D image into a long 1D vector (e.g., a 28×28 image can be reshaped into a 784-length vector). This is often done to feed image data into certain algorithms that expect a vector input.
- **Text:** Through embedding techniques, a word or a document can be represented as a vector (for instance, a 300-dimensional vector representing the concept of that word).
- **State in AI:** If an AI agent has certain sensory inputs, those can form a state vector representing the current situation.

Vectors are the basic unit of data in machine learning. Many algorithms compute similarities between vectors (using dot products or distances), or apply transformations to vectors (using matrices).

### 4.2 Matrices

**Description:** A **matrix** is a two-dimensional array of numbers, with rows and columns. It’s like a table of numbers with size `m × n` (m rows, n columns). Matrices can represent transformations or relationships between sets of numbers. In linear algebra, matrices are used to solve systems of linear equations, to represent linear transformations (like rotating or scaling coordinates), and much more. Notation-wise, matrices are usually denoted by capital letters (A, B, X, …), and a specific entry in a matrix is referenced by two indices (row i, column j).

In data science, some common uses of matrices:
- A dataset is often represented as a matrix (**data matrix**): each row is an example and each column is a feature (as discussed, shape = (num_samples, num_features)).
- An image is naturally a matrix of pixel values (plus possibly a channel dimension).
- Matrices can represent graphs or networks (adjacency matrix), or connections in a neural network (weight matrices between layers).
- In linear models, we often have a matrix of input data and a vector of parameters, etc.

**Python/NumPy Example:** Creating and using matrices.

```python
# Create a 2x3 matrix
M = np.array([[1, 2, 3],
              [4, 5, 6]])
print("Matrix M:\n", M)
print("Shape of M:", M.shape)
```

Output:
```
Matrix M:
 [[1 2 3]
  [4 5 6]]
Shape of M: (2, 3)
```

This matrix M has 2 rows and 3 columns. We can access elements or submatrices via indexing/slicing:
```python
print("Element M[0,1] =", M[0,1])   # element at 1st row, 2nd column (0-based index) -> 2
first_row = M[0, :]                # 1st row (as a vector)
col_3 = M[:, 2]                    # 3rd column (as a vector)
print("First row:", first_row)
print("Third column:", col_3)
```

Element `M[0,1]` would be 2 (remember indexing starts at 0). `M[0, :]` gives `[1,2,3]`. `M[:, 2]` gives `[3, 6]` (all rows, col index 2).

We can do operations on matrices element-wise similar to vectors (addition, subtraction, scalar multiply, etc., as long as shapes align or via broadcasting). For example:

```python
N = np.array([[7, 8, 9],
              [1, 2, 3]])
print("M + N =\n", M + N)
print("M * 2 =\n", M * 2)
```

`M+N` adds corresponding elements (resulting in `[[8,10,12],[5,7,9]]`). `M*2` multiplies every element of M by 2.

**Use in Data/ML:**
- **Data Matrix:** as mentioned, treating the whole dataset as a matrix allows vectorized computations. For instance, if `X` is an (N×M) matrix of data and `w` is an (M×1) weight vector, then `X @ w` yields an N×1 vector of predictions (one per data point). This is how we express making predictions for multiple data points in one go.
- **Covariance Matrix:** In statistics/ML, we compute an *M×M* covariance matrix of features to understand how features vary together. The covariance matrix is then used in PCA (via eigen-decomposition).
- **Transformation Matrices:** In deep learning, the weights between layers can be thought of as a matrix that transforms input vectors to output vectors. In image processing, some filters (if applied linearly) can be represented as matrix operations on pixel data.
- **Adjacency Matrices:** Representing connections (in a social network or graph) often uses a matrix where entry (i,j) indicates connection strength from node i to j.

Understanding how to work with matrices is crucial since any non-trivial ML model will involve matrices of parameters or data. Next, we’ll discuss operations *between* matrices (and vectors) that go beyond simple element-wise arithmetic.

### 4.3 Matrix Multiplication

**Description:** *Matrix multiplication* is a fundamental operation where two matrices are multiplied to produce a new matrix. This is **not** done element-wise, but follows a specific rule: if A is of shape (p×q) and B is of shape (q×r), then their product C = A × B is of shape (p×r). Each element of C is computed by taking a row of A and a column of B and computing their dot product (multiply corresponding elements and sum them up). In formula, if $C = A \times B$, then 

$$
C_{i,j} = \sum_{k=1}^{q} A_{i,k} \times B_{k,j}.
$$

So C[i,j] is the dot product of row i of A with column j of B.

For matrix multiplication to be valid, the **inner dimensions** must match (the number of columns of the first matrix must equal the number of rows of the second matrix). If you have incompatible shapes, you cannot multiply them in the standard linear algebra sense.

Matrix multiplication represents function composition or transformations applied in sequence. For example, if matrix $A$ transforms a vector $x$ into $y$ (i.e., $y = A x$), and matrix $B$ transforms $y$ into $z$ ($z = B y$), then $z = B (A x) = (B A) x$. So the composed transformation is represented by the product $B A$. (Note: pay attention to order – matrix multiplication is *not commutative*, meaning $A B \neq B A$ in general.)

**Python/NumPy Example:** We can use `np.dot()` or the `@` operator to do matrix multiplication in NumPy.

```python
# Define two matrices A (2x3) and B (3x2) such that we can multiply them
A = np.array([[1, 2, 3],
              [4, 5, 6]])   # shape (2,3)
B = np.array([[7, 8],
              [9, 10],
              [11, 12]])    # shape (3,2)

C = A.dot(B)    # or C = A @ B
print("A shape:", A.shape, ", B shape:", B.shape)
print("C = A @ B =\n", C)
print("Shape of C:", C.shape)
```

Here A is 2×3, B is 3×2, so the result C will be 2×2. Let's manually verify one element to ensure understanding:
- C[0,0] = (1*7 + 2*9 + 3*11) = 7 + 18 + 33 = 58.
- C[0,1] = (1*8 + 2*10 + 3*12) = 8 + 20 + 36 = 64.
And similarly for the second row. The output will be:

```
C =
 [[ 58  64]
  [139 154]]
Shape of C: (2, 2)
```

NumPy computed that for us. Notice we could not have multiplied A (2×3) with, say, a (2×2) matrix directly – shape mismatch.

**Use in Data/ML:**
Matrix multiplication is everywhere in machine learning:
- **Linear Regression/Linear Models:** If X is your data matrix (N samples × M features) and β is a parameter vector (M × 1), then predictions $\hat{y}$ for all N samples can be computed as the matrix product $X \times \beta$ (result is N × 1). This is essentially performing N dot-products (one for each sample).
- **Neural Networks:** The computation in each layer of a neural network is often a matrix multiply: if you have an input vector, it’s multiplied by a weight matrix to produce an output vector for the next layer. When you process multiple inputs at once (batch processing), you actually use matrix multiplication between a batch matrix and the weight matrix.
- **Word Embeddings:** In NLP, if you represent the vocabulary as vectors (one-hot encodings), multiplying a one-hot vector (which is mostly zeros and a 1 for the target word index) by an **embedding matrix** yields the vector for that word. That’s matrix multiplication under the hood: one-hot (1×V) times embedding matrix (V×D) = word embedding (1×D).
- **Transformation Composition:** As mentioned, applying multiple linear transformations in sequence is captured by matrix multiplication. For instance, to do a 2D rotation then scaling on coordinates, you multiply the coordinate vector by a rotation matrix and then by a scaling matrix (which combined is one matrix multiplication by the product of those matrices).
- **Covariance and PCA:** Computing the covariance matrix involves a multiplication (actually $X^T X$ scaled by 1/N gives covariance if X is zero-mean). And to project data onto principal components, you multiply by a matrix of eigenvectors.

In summary, matrix multiplication allows you to succinctly express combining information from multiple sources (features * times * weights, etc.). Machine learning libraries and hardware (like GPUs) are optimized for large matrix multiplications, which is why we try to formulate computations in those terms.

### 4.4 Dot Product

**Description:** The **dot product** (also called scalar product or inner product) is an operation that takes two vectors of the same length and returns a single number (a scalar). If $a$ and $b$ are vectors $(a_1, a_2, ..., a_n)$ and $(b_1, b_2, ..., b_n)$, then 

$$
 a \cdot b = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n. 
$$

It's essentially multiplying corresponding components and summing them. We saw this concept inside matrix multiplication (each entry was a row dot a column). The dot product has a geometric interpretation: $a \cdot b = \|a\|\|b\|\cos\theta$, where $\theta$ is the angle between the two vectors. So if two vectors point in similar directions, their dot product is large (and positive); if they are orthogonal (90° apart), dot product is 0 ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=Two%20vectors%20are%20considered%20orthogonal,models%20operate%20independently%20or%20not)); if they point opposite, dot product is negative.

**Why is dot product important?** It is used to measure **similarity** of vectors (especially in ML for things like cosine similarity), to project one vector onto another, and it's the building block of many algorithms (like the core of a neuron in a neural network is a dot product of weights and inputs). 

**Python/NumPy Example:** Dot product of two vectors.

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
dot = np.dot(a, b)  # or a.dot(b) or a @ b (for 1D, @ does dot)
print("a · b =", dot)
```

Calculation: $1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32$. So `dot` should be 32.

We can also confirm this by breaking it down:
```python
elementwise = a * b
print("Element-wise multiplication:", elementwise)
print("Sum of elements:", elementwise.sum())
```
This will show `[4 10 18]` and sum `32`. NumPy’s `np.dot` did exactly that under the hood.

**Use in Data/ML:**
- **Feature Weights:** If you have a feature vector and a weight vector, the prediction of a linear model is a dot product $w \cdot x$ (plus maybe a bias). For instance, in linear regression or in a single neuron of a neural net, you compute weighted sum of inputs – that’s a dot product.
- **Similarity:** In information retrieval or recommender systems, you might compute how similar two users are by taking the dot product of their preference vectors. Cosine similarity between two vectors is basically $\frac{a \cdot b}{\|a\|\|b\|}$. If vectors are normalized to length 1, cosine similarity is exactly the dot product. Word embeddings are often compared via dot product to find similar words ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=%2A%20NLP%20,dot%20products%20alongside%20matrix%20multiplication)).
- **Orthogonality:** As noted, if $a \cdot b = 0$, the vectors are orthogonal (uncorrelated in a sense). In ML, this concept appears in orthogonal feature vectors or orthogonal weight initialization in neural networks, etc., meaning components that capture independent information.
- **Matrix multiplication connection:** When we do $X @ w$ for predictions, each output is a dot product of a data row with the weight vector. So dot product is the elemental operation inside matrix multiplication.

Practically, understanding dot product helps in grasping algorithms like **principal component analysis**, where we project data onto principal axes via dot products, or **support vector machines** where maximizing the margin relates to dot products between data points and weight vectors.

*Exercise:* Take two vectors (e.g., `[1,2,3]` and `[1,0,-1]`). Compute their dot product manually and with NumPy. Interpret the result: are they orthogonal, somewhat aligned, or opposite? (Hint: If the result is 0, they are orthogonal; if positive, they have an acute angle; if negative, obtuse angle between them.)

### 4.5 Eigenvalues and Eigenvectors

**Description:** *Eigenvalues* and *eigenvectors* are concepts from linear algebra that often pop up in more advanced data analysis (like PCA). Given a square matrix **A**, an eigenvector **v** is a non-zero vector such that when **A** multiplies **v**, the result is just a scalar multiple of **v**. That is:

\[ A \mathbf{v} = \lambda \mathbf{v}, \]

where **v** is the eigenvector and **λ** (lambda) is the corresponding eigenvalue (a scalar). In plain language, applying the transformation A to v **does not change its direction** – it only scales v by factor λ.

Every matrix has certain special vectors (eigenvectors) that *stay in the same direction* under that matrix transformation. The scalar by which it gets scaled is the eigenvalue. Finding eigenvalues and eigenvectors involves solving the equation $(A - \lambda I)\mathbf{v} = 0$ for non-zero v, which leads to a characteristic polynomial. But we won’t go into that detail here.

**Why are eigenvectors/values useful in ML?** They help identify principal directions in data:
- **Principal Component Analysis (PCA):** We compute eigenvectors of the covariance matrix of the data. The eigenvector with the largest eigenvalue indicates the direction of maximum variance in the data (first principal component), the next gives the second principal component, and so on ([Linear Algebra Techniques in Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-techniques-in-data-science/#:~:text=Eigenvalues%20and%20eigenvectors%20are%20essential,the%20most%20important%20underlying%20factors)). By projecting data onto the top eigenvectors, we reduce dimensionality while retaining most variance.
- **Spectral clustering and Graph algorithms:** Eigenvalues of graph Laplacian matrices are used to find community structures.
- **Linear transformations understanding:** By examining eigenvalues, we know if a matrix stretches or squishes along certain axes, and whether it's invertible (if any eigenvalue is zero, matrix is singular).
- **Google’s PageRank:** The PageRank algorithm finds an eigenvector of the web graph’s Google matrix (the steady-state distribution).
- Even in differential equations or system dynamics, eigenvalues indicate stability.

For a beginner course, the main takeaway is PCA: using eigenvectors of data’s covariance to find new axes for the data that are uncorrelated and ordered by importance.

**Python/NumPy Example:** Finding eigenvalues and eigenvectors using NumPy’s linear algebra module `np.linalg`.

```python
# Define a simple 2x2 matrix
A = np.array([[2, 1],
              [1, 2]])
# Compute eigenvalues and eigenvectors
eig_vals, eig_vecs = np.linalg.eig(A)
print("Matrix A:\n", A)
print("Eigenvalues:", eig_vals)
print("Eigenvectors:\n", eig_vecs)
```

For matrix A = [[2,1],[1,2]], the eigenvalues should turn out to be 3 and 1. The eigenvectors might be (normalized) versions of [1,1] and [1,-1] respectively:
- λ=3 eigenvector ~ [0.707, 0.707] (which is [1,1] normalized),
- λ=1 eigenvector ~ [0.707, -0.707] (which is [1,-1] normalized).

The output could be:
```
Eigenvalues: [3. 1.]
Eigenvectors:
 [[ 0.70710678 -0.70710678]
  [ 0.70710678  0.70710678]]
```
Here the first column is the eigenvector for eigenvalue 3, second column for eigenvalue 1. We can verify: if we multiply A by the first eigenvector, it should equal 3 times that eigenvector.

Let's verify one eigenpair for understanding:
```python
v = eig_vecs[:, 0]      # eigenvector corresponding to eig_vals[0]
lambda_val = eig_vals[0]
print("A @ v:", A.dot(v))
print("lambda * v:", lambda_val * v)
```
This will show that `A @ v` is essentially the same as `3 * v` (within numerical precision).

**Use in Data/ML:**
- **Dimensionality Reduction (PCA):** As mentioned, eigenvectors of the covariance matrix give principal components ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=Eigenvalues%20and%20eigenvectors%20are%20used,dimensionality%20reduction%20and%20feature%20extraction)). If you have high-dimensional data, you can compute these to find a lower-dimensional representation capturing most variance.
- **Feature Engineering:** Sometimes, combinations of features (eigenvectors are essentially linear combinations) can be more informative than the original features. PCA provides those combinations.
- **Explaining Variance:** Eigenvalues in PCA tell you how much variance each principal component (eigenvector direction) accounts for. A large eigenvalue means that axis has a lot of the data’s variance.
- **Markov Chains/Transition Matrices:** The steady-state distribution of a Markov chain is an eigenvector of the transition matrix (eigenvalue 1). This concept is used in algorithms like PageRank.

While eigenvalues/eigenvectors are a bit more advanced than vectors and matrices, they illustrate the power of linear algebra in uncovering structure in data. Even if the term sounds abstract, the practical use-case like PCA directly ties to it: **“find me the directions (eigenvectors) in which my data varies the most.”**

*Exercise:* If you have a small 3x3 covariance-like matrix (symmetric matrix), use `np.linalg.eig` to find its eigenvalues. Then check that the sum of the eigenvalues equals the trace of the matrix (sum of diagonal elements) – a property that always holds. This will give a bit of hands-on feel for eigen computations.

## 5. Introduction to NumPy for Linear Algebra and Data Manipulation

We’ve been using NumPy in examples above without a formal introduction. Now, let’s dive into NumPy itself – how to get it, and key operations you'll use frequently when handling data or performing linear algebra in Python.

### 5.1 Installing and Importing NumPy

To use NumPy, you first need to have it installed in your Python environment. It’s one of the most common libraries, so many environments have it pre-installed. If not, you can install it via pip:

```bash
pip install numpy
```

Or if you're using Anaconda distribution, it's already included; otherwise `conda install numpy` would do.

After installation, in your Python script or notebook, import NumPy (by convention it's imported as `np` for brevity):

```python
import numpy as np
```

Now you can access NumPy functions with the `np.` prefix (e.g., `np.array`, `np.dot`, `np.linalg.eig`, etc.) ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=After%20installing%20NumPy%2C%20it%20may,imported%20into%20Python%20code%20like)).

### 5.2 Creating and Inspecting Arrays

The core of NumPy is the **ndarray** (n-dimensional array) object. You can create arrays in several ways:

- **From Python lists or tuples:** Using `np.array()` constructor.
- **Using built-in functions:** e.g., `np.zeros`, `np.ones` for arrays of all 0s or 1s, `np.eye` for identity matrix, `np.arange` (similar to range) or `np.linspace` for sequences, or random generators like `np.random.rand` for random numbers.
- **Reading from files:** functions like `np.loadtxt` or `np.genfromtxt` for text files, or `np.load` for .npy binary files, etc.

**Examples:**

```python
# From lists
a = np.array([10, 20, 30])              # 1D array (vector) from list
b = np.array([[1,2,3], [4,5,6]])        # 2D array (matrix) from list of lists

# Using built-ins
c = np.zeros((2, 5))                    # 2x5 array of zeros (floats by default)
d = np.ones((3, 3), dtype=int)          # 3x3 array of ones (integers)
e = np.identity(4)                      # 4x4 identity matrix (1s on diagonal, 0 elsewhere)
f = np.arange(0, 10, 2)                 # array([0, 2, 4, 6, 8]) – like range, step=2
g = np.linspace(0, 1, 5)                # 5 points between 0 and 1 inclusive: array([0. , 0.25, 0.5 , 0.75, 1.])
h = np.random.rand(2, 3)                # 2x3 array of random floats in [0,1)
```

After creating an array, you might want to inspect its properties:
- `array.shape` – gives a tuple of dimensions.
- `array.dtype` – data type of elements (e.g., float64, int32).
- `array.ndim` – number of dimensions.
- `array.size` – total number of elements (product of dimensions).

Example:
```python
X = np.array([[5.2, 7.1, 3.0],
              [2.0, 9.3, 1.1]])
print("Shape:", X.shape)        # (2, 3)
print("Dtype:", X.dtype)        # float64
print("Num of dimensions:", X.ndim)  # 2 (because it's 2D)
print("Total elements:", X.size)     # 6 (2*3)
```

### 5.3 Basic Arithmetic Operations

NumPy allows element-wise arithmetic on arrays very intuitively, much like scalar arithmetic:
- Addition, subtraction (`+`, `-`)
- Multiplication, division (`*`, `/`)
- Exponentiation (`**`), modulo (`%`), etc., apply element-wise.

Crucially, **these operations are vectorized**, meaning they happen on each element without the need for explicit loops in Python (this gives huge speed benefits using optimized C code under the hood).

**Example:**

```python
x = np.array([1, 2, 3])
y = np.array([10, 10, 10])
print("x + y =", x + y)         # array([11, 12, 13])
print("x * y =", x * y)         # array([10, 20, 30])
print("x + 5 =", x + 5)         # array([6, 7, 8])  (adds 5 to each element)
print("x ** 2 =", x ** 2)       # array([1, 4, 9])  (squares each element)
```

As shown, `x + y` adds corresponding elements (requires shapes to match or be broadcastable, see broadcasting later). `x + 5` adds 5 to every element of x (scalar is broadcasted to array). 

Note: In NumPy, `*` is **not** matrix multiplication; it is element-wise multiplication. For matrix multiplication, use `@` or `np.dot` as discussed earlier. For example:
```python
A = np.array([[1,2],[3,4]])
B = np.array([[5,6],[7,8]])
print("A * B (element-wise):\n", A * B)
print("A @ B (matrix multiply):\n", A @ B)
```
Element-wise `A * B` would give `[[5,12],[21,32]]` (just multiply each entry). `A @ B` would perform matrix multiplication (result `[[19,22],[43,50]]` in this case).

NumPy also has built-in **universal functions** (ufuncs) that apply to each element:
- `np.sqrt(x)` – square root of each element.
- `np.exp(x)` – exponential (e^x) each element.
- `np.sin, np.log, np.abs`, etc. operate element-wise on an array.

You can combine operations; for example:
```python
val = np.sin(np.array([0, np.pi/2, np.pi]))  # array([0., 1., 0.]) (sin of 0, 90°, 180°)
```

### 5.4 Indexing and Slicing

Indexing in NumPy works much like Python lists, but for multi-dimensional arrays you provide an index for each axis. Remember, indices are 0-based.

For a 1D array (vector) `v`, `v[i]` gets the i-th element. For a 2D array `M`, `M[i, j]` gets the element at row i and column j. You can also use `:` to slice ranges, similar to Python lists.

**Basic indexing examples:**

```python
arr = np.array([10, 11, 12, 13, 14])
print(arr[0])         # 10 (first element)
print(arr[2:5])       # [12 13 14] (slice from index 2 up to 4)
print(arr[-1])        # 14 (last element, negative index works)

M = np.array([[5, 6, 7],
              [8, 9, 10],
              [1, 2, 3]])
print(M[1, 2])        # element at 2nd row, 3rd col -> 10
print(M[0])           # prints first row (treats 0 as row index) -> [5 6 7]
print(M[:, 1])        # all rows, 2nd column -> [6 9 2]
print(M[0:2, 1:3])    # submatrix: rows 0-1 and cols 1-2 -> [[6 7],[9 10]]
```

Important things to note:
- `M[0]` gives the first row (same as `M[0, :]`).
- `M[:, 1]` gives the second column as a 1D array.
- You can slice multiple axes: `0:2` for rows (0 and 1) and `1:3` for cols (1 and 2).
- If you assign to a sliced portion, it will modify the original array (because slicing returns a *view*, not a copy, in NumPy ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=new%20list%2C%20but%20slicing%20an,be%20mutated%20using%20the%20view)) ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=Indexing%20and%20slicing%20operations%20are,useful%20when%20you%E2%80%99re%20manipulating%20matrices))). For example: `sub = M[:2, :2]; sub[:] = 0` will set the top-left 2x2 block of M to zeros.

NumPy also supports **fancy indexing** (using arrays of indices) and **boolean indexing** (using a boolean mask array to pick elements), which are advanced but extremely useful. Briefly:
- Fancy index: `M[[0,2]]` would pick row 0 and row 2.
- Boolean mask: `mask = (arr % 2 == 0); evens = arr[mask]` picks out all even numbers from `arr`.

These are powerful for filtering data or selecting arbitrary subsets.

**Example:**

```python
scores = np.array([65, 80, 90, 50, 85])
passed = scores[scores >= 60]
print("Scores:", scores)
print("Passed scores:", passed)
```
This uses boolean indexing to filter scores >= 60. Output:
```
Scores: [65 80 90 50 85]
Passed scores: [65 80 90 85]
```
(Notice 50 was omitted in the passed array.)

### 5.5 Reshaping and Resizing

Often you'll need to change the shape of an array without changing its data:
- **reshape:** `arr.reshape(new_shape)` returns a view (if possible) of the array with the new shape. The number of elements must remain the same. You can use `-1` in one dimension to let NumPy infer the correct size.
- **flatten/ravel:** to collapse an N-D array into 1D. `arr.flatten()` returns a copy, `arr.ravel()` returns a view if possible.
- **transpose:** For 2D, `arr.T` gives the transpose (rows and columns swapped). For higher dimensions, `arr.transpose` with axes can reorder axes.

**Examples:**

```python
A = np.arange(1, 13)  # array([1,2,...,12]) shape (12,)
B = A.reshape(3, 4)   # reshape to 3x4
print("B:\n", B)
# B might look like:
# [[ 1  2  3  4]
#  [ 5  6  7  8]
#  [ 9 10 11 12]]
print("B shape:", B.shape)           # (3,4)
C = B.reshape(2, 2, 3)               # reshape 3x4 into 2x2x3 (since 3*4 = 24 elements, 2*2*3 = 12, wait that doesn't match!)
```

Actually, 3x4 is 12 elements, so 2x2x3 is also 12 – it's valid. `C` will have shape (2,2,3). If we printed `C`, it might look like a 3D structure.

Using `-1`:
```python
D = B.reshape(2, -1)   # Here we say 2 rows, and -1 means "infer the number of columns"
print("D shape:", D.shape)  # (2, 6) because 2*6 = 12
```

If you try to reshape to an incompatible shape, NumPy will throw an error.

Flattening:
```python
flat = B.flatten()   # copy as 1D
print(flat)          # [1 2 3 ... 12]
```
This gives a new array (if you modify it, B won’t change). `B.ravel()` would give a view if B is contiguous in memory, which it is, meaning changes to ravel output might reflect in B.

Transposing:
```python
print("B^T:\n", B.T)
# B^T would be 4x3:
# [[ 1  5  9]
#  [ 2  6 10]
#  [ 3  7 11]
#  [ 4  8 12]]
```

### 5.6 Stacking and Splitting

Often we need to combine arrays or split them:
- **Stacking**: concatenating arrays along a certain axis. If shapes align appropriately, you can use:
  - `np.concatenate` (general purpose, you specify axis).
  - `np.vstack` (stack vertically, i.e., one on top of another, which is axis=0 concatenation for 2D).
  - `np.hstack` (stack horizontally, axis=1 for 2D).
  - `np.stack` (to create a new axis and stack along it, if needed).
- **Splitting**: dividing an array into multiple sub-arrays:
  - `np.split` (specify indices or sections).
  - `np.hsplit`, `np.vsplit` for specific directions in 2D.

**Examples (Stacking):**

```python
p = np.array([1,2,3])
q = np.array([4,5,6])
# Horizontal stack (make them one longer row)
hq = np.hstack((p, q))
print("hstack:", hq)         # [1 2 3 4 5 6]

# Vertical stack (make them 2 rows)
vq = np.vstack((p, q))
print("vstack:\n", vq)
# [[1 2 3]
#  [4 5 6]]
```

Note: In the above, `p` and `q` were 1D. `hstack` gave a 1D of length 6, `vstack` gave a 2x3.

If we had two 2D arrays with same number of columns, we could vstack (adding more rows). If same number of rows, we could hstack (adding more columns).

Concatenate general example:
```python
X1 = np.array([[1, 2], [3, 4]])    # shape (2,2)
X2 = np.array([[5, 6], [7, 8]])    # shape (2,2)
cat0 = np.concatenate((X1, X2), axis=0)  # stack rows -> shape (4,2)
cat1 = np.concatenate((X1, X2), axis=1)  # stack cols -> shape (2,4)
print("Concatenate axis=0:\n", cat0)
# [[1 2]
#  [3 4]
#  [5 6]
#  [7 8]]
print("Concatenate axis=1:\n", cat1)
# [[1 2 5 6]
#  [3 4 7 8]]
```

**Examples (Splitting):**

```python
Z = np.arange(10)  # [0 1 2 3 4 5 6 7 8 9]
# Split Z into 3 parts (you provide indices where to split)
part1, part2, part3 = np.split(Z, [4, 7])
print(part1, part2, part3)
# part1 = [0 1 2 3]
# part2 = [4 5 6]
# part3 = [7 8 9]
```

The indices `[4,7]` meant: break before index 4 (so part1 is Z[0:4]), and before index 7 (part2 is Z[4:7]), and the rest is part3 (Z[7:]).

For a 2D array, `np.vsplit` could split into submatrices by rows, `np.hsplit` by columns.

### 5.7 Broadcasting

We touched on *broadcasting* earlier. It is a mechanism in NumPy that allows arithmetic operations on arrays of different shapes, by automatically “stretching” one array to match the shape of the other **without actually copying data** ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=NumPy%20understands%20that%20the%20multiplication,ValueError)). Broadcasting rules are:
1. If arrays have different number of dimensions, prepend 1s to the shape of the smaller array until the dimensions match.
2. Then, for each dimension, the sizes must either be equal, or one of them is 1 (in which case that array is stretched to match the other’s size in that dimension).
3. If any dimension disagrees (neither is 1 and sizes differ), the operation is invalid.

**Simple broadcasting example:**
```python
data = np.array([1.0, 2.0, 3.0])
print(data * 2)   # Here 2 is treated as [2,2,2] to match shape -> [2. 4. 6.]
```
Scalar with array is the simplest broadcast – it applies to each element ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=,1.6%20array%28%5B1.6%2C%203.2)).

Another example:
```python
A = np.array([[1,2,3],
              [4,5,6]])      # shape (2,3)
b = np.array([10, 20, 30])   # shape (3,)
C = A + b
print("A + b =\n", C)
```
Here `b` is 1D of length 3. NumPy will treat `b` as a 2x3 by *replicating* it in a way for each row of A (because A has 2 rows, and b's length matches A's columns). The result:
```
A + b =
 [[11 22 33]
  [14 25 36]]
```
It added 10,20,30 to the first row and also 10,20,30 to the second row. `b` was broadcasted to shape (2,3) conceptually. This is very convenient: we didn't have to manually stack `b` to match `A` – NumPy did it for us.

If we tried to add a shape (2,3) and shape (2,) array, what happens?
```python
c = np.array([7,8])    # shape (2,)
# A (2x3) plus c (length 2) -> the second dimension doesn't match and neither is 1, so this would error.
```
Yes, (2,3) and (2,) are not compatible because after aligning dimensions: A is (2,3), c would become (1,2) to match two dims which is then expanded to (2,2) vs (2,3) – mismatch in second dim (2 vs 3). So you'd get a ValueError.

**Use of broadcasting:**
- Often when you want to apply some operation row-wise or column-wise without writing loops. For example, subtract the mean of each column from the columns: if `col_mean` is shape (3,) for 3 columns, doing `A - col_mean` will subtract the corresponding mean from each column of A (because (3,) broadcasts to (2,3)).
- Add a vector to each row (like above example) or each column (if you shape things correctly).
- Efficient scalar operations (we already used).
- Many numpy functions inherently broadcast, e.g., `np.maximum(array, 0)` will apply 0 as needed to compare with every element (used in ReLU activation in neural nets).

**Caution:** While broadcasting is powerful, always ensure that the intended expansion is what you want, as it can sometimes lead to unintentional results if shapes align incorrectly by coincidence.

### 5.8 Aggregation Functions

NumPy provides a suite of **aggregation** (reduction) functions that compute a summary statistic over the elements of an array. Common ones include:
- `np.sum` – sum of all elements (or along an axis)
- `np.prod` – product of all elements
- `np.mean` – average
- `np.std`, `np.var` – standard deviation, variance
- `np.min`, `np.max` – minimum, maximum
- `np.argmin`, `np.argmax` – indices of min/max
- etc.

These can operate on the entire array or along a specific axis:
- If you do `array.sum()`, it sums up everything in the array (flattened).
- If you do `array.sum(axis=0)`, it will sum **down the rows** (i.e., produce a sum for each column).
- `array.sum(axis=1)` would sum **across the columns** (a sum for each row).

Example:
```python
M = np.array([[2, 4, 6],
              [1, 3, 5]])
print("Total sum:", M.sum())            # 2+4+6+1+3+5 = 21
print("Sum by columns:", M.sum(axis=0)) # array([3, 7, 11]) -> sum of each column
print("Sum by rows:", M.sum(axis=1))    # array([12, 9]) -> sum of each row
print("Mean of all elements:", M.mean())# 21/6 = 3.5
print("Max value:", M.max(), " at index", M.argmax())
```

In the above, `M.argmax()` without axis will return the index in the flattened array of the maximum (by default flat index 0..5). There are ways to get the 2D index via `np.unravel_index` if needed. But you can also do `M.argmax(axis=1)` to get the index of max in each row, for example.

Another useful function is `np.cumsum` (cumulative sum) which will give a running total across an array, and similarly `np.cumprod`.

Aggregation functions are very optimized in NumPy (implemented in C). Rather than writing a Python loop to sum or average, always use these – it’s cleaner and much faster.

**Example:**
```python
data = np.random.rand(1000000)  # 1 million random numbers
print("Mean (via np.mean):", data.mean())
# If we were to do Python loop, it'd be much slower.
```

Using these in data contexts:
- Summing all elements could give a total like total population if the matrix held population counts.
- Summing by axis gives distributions (e.g., sum sales by product vs by region if your matrix is products × regions).
- Mean and std are used to normalize data (subtract mean, divide std).
- Min/Max can normalize to [0,1] range by (x - min)/(max-min).
- In ML, loss functions often involve summing residuals or errors, etc., which is done with these sums.

**Note on axes:** The axis number in NumPy corresponds to the dimension index. For a 2D array, axis=0 means operate down each column (compress the 0th index, which is the row index – so result has one value per column), axis=1 means operate across each row. For a 3D array, axis=0 would collapse across the first dimension, and so on. Sometimes it takes a bit of practice to remember which axis is which – but the rule of thumb: axis specifies which dimension *will be eliminated* (reduced) by the operation. E.g., sum axis=1 on shape (2,3) -> result shape (2,) because you eliminated dimension 1 (columns) and have one result per row.

### 5.9 Other Handy Operations

A few miscellaneous but handy things in NumPy:
- **Copy vs View:** As mentioned, slices are views. If you want a true copy of an array (that you can modify independently), use `array.copy()`.
- **Sorting:** `np.sort(array)` returns a sorted copy. `array.sort()` sorts in-place. For multi-dim arrays, you can sort along an axis.
- **Unique elements:** `np.unique(array)` gives sorted unique values (useful for classification labels etc).
- **Matrix inverse and linear algebra:** NumPy provides some linear algebra routines in `np.linalg` module. For example, `np.linalg.inv(A)` for inverse of a matrix, `np.linalg.det(A)` for determinant, `np.linalg.solve(A, b)` to solve linear system A x = b. Use these with caution – inversion is expensive and not needed unless specifically required (and matrix must be square and full-rank).
- **Random numbers:** Besides `np.random.rand`, there's `np.random.randn` for normal distribution, or more controlled random via `np.random.RandomState`. Modern NumPy (v1.17+) recommends using `np.random.default_rng()` to create a random generator instance for better control.
- **Save/Load:** You can save arrays to disk and load them. `np.save('file.npy', arr)` and `np.load('file.npy')` for NumPy’s binary format; or `np.savetxt('file.txt', arr, delimiter=',')` for text.

With these tools, you can handle most data wrangling tasks efficiently.

*Exercise:* Create a random 5x5 matrix of integers between 0 and 9. Then:
1. Compute the sum of all elements.
2. Compute the mean of each row.
3. Subtract the row mean from each element in that row (broadcasting).
4. Verify that now each row’s mean is approximately 0 (floating point rounding aside).
This exercise touches on creation, aggregation, broadcasting, and verification.

## 6. Summary and Self-Test (Multiple Choice Questions)

We have covered a lot of ground: from basic concepts of data, AI, and ML, to how we represent different data types as arrays, to linear algebra fundamentals (vectors, matrices, etc.), and a comprehensive introduction to NumPy for handling such data and computations. To summarize, here are some key takeaways:
- Data in computers is ultimately numeric; understanding how to structure data as vectors/matrices is key for machine learning.
- Linear algebra provides a language (and tools like dot products, matrix multiplication) to formulate data processing and ML algorithms efficiently ([Linear Algebra Required for Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-required-for-data-science/#:~:text=A%20solid%20understanding%20of%20linear,optimization%20and%20machine%20learning%20models)).
- AI/ML are transforming many real-life domains, and they heavily rely on numerical data manipulation (hence the importance of libraries like NumPy).
- NumPy offers high-performance array operations that make implementing linear algebra operations and data transformations straightforward and concise ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=NumPy%20understands%20that%20the%20multiplication,ValueError)).

Now, test your understanding with some multiple-choice questions. These will help reinforce important points from each section.

**1. Which of the following is a real-life example of machine learning in action?**  
A. A calculator performing addition and subtraction.  
B. An email service automatically filtering spam messages into a spam folder.  
C. Writing a for-loop to print numbers 1 to 100.  
D. A graphic designer manually creating an image in Photoshop.  
*Your Answer:* B. (Email spam filtering is typically done with ML algorithms that learned to recognize spam ([Machine Learning Examples, Applications & Use Cases | IBM](https://www.ibm.com/think/topics/machine-learning-use-cases#:~:text=ML%20algorithms%20in%20Google%E2%80%99s%20Gmail,categorize%20emails%20as%20they%20come)).)

**2. In the context of machine learning, what is a **feature vector**?**  
A. A special type of matrix used for images.  
B. A list of hyperparameters of a model.  
C. A list of numeric values that represent the characteristics of one data sample.  
D. The vector of predictions output by a model.  
*Your Answer:* C. (A feature vector is an array of features describing one sample, essentially a data point represented as a vector.)

**3. How is a grayscale image typically represented for processing in a computer?**  
A. As a list of objects each describing a pixel’s properties.  
B. As a 2D array (matrix) of intensity values for each pixel.  
C. As a 1D list of pixel coordinates.  
D. As an RGB tuple for each pixel.  
*Your Answer:* B. (A grayscale image can be seen as a 2D matrix of intensity values, where each entry is a pixel’s brightness.)

**4. What does the dot product of two vectors yield?**  
A. Another vector perpendicular to both.  
B. A scalar value.  
C. A matrix of pairwise multiplications.  
D. The angle between the two vectors (in degrees).  
*Your Answer:* B. (The dot product produces a single number ([How numpy dot works in Python? Best example - KajoData](https://kajodata.com/en/knowledge-base-excel-sql-python/knowledge-base-python-tech-skills/how-numpy-dot-works-in-python-examples-mmk/#:~:text=dot,dot%28a%2C%20b%29%20print%28result%29)), computed as sum of element-wise products.)

**5. If `A` is a 2×3 matrix and `B` is a 3×4 matrix, can you multiply them (A * B) and what will be the shape of the result if yes?**  
A. No, you cannot multiply a 2×3 with a 3×4.  
B. Yes, result will be 2×4.  
C. Yes, result will be 3×3.  
D. Yes, result will be 2×3.  
*Your Answer:* B. (Yes, inner dimensions match (3), so result is 2×4.)

**6. Which NumPy code snippet will correctly compute the element-wise **sum of two arrays** `x` and `y`? (Assume they have the same shape.)**  
A. `np.dot(x, y)`  
B. `x + y`  
C. `np.add(x, y)`  
D. Both B and C.  
*Your Answer:* D. (Using the `+` operator on arrays uses NumPy’s broadcasting addition, equivalent to `np.add`. Dot would do a dot product/matrix multiplication, which is not element-wise addition.)

**7. What is *broadcasting* in NumPy?**  
A. A way to send NumPy arrays to multiple CPU cores for parallel processing.  
B. A mechanism of expanding smaller arrays in an operation so that they match the shape of the larger array ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=NumPy%20understands%20that%20the%20multiplication,ValueError)).  
C. Converting a Python list into a NumPy array.  
D. A method of normalizing data to a standard scale.  
*Your Answer:* B. (Broadcasting allows operations on arrays of different shapes by implicitly expanding one of them to match the other's shape in that operation ([NumPy: the absolute basics for beginners — NumPy v2.2 Manual](https://numpy.org/doc/stable/user/absolute_beginners.html#:~:text=NumPy%20understands%20that%20the%20multiplication,ValueError)).)

**8. You have an array `data = np.array([[2, 4, 6],[1, 3, 5]])`. What will `data.mean(axis=0)` return?**  
A. The overall mean of all numbers in `data`.  
B. A 1D array of length 2 containing the mean of each row.  
C. A 1D array of length 3 containing the mean of each column.  
D. The same shape as `data` with each value replaced by the mean.  
*Your Answer:* C. (axis=0 means collapse rows, giving mean of each column. For each of the 3 columns: means would be [ (2+1)/2, (4+3)/2, (6+5)/2 ] = [1.5, 3.5, 5.5 ].)

**9. Which statement about eigenvectors is TRUE?**  
A. An eigenvector of matrix A is a vector that, when multiplied by A, results in zero.  
B. Eigenvectors of a matrix are always orthogonal to each other.  
C. PCA uses eigenvectors of the data’s covariance matrix to find principal components ([Linear Algebra Techniques in Data Science | GeeksforGeeks](https://www.geeksforgeeks.org/linear-algebra-techniques-in-data-science/#:~:text=Eigenvalues%20and%20eigenvectors%20are%20essential,the%20most%20important%20underlying%20factors)).  
D. Every square matrix has the same number of eigenvectors as its size (including multiplicity).  
*Your Answer:* C. (Principal Component Analysis indeed involves computing eigenvectors of the covariance matrix to identify principal directions in the data.)

**10. What does the NumPy function `np.linalg.inv()` do?**  
A. Computes the inverse of each element in an array (1/x).  
B. Computes the matrix inverse of a square matrix (if it exists).  
C. Inverts the sign of all elements in the array.  
D. It’s not a valid NumPy function.  
*Your Answer:* B. (`np.linalg.inv` returns the inverse of a given square matrix, such that A * A_inv = I.)

