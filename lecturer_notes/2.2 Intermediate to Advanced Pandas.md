
# Pandas for Intermediate/Advanced Users: Lecture Notes

## Introduction to Pandas  
Pandas is a powerful Python library for data manipulation and analysis, built on top of NumPy. It introduces two primary data structures: **Series** (1D labeled array) and **DataFrame** (2D labeled table). These provide tabular, spreadsheet-like data handling with labeled axes (rows and columns) and a rich set of operations for data wrangling. The key features of pandas include:  
- **Ease of Use:** Intuitive high-level APIs for filtering, aggregating, and transforming data.  
- **Alignment and Indexing:** Automatic alignment of data by labels (index), making it easy to merge and compare datasets.  
- **Integration with NumPy:** Fast vectorized operations under the hood (avoiding slow Python loops) and seamless conversion between pandas and NumPy data structures.  
- **Data I/O:** Utilities for reading/writing data from CSV, Excel, SQL, JSON, and more (not covered in detail here, but crucial for real-world use).  

Let's quickly recall the core data structures:  

- **Series:** A one-dimensional array of data with an index (like an ordered dictionary). Useful for representing a single column or vector of data. For example:  

  ```python
  import pandas as pd
  import numpy as np
  # Create a Series of GDP values indexed by country
  gdp = pd.Series([21.43, 2.83, 2.72], index=["USA", "UK", "France"])
  print(gdp)
  ```  

  This produces a labeled list of values, where each value is associated with a country name. We can access values by label (`gdp["USA"]`) or by integer position (`gdp.iloc[0]`).  

- **DataFrame:** A two-dimensional labeled data structure with columns (which can be thought of as a dict of Series sharing the same index). DataFrames allow heterogeneous data types across columns (e.g., numbers, strings, dates). For example:  

  ```python
  # Create a DataFrame of population (millions) and area (km^2) for some countries
  data = {
      "Country": ["USA", "UK", "France"],
      "Population": [331, 67, 65],
      "Area": [9834, 243, 551]  # areas in thousand km^2 for simplicity
  }
  df = pd.DataFrame(data)
  print(df)
  ```  

  This yields a table with columns *Country*, *Population*, *Area*. Each row is indexed by a default integer index (0,1,2) unless specified. We can set a column as index (`df.set_index("Country")`) to label rows by country.  

**Core Concepts Recap:** Pandas uses **indexes** to label rows, enabling intuitive data alignment for operations like join/merge or arithmetic (adding two Series/DataFrames aligns by index labels). Many operations (like filtering, selecting subsets with `.loc` and `.iloc`, boolean indexing, etc.) are based on these labels. Understanding the index is fundamental to advanced pandas features like MultiIndexing and time-series indexing.  

*Now that we have a quick recap of the basics, let's dive into advanced topics to leverage pandas' full power.*  

## MultiIndexing (Hierarchical Indexing)  
**MultiIndex** (hierarchical index) allows multiple levels of index (row or column) in a pandas Series or DataFrame. This is useful for handling higher-dimensional data in a 2D tabular format. For example, you might have data recorded by *Region* and *Product*, or *Year* and *Month*, and want to index by both. MultiIndexing provides a natural way to represent such data without resorting to multi-dimensional arrays.  

Key capabilities of MultiIndexing include:  
- **Hierarchical rows or columns:** You can have an index with multiple levels (e.g., Region -> Product -> Date) and/or columns with multiple levels (e.g., aggregated metrics by category).  
- **Convenient selection:** The `.loc` indexer allows selection using tuples for multiple levels (e.g., `df.loc[("North", "Product A"), :]` to get all data for Product A in North region).  
- **Reshaping with MultiIndex:** Functions like `stack` and `unstack` rely on MultiIndex to pivot data between wide and long formats (more on these in the reshaping section).  

**Creating a MultiIndex DataFrame:**  
You can create a MultiIndex for rows by providing multiple indexing arrays or using pandas helpers. For example, let's create a simple sales DataFrame indexed by Region and Product:  

```python
import pandas as pd
# Define multi-level index using a list of tuples or pd.MultiIndex
index = pd.MultiIndex.from_product([["North", "South"], ["Product A", "Product B"]], 
                                   names=["Region", "Product"])
sales = [100, 150, 80, 120]  # sales figures
df_multi = pd.DataFrame({"Sales": sales}, index=index)
print(df_multi)
```  

This yields a DataFrame with a two-level row index (Region, Product) and a single column **Sales**:  

```text
                  Sales  
Region  Product          
North   Product A   100  
        Product B   150  
South   Product A    80  
        Product B   120  
```  

Here, *North/Product A* corresponds to a sales value of 100, etc. Notice how the index is displayed hierarchically. The DataFrame’s `index.names` are *Region* and *Product*.  

**Selecting data in a MultiIndex:**  
To select subsets, use `.loc` with a tuple for multi-index values:  

- **Partial index:** You can specify just the first level to get all data for that level. For instance, `df_multi.loc["North"]` returns all rows where Region is North (both Product A and B).  
- **Full index tuple:** `df_multi.loc[("South", "Product B")]` returns the specific entry for South/Product B.  
- **Slicing:** You can slice within `.loc` by providing slice objects or range of labels for one or more levels. For example, `df_multi.loc[("North", slice(None)), :]` would fetch all rows in North for all products (`slice(None)` means “all” for that level).  

For example:  

```python
# Select all products in the North region
print(df_multi.loc["North"])
# Select the Sales for South region, Product B
print(df_multi.loc[("South", "Product B"), "Sales"])
```  

The first print will show the sub-DataFrame for *North* (still indexed by the second level *Product*), and the second print will output the scalar sales value (120 in this case).  

**MultiIndex Columns:**  
Similarly, DataFrame columns can have multiple levels. This happens, for instance, after aggregating with multiple functions (resulting in composite column names), or by design when storing related metrics under a top-level category. Accessing multi-level columns also uses tuple notation (e.g., `df.columns = pd.MultiIndex.from_product([["Metric1","Metric2"], ["mean","max"]])`). We will see an example of multi-index columns in the GroupBy section.  

**Stacking and Unstacking (pivoting with MultiIndex):**  
MultiIndex is tightly connected to pandas’ reshaping capabilities. The `.stack()` method moves an index level into the columns, and `.unstack()` moves a column level (or inner row index level) into the index. For example, if we want to pivot our `df_multi` so that *Product A* and *Product B* become columns and *Region* remains the index:  

```python
# Unstack the Product level to columns
df_unstacked = df_multi.unstack(level="Product")
print(df_unstacked)
```  

This produces:  

```text
          Sales            
Product Product A Product B  
Region                      
North        100       150  
South         80       120  
```  

Now the DataFrame has a single-level index (Region) and a MultiIndex in columns: the outer level is *Sales* (the original column name) and the inner level are the Products. In this format, we could easily compare or plot sales of Product A vs Product B by region. If we wanted to flatten the columns, since *Sales* is just an outer group with one value, we could do `df_unstacked.columns = df_unstacked.columns.droplevel(0)` to drop the redundant outer level.  

Conversely, `.stack()` would take a column level and push it into the index, increasing the index depth. Using it on `df_unstacked` would return us to the original shape.  

**Why MultiIndex?** It provides a clear way to handle and navigate multi-dimensional data. Instead of concatenating labels (e.g., making a single index like "North_ProductA"), you keep the data structured. This enables powerful grouping and reshaping operations, as well as clearer code when dealing with combinations of categories. It’s especially useful in time-series data with hierarchical grouping (Year/Month/Day, or different categories of time series), and in representing the result of groupby aggregations (which often produce MultiIndex).  

## Time Series Manipulation  
Pandas excels at time series data handling. It has a special index type for dates/timestamps and provides many functions to manipulate date/time indexed data. Key features include date range generation, frequency conversion (resampling), rolling/window operations, date shifting, and convenient date property access.  

**DateTime Index and Frequency:**  
To work with time series, we often set the DataFrame (or Series) index to datetime values (pandas `Timestamp` or `DatetimeIndex`). You can create a DatetimeIndex using `pd.date_range`. For example:  

```python
import pandas as pd
import numpy as np

# Create a date range for Jan 2022, daily frequency
dates = pd.date_range(start="2022-01-01", periods=10, freq="D")
# Create a Series of hypothetical stock prices for these dates
prices = pd.Series([116, 107, 106, 100, 109, 119, 105, 114, 118, 108], index=dates)
df_ts = prices.to_frame(name="Price")
print(df_ts.head())
```  

This yields a DataFrame with dates as the index and a single column Price:  

```text
            Price  
2022-01-01   116  
2022-01-02   107  
2022-01-03   106  
2022-01-04   100  
2022-01-05   109  
...           ...  
```  

Notice that pandas nicely formats the datetime index. With a DatetimeIndex, pandas enables time-based indexing and slicing. For example, `df_ts.loc["2022-01-03"]` would give the price on that date, and `df_ts.loc["2022-01-01":"2022-01-05"]` returns a slice of the DataFrame for the first 5 days of January. You can even use partial string matching: `df_ts.loc["2022-01"]` would get all data for January 2022.  

**Resampling and Frequency Conversion:**  
Resampling means converting a time series to a different frequency, aggregating or interpolating as needed. For instance:  
- **Downsampling (reduce frequency):** e.g., daily data to monthly data by aggregating (sum, mean, etc.).  
- **Upsampling (increase frequency):** e.g., monthly to daily (often needs interpolation or forward-filling).  

Pandas provides a `.resample()` method for time-indexed data. You call `.resample(<rule>)` and then an aggregation function. Common rules include `'M'` for month-end, `'W'` for week, `'D'` for day, `'H'` for hour, etc.  

Example – compute weekly average price from daily data:  

```python
# Resample to weekly frequency (W-SUN by default for week ending Sunday) and take the mean
weekly_mean = df_ts.resample("W").mean()
print(weekly_mean)
```  

Suppose our daily data spanned a couple of weeks, the output might look like:  

```text
                 Price  
2022-01-02    111.5    # average of data up to Jan 2 (first week)
2022-01-09    110.14   # average for week ending Jan 9
2022-01-16    108.0    # average for week ending Jan 16
```  

Each timestamp in the resampled result represents the period end (by default). We could specify `label='left'` or `label='right'` in `resample()` to adjust how labels are assigned, and `closed='left'/'right'` to adjust which end is inclusive for the interval.  

For monthly data, we might use `'M'` (month end) or `'MS'` (month start) depending on whether we want to label the period by its start or end date.  

**Rolling Window Operations:**  
Rolling (moving window) computations are essential in time series (for smoothing, moving averages, rolling sums, etc.). The `.rolling(window)` method creates a rolling object that you can apply aggregations on. The window can be a fixed number of periods or a time offset (like '7D' for a 7-day window if your index is dates). For example, to compute a 3-day moving average of our price:  

```python
df_ts["3-day MA"] = df_ts["Price"].rolling(window=3).mean()
print(df_ts[["Price", "3-day MA"]])
```  

The result will show a new column with the 3-day moving average of the price, where the first two entries are NaN (because a 3-day window isn't available until the third day). For instance:  

```text
            Price   3-day MA  
2022-01-01   116       NaN  
2022-01-02   107       NaN  
2022-01-03   106    109.67  
2022-01-04   100    104.33  
2022-01-05   109    105.00  
...           ...       ...  
```  

Rolling windows can also be combined with multiple aggregates or custom functions (via `.apply`). You can calculate rolling standard deviations, sums, etc. Common in finance are rolling mean (moving average) and rolling volatility (std).  

**Shift and Differences:**  
`.shift()` is a handy method to align data with a lag or lead. For time series, `df.shift(1)` will move the data forward by one time step (so value at today moves to tomorrow’s row), which is useful for creating lag features or calculating changes. For example, **percentage change** can be computed as:  

```python
df_ts["Pct_change"] = df_ts["Price"].pct_change()  # (current - prev)/prev
```  

This uses shift internally (the first entry will be NaN because there’s no previous day to compare). Similarly, `df_ts["Diff"] = df_ts["Price"].diff()` would give actual differences between consecutive days.  

**Date/Time Properties and Time Zones:**  
Pandas `DatetimeIndex` (and datetime Series via the `.dt` accessor) have many attributes: `.year`, `.month`, `.day_name()`, etc., to extract components of the date. For example, `df_ts.index.day_name()` might return an Index of weekday names for each date. These can be useful for grouping (e.g., average by weekday). Pandas also supports time zone-aware datetimes (`tz_localize` and `tz_convert` for setting or converting timezone).  

In summary, pandas makes time series analysis convenient: you can slice by date ranges, resample for aggregations like weekly/monthly summaries, use rolling calculations for moving stats, and easily compute lags or leads. We’ll see a practical example of time series analysis in the case studies section.  

## GroupBy Operations and Complex Aggregations  
The **groupby** operation is one of the most powerful features of pandas for data analysis. It allows you to split the data into groups based on some criteria, apply some function to each group independently, and then combine the results. This follows the *split-apply-combine* paradigm. Basic groupby usage is straightforward (e.g., grouping by a column and computing a single aggregate like mean or sum), but here we explore more complex scenarios: grouping by multiple keys, multiple aggregations, and custom aggregation functions.  

**Basic GroupBy Recap:**  
Suppose we have a DataFrame of sales:  

```python
df_sales = pd.DataFrame({
    "Region": ["North", "North", "South", "South"],
    "Product": ["A", "B", "A", "B"],
    "Sales": [250, 100, 300, 200]
})
# Group by Region and get total sales per region
print(df_sales.groupby("Region")["Sales"].sum())
```  

This will output the sum of Sales for North and South region separately:  

```text
Region  
North    350  
South    500  
Name: Sales, dtype: int64  
```  

But pandas can do much more than a single aggregation:  

**Grouping by Multiple Columns:**  
You can group by more than one key by passing a list of column names to `groupby`. The result will have a MultiIndex (as discussed in the MultiIndex section) with one level per group key. For example:  

```python
# Group by Region and Product, then sum sales
region_product_sales = df_sales.groupby(["Region", "Product"])["Sales"].sum()
print(region_product_sales)
```  

Output:  

```text
Region  Product  
North   A          250  
        B          100  
South   A          300  
        B          200  
Name: Sales, dtype: int64  
```  

Here the index of the result is a MultiIndex with levels (Region, Product). If you convert this Series to a DataFrame (using `.reset_index()` or `.to_frame()`), those index levels become regular columns again. Grouping by multiple keys is very useful for hierarchical aggregations (e.g., breakdown by region and product).  

**Multiple Aggregations on Different Columns:**  
Pandas lets you apply different aggregation functions to different columns in one go. Use `.agg()` with a dictionary mapping column names to functions or list of functions. The result will have MultiIndex columns where the top level is the original column and the lower level is the function applied. For example, let's consider a DataFrame with multiple value columns:  

```python
df = pd.DataFrame({
    "Category": ["A","A","A","B","B","B"],
    "Type": ["X","Y","X","Y","X","Y"],
    "Value1": [10, 20, 15, 10, 5, 15],
    "Value2": [1, 3, 5, 2, 4, 6]
})
# Group by Category and compute multiple aggregations:
agg_results = df.groupby("Category").agg({
    "Value1": "mean",            # mean of Value1 per category
    "Value2": ["min", "max"]     # min and max of Value2 per category
})
print(agg_results)
```  

Output will be a DataFrame with multi-level columns:  

```text
          Value1     Value2       
            mean   min   max  
Category                    
A           15.0   1     5  
B           10.0   2     6  
```  

Here **Category A** had Value1 mean = 15.0, Value2 min=1, max=5; **Category B** had Value1 mean = 10.0, Value2 min=2, max=6. The columns are a MultiIndex: *Value1* (mean) and *Value2* (min and max). To access a specific result, you could use `agg_results["Value2", "max"]` or similarly. If needed, you can flatten the column index after aggregation using `agg_results.columns = ['_'.join(col) for col in agg_results.columns]` to get single-level column names like `Value1_mean`, `Value2_min`, etc.  

**Custom Aggregation Functions:**  
Besides built-in functions ('mean','sum','min','max','count','std', etc.), you can pass your own function to `.agg()` or use `.apply()` on groupby objects. For instance, say we want to compute the range (max-min) of `Value2` for each Category:  

```python
range_per_cat = df.groupby("Category")["Value2"].apply(lambda x: x.max() - x.min())
print(range_per_cat)
```  

This will output a Series with the range of Value2 for each category (in our data, Category A range = 4, B range = 4 as well). Using `.apply` on a group applies the function to each sub-DataFrame (or Series if selecting a column) corresponding to each group. Use `.apply` for more complex computations that might not be expressible as a single built-in aggregation. Keep in mind `.apply` is flexible but can be slower than built-ins.  

**Grouping by Index or a Function:**  
You are not limited to grouping by existing columns. You can group by the DataFrame’s index (pass `level` parameter) or even by a transformation of a column. For example, if `df` had a date column and you wanted to group by year: `df.groupby(df["date"].dt.year).sum()`. You can also supply a function to `groupby`, which will be called on the index to determine groups.  

**GroupBy + Filter/Transform:**  
- `.filter()` allows you to filter out whole groups based on a group-wise condition. For instance, `df.groupby("Category").filter(lambda g: g["Value1"].mean() > 10)` would keep only categories whose average Value1 is > 10.  
- `.transform()` computes a result for each group and broadcasts it back to the original shape. It’s useful for creating new columns that are group-wise computations but aligned with the original DataFrame. For example, `df["Value1_group_mean"] = df.groupby("Category")["Value1"].transform("mean")` will add a column with the mean Value1 of the respective category for each row.  

In summary, GroupBy allows sophisticated data aggregation and transformation. You can get summary statistics broken down by categories, apply custom business logic per group, and even produce higher-dimensional outputs (via multi-level indices/columns) to capture multiple summaries at once. This is a backbone of data analysis tasks like summarizing sales by various dimensions, computing KPIs per group, etc.  

## Data Reshaping: Melt, Pivot, Stack, and Unstack  
Data often comes in “wide” or “long” formats, and different analyses or libraries might require different shapes. Pandas provides powerful functions to reshape data:  
- **Pivot / Pivot_table**: turn long format data into a wide format (spreading rows into columns).  
- **Melt**: the reverse of pivot, turning wide format into a long, normalized format.  
- **Stack/Unstack**: switch data between row MultiIndex and column MultiIndex representations.  

These operations help in transforming data for analysis or visualization (e.g., going from a table of yearly values to a time series, or vice-versa).  

**Melt (Unpivot):**  
`pd.melt()` converts wide data (many columns) into long format (columns “variable” and “value”). You typically specify an identifier column(s) to keep, and melt all other columns.  

For example, suppose we have population of cities in 2010 and 2020 in a wide format:  

```python
df_wide = pd.DataFrame({
    "City": ["New York", "Los Angeles", "Chicago"],
    "2010": [8.0, 3.8, 2.7],   # population in millions
    "2020": [8.3, 3.9, 2.7]
})
print(df_wide)
```  

This shows:  

```text
          City   2010  2020  
0     New York   8.0   8.3  
1  Los Angeles   3.8   3.9  
2      Chicago   2.7   2.7  
```  

Each row is a city, with two separate year columns. If we want a long format where each row is a (City, Year, Population) triplet, we can melt:  

```python
df_long = df_wide.melt(id_vars="City", var_name="Year", value_name="Population")
print(df_long)
```  

Output:  

```text
          City   Year  Population  
0     New York   2010         8.0  
1  Los Angeles   2010         3.8  
2      Chicago   2010         2.7  
3     New York   2020         8.3  
4  Los Angeles   2020         3.9  
5      Chicago   2020         2.7  
```  

Now each row is one city-year combination, which is often easier to feed into plotting functions or analyze year-over-year changes. Notice that Year came out as a string here (since the original column names were strings "2010", "2020"). We could convert the Year column to int or to datetime (e.g., `pd.to_datetime(df_long["Year"], format="%Y")`) if needed.  

**Pivot and Pivot Table (Wide Format):**  
`DataFrame.pivot` is essentially the opposite of melt. Given a DataFrame with columns identifying *index*, *columns*, and *values*, it will reshape the data so that unique combinations of index and column keys become a table. Use `pivot_table` when you have duplicate entries for an index/column pair and need to aggregate (it’s like Excel’s Pivot Table).  

For example, starting from the `df_long` we just created (City, Year, Population), we can pivot it back to wide form:  

```python
df_wide_again = df_long.pivot(index="City", columns="Year", values="Population")
print(df_wide_again)
```  

This yields:  

```text
Year         2010  2020  
City                   
Chicago       2.7   2.7  
Los Angeles   3.8   3.9  
New York      8.0   8.3  
```  

The DataFrame index is City, columns are Year, and values are populations. This matches our original wide format (possibly sorted differently by index). If there were multiple entries for a given city-year in the long data, `pivot` would fail due to duplicates; in that case, `pivot_table` is used with an aggregation function.  

**Example of pivot_table with aggregation:**  
Using our earlier `df` with Category, Type, Value1, Value2, suppose we want a table of the mean Value1 for each Category vs Type combination (a 2D matrix). We noticed we have duplicates for each Category-Type in that data. We can do:  

```python
mean_val1 = df.pivot_table(index="Category", columns="Type", values="Value1", aggfunc="mean")
print(mean_val1)
```  

Output might be:  

```text
Type         X      Y  
Category              
A         12.5   20.0  
B          5.0   12.5  
```  

This indicates for Category A, Type X average Value1 is 12.5, Type Y is 20; for B, X is 5, Y is 12.5. This matches the data we had (Category A had two X entries 10,15 avg 12.5; one Y entry 20; Category B had one X=5, two Y 10,15 avg 12.5). `pivot_table` gracefully handled the duplicates by taking the mean. If we wanted sum instead, we could use `aggfunc="sum"`.  

**Stack and Unstack:**  
These are similar to melt/pivot but work with hierarchical indexes. `.stack()` moves the innermost column index to become the innermost row index. `.unstack()` moves the innermost row index to become the innermost column index. They are very useful when you have MultiIndexed DataFrames. We already saw `df_multi.unstack("Product")` example in the MultiIndex section.  

Another use: if you have a DataFrame with some columns that really represent a dimension, you can set them as index and then stack. For example, if you had columns for different test scores like `Math_score, Eng_score` for each student, you could do `df.set_index(["Student"]).stack()` (stacking by default stacks column labels into a row index level) to get a Series with a MultiIndex (Student, subject) mapping to score.  

**Transpose:**  
Don’t forget, for simpler reshaping, `df.T` will transpose rows and columns of a DataFrame (swap them). This is useful occasionally when you want to flip a dataset for presentation or to align differently.  

**Summary:** Reshaping is about converting data formats: *wide to long* (melt/stack) or *long to wide* (pivot/unstack). Mastering these allows you to prepare data in the form best suited for analysis or visualization. Often, data is stored in a long normalized form (one observation per row), but analysis might require a cross-tab (pivot), or you may need to take a cross-tab and melt it for an algorithm input. Pandas makes these transformations straightforward.  

## Efficient Joins and Merges  
Data rarely comes in one table; pandas provides SQL-like merge operations to combine DataFrames. Joins/Merges in pandas are done with `pd.merge()` or the DataFrame `.merge()` method, and `DataFrame.join()` for convenience when joining on index. As an intermediate/advanced user, it's important to understand merge types and how to use indexes effectively in merges.  

**Types of Joins:**  
- **Inner Join:** Only include rows with keys present in both tables (intersection).  
- **Left Join:** Include all rows from the “left” table, and matching info from the right (if no match, result will have NaN for right-side columns).  
- **Right Join:** Opposite of left (all rows from right table, with possible NaN for left columns).  
- **Outer Join:** Include all rows from both tables (full union of keys, NaN where one side is missing).  

The merge key can be one or multiple columns (even a combination of columns with different names via `left_on` and `right_on`). If not specified, pandas will merge on any common column names by default, or on indices if you use `join` or specify `left_index=True`.  

**Example Scenario:** We have a customer table and an orders table. We want to combine them to analyze customer behavior.  

```python
customers = pd.DataFrame({
    "CustomerID": [1, 2, 3],
    "Name": ["Alice", "Bob", "Charlie"],
    "Segment": ["Premium", "Standard", "Standard"]
})
orders = pd.DataFrame({
    "OrderID": [101, 102, 103, 104],
    "CustomerID": [1,   2,   1,   4],
    "Amount": [250, 100, 200, 150]
})
print("Customers:\n", customers)
print("Orders:\n", orders)
```  

**Inner join** (customers with orders):  

```python
inner_merged = pd.merge(customers, orders, on="CustomerID", how="inner")
print(inner_merged)
```  

This will include only customers that have orders (common CustomerID in both):  

```text
   CustomerID   Name   Segment  OrderID  Amount  
0           1  Alice   Premium      101     250  
1           1  Alice   Premium      103     200  
2           2    Bob  Standard      102     100  
```  

Customer 3 (Charlie) is not in the result because he had no orders, and Order 104 with CustomerID 4 is not included because customer 4 wasn’t in the customer list.  

**Left join** (all customers, attach orders if available):  

```python
left_merged = pd.merge(customers, orders, on="CustomerID", how="left")
print(left_merged)
```  

Output (with left join, Customer 3 appears with NaNs for order info):  

```text
   CustomerID     Name   Segment  OrderID  Amount  
0           1    Alice   Premium    101.0   250.0  
1           1    Alice   Premium    103.0   200.0  
2           2      Bob  Standard    102.0   100.0  
3           3  Charlie  Standard      NaN     NaN  
```  

Charlie has no orders, so OrderID and Amount are NaN for that row. Notice Alice appears twice because she had two orders (the merge duplicates the customer info for each matching order).  

**Outer join** (all customers and all orders):  

```python
outer_merged = pd.merge(customers, orders, on="CustomerID", how="outer")
print(outer_merged)
```  

This would include the order with CustomerID 4 as well, with NaN for customer Name/Segment, and Charlie with NaN for orders:  

```text
   CustomerID     Name   Segment  OrderID  Amount  
0           1    Alice   Premium    101.0   250.0  
1           1    Alice   Premium    103.0   200.0  
2           2      Bob  Standard    102.0   100.0  
3           3  Charlie  Standard      NaN     NaN  
4           4      NaN       NaN    104.0   150.0  
```  

Typically, you choose the join type based on context (e.g., left join to keep all customers even if no orders, inner to only analyze those with orders, etc.).  

**Merging on Index:** If your key is the index of one or both DataFrames, you can use `left_index=True` and/or `right_index=True`. Alternatively, if both DataFrames have the same index that is the key, `df1.join(df2)` is a shortcut. For example, if `customers` was indexed by CustomerID (after `customers.set_index("CustomerID")`), we could do `customers.join(orders.set_index("CustomerID"), how="left")`.  

**Concatenation (Appending):** Another common operation is stacking DataFrames vertically or side by side. Use `pd.concat` for this:  
- To append rows: `pd.concat([df1, df2], axis=0)`. This requires the columns to match (or will include all columns union).  
- To concatenate columns: `pd.concat([df1, df2], axis=1)`. This aligns on index (like a join on index).  

There's also `df.append(df2)` which is essentially a concat on rows (to be safe, prefer `pd.concat` in new code since `append` might be deprecated).  

**Efficient merging tips:**  
- Ensure the key columns are indexed if merging large DataFrames; merging on index can be faster in some cases, or sort your data on the key for potential speed gains.  
- Pandas will use efficient hash join algorithms under the hood, but if you have a sorted merge (like time series data where you want to merge on or near a sorted timestamp), consider using `pd.merge_asof` for time-based merging or when merging on nearest keys (common in syncing time series by timestamp tolerance).  
- For very large merges that don't fit in memory, you may need to resort to chunking or using databases/Dask, but within pandas, merges are optimized in C code and generally quite fast for moderately large data (millions of rows).  

In summary, mastering merges allows you to combine disparate data sources: enriching a dataset with additional features, creating master tables for analysis, or joining results of different computations. It’s analogous to SQL joins, with flexibility to join on multiple keys and use indices.  

## Handling Missing Data and Outliers  
Real-world data is messy. Pandas provides a suite of tools to identify and handle missing values (NaNs) and to detect or handle outliers. Properly dealing with missing data and outliers is critical to maintain data integrity for analysis.  

**Missing Data (NaN) in Pandas:**  
Missing or null values in pandas are typically represented by `NaN` (Not a Number) for floats and `NaT` (Not a Time) for datetimes (and `None` or `NaN` in object dtype). Key functions:  
- `df.isna()` or `df.isnull()`: boolean mask of same shape indicating missing entries.  
- `df.notna()` or `df.notnull()`: inverse of the above.  
- `df.dropna(...)`: remove missing values.  
- `df.fillna(...)`: fill in missing values with a specified strategy.  

**Detecting and dropping missing values:**  

```python
import numpy as np
df_miss = pd.DataFrame({
    "A": [1, np.nan, 3, 4],
    "B": [np.nan, 5, 6, 1000]
})
print(df_miss)
# Drop any rows that contain NaN in any column
print(df_miss.dropna())
```  

Original `df_miss`:  

```text
     A       B  
0  1.0     NaN  
1  NaN     5.0  
2  3.0     6.0  
3  4.0  1000.0  
```  

After `dropna()`:  

```text
     A      B  
2  3.0    6.0  
3  4.0  1000.0  
```  

By default, `dropna()` drops any row with *any* missing value. We could also drop columns with missing values by `df_miss.dropna(axis=1)`, or only drop rows if *all* values are missing via `how='all'`, or specify a subset of columns to consider. In practice, you often drop NA when the row is not useful without those values, or you drop columns if too many missing.  

**Filling missing values:**  
Common strategies include filling with a constant (zero, or a placeholder), forward-fill/backward-fill (propagate last known value forward or backward, good for time series or categorical data like filling last known category), or using a summary statistic (mean/median). Pandas `fillna` can take a scalar, dict (to fill different columns with different values), or use methods `'ffill'` (forward fill) or `'bfill'` (backward fill).  

```python
# Fill missing A with the mean of A, missing B with 0
filled = df_miss.fillna({"A": df_miss["A"].mean(), "B": 0})
print(filled)
```  

This yields:  

```text
          A       B  
0  1.000000     0.0   # B filled with 0  
1  2.666667     5.0   # A filled with mean(1,3,4)=2.667  
2  3.000000     6.0  
3  4.000000  1000.0  
```  

**Interpolation:** For numeric sequences or time series, `df.interpolate()` can fill missing values by interpolating between known values (linear, polynomial, time, etc.). In our example, if we interpolate column A linearly:  

```python
print(df_miss.interpolate())
```  

We get A's missing value filled by linear interpolation (2.0, since halfway between 1 and 3), while B's first value remains NaN because interpolation can't infer without a starting point before it:  

```text
     A       B  
0  1.0     NaN  
1  2.0     5.0  
2  3.0     6.0  
3  4.0  1000.0  
```  

**Handling Outliers:**  
Outliers are unusual values that deviate significantly from others. They can result from errors or genuine extreme variations. Pandas itself doesn’t have a one-click outlier removal, but provides tools to detect and handle them:  
- You can use statistical methods to find outliers: e.g., values beyond 3 standard deviations from the mean, or beyond certain quantiles.  
- Once identified (via boolean masks), you can decide to drop them or cap (winsorize) them.  

For example, in column B of `df_miss`, the value 1000 is an outlier compared to others (~5,6). We might decide to cap it at a certain threshold or replace it with a more typical value (like median).  

```python
# Identify outliers in B > 100 (just an arbitrary threshold for this example)
outliers = df_miss["B"] > 100
print(df_miss[outliers])
# Replace outliers in B with the median of non-outliers
median_B = df_miss.loc[~outliers, "B"].median()
df_miss["B"] = np.where(outliers, median_B, df_miss["B"])
print(df_miss)
```  

First, the identified outlier row:  

```text
     A       B  
3  4.0  1000.0  
```  

After replacing B outliers with median (which was median of [5,6] = 5.5, so roughly 6 if using integer or leaving float):  

```text
     A    B  
0  1.0  NaN  
1  NaN  5.0  
2  3.0  6.0  
3  4.0  5.5  
```  

(If we had filled NA first or chosen to drop NA, we could better handle row0's NaN too. In practice, you'd handle missing and outliers in a coordinated way.)  

Other approaches:  
- **Z-score method:** Compute `(value - mean)/std` for a column, and consider values with z-score beyond a threshold (like 3 or 4) as outliers. You can do this easily with pandas: `z = (df[col] - df[col].mean())/df[col].std()`, then use `abs(z) > 3`.  
- **Quantiles:** Identify the 1st and 99th percentile, for example, and cap data at those. E.g., `lower, upper = df[col].quantile([0.01, 0.99]); df[col] = df[col].clip(lower, upper)`.  

**Missing Data in Merging/Joins:** As we saw, merges can introduce NaNs for unmatched entries. The same principles apply: you might fill them (for example, fill missing order amounts with 0 meaning no orders), or filter them out depending on context.  

In summary, pandas enables robust handling of missing data through dropping or filling, and flexible identification of outliers for removal or treatment. As an advanced user, you should choose strategies that make sense for your dataset (e.g., forward-fill for time series sensor data vs median fill for numeric data vs dropping highly sparse columns) and be aware of how these decisions impact your analysis.  

## Performance Optimization in Pandas  
Pandas is very powerful, but with power comes the need to use it properly for performance. Intermediate/advanced users often face large datasets and computationally heavy operations. Here we cover some tips for optimizing performance: vectorization, avoiding Python-level loops, using appropriate dtypes (especially categorical), and memory footprint considerations.  

**1. Vectorization vs Loops:**  
Pandas (and NumPy) operations are heavily optimized in C, and you should leverage them instead of Python loops. **Vectorized operations** mean applying an operation on the entire array/Series at once, rather than iterating Python objects. This is not only more concise but drastically faster.  

For example, suppose we want to square a million numbers. Compare a vectorized approach vs a Python loop:  

```python
import numpy as np
arr = np.arange(1000000)
# Vectorized operation
result_vec = arr * 2   # multiply every element by 2
# Python loop (not using pandas, but for illustration)
result_loop = [x * 2 for x in arr]
```  

Even without timing, we can assert that `result_vec` and `result_loop` produce the same result. However, the vectorized version leverages optimized C code underneath. In timing tests, vectorization can be **orders of magnitude faster** than pure Python loops (for 1e6 elements, often 50x+ faster). In pandas, this means: whenever possible use array operations (`+`, `-`, `*`, `**`, `.mean()`, `.sum()`, etc.) on Series/DataFrames rather than iterating with `for` or using DataFrame.apply row-by-row.  

For instance, to create a new column as a function of existing columns, do:  
```python
df["new_col"] = df["col1"] + df["col2"]
```  
This will be much faster than:  
```python
df["new_col"] = df.apply(lambda row: row["col1"] + row["col2"], axis=1)  # avoid this
```  
The `.apply` with axis=1 loops in Python over rows. While `.apply` is sometimes necessary for complex logic, always check if a vectorized method or a combination of built-in functions can achieve the same result. Similarly, operations like boolean filtering (`df[df["col"] > 0]`) are vectorized and fast.  

**2. Use Categorical dtype for repetitive text data:**  
If you have string (object dtype) columns with many repeated values (like category labels, IDs, etc.), converting them to `category` dtype can save memory and potentially speed up operations like sorting or groupby. A categorical dtype internally stores data as integers (codes) with a lookup table for categories. Memory usage becomes proportional to number of unique categories plus length of the data (instead of length * size of each string).  

Example:  

```python
n = 10000
df_perf = pd.DataFrame({"label": ["CategoryA"] * (n//2) + ["CategoryB"] * (n//2)})
print("Original memory (bytes):", df_perf["label"].memory_usage(deep=True))
df_perf["label_cat"] = df_perf["label"].astype("category")
print("After categorical (bytes):", df_perf["label_cat"].memory_usage(deep=True))
```  

This might output something like:  

```text
Original memory (bytes): 660000  
After categorical (bytes): 10464  
```  

The exact numbers can vary, but clearly the categorical column uses a tiny fraction of memory compared to the original object column. In our example ~0.66 MB vs ~10 KB, because we had only 2 unique categories. This memory benefit grows with the repetitiveness of data.  

Besides memory, categorical can also speed up operations like comparisons and groupings since they operate on integer codes. However, note that if nearly every value is unique (like unique IDs), categorical might not help much or could even use more memory overhead. Use it when appropriate (e.g., columns like "Country", "Gender", "Segment", etc., which have a fixed set of categories).  

**3. Use appropriate numeric dtypes:**  
By default pandas may use 64-bit floats or ints for numeric data. For large datasets, using smaller precision can save memory and sometimes improve speed due to better cache usage. For example, if you have an integer column that only ranges 0-100, using `np.int8` (8-bit) instead of `np.int64` (64-bit) cuts memory usage to 1/8.  

```python
arr_int64 = pd.Series(np.random.randint(0, 100, size=10000), dtype=np.int64)
arr_int8  = arr_int64.astype(np.int8)
print(arr_int64.memory_usage(deep=True), "bytes for int64 vs", 
      arr_int8.memory_usage(deep=True), "bytes for int8")
```  

This will show a substantial reduction (e.g., ~80k bytes vs ~10k bytes for 10k elements, consistent with 8x difference). Similarly, if you have float data that doesn’t require double precision, using float32 cuts memory in half. Be mindful of the range and precision needs: downcasting too far can lead to overflow or precision loss.  

**4. Avoid copying data unnecessarily:**  
Many pandas operations return new objects (since pandas data structures are immutable in many operations). Chaining operations can sometimes create intermediate copies of data. Where possible, use in-place modifications (`inplace=True` for certain ops like `df.dropna(inplace=True)`) to save memory, but note that `inplace=True` might not always give performance improvements and is being phased out of some pandas APIs. Another trick is to reuse columns rather than creating too many intermediate columns.  

Also, if you need to iterate row-by-row (which we try to avoid), prefer using `itertuples()` instead of `iterrows()`. `iterrows()` yields Series for each row (heavy), whereas `itertuples()` yields namedtuples (or regular tuples if `name=None`) which is much faster to iterate in Python.  

**5. `eval` and `query`:**  
Pandas provides `df.eval(expr)` and `df.query(expr)` which can sometimes speed up operations by parsing expressions and performing them in a vectorized manner or even using multiple cores. For example, `df.eval('new_col = col1 + col2')` might be slightly faster and is certainly more concise for many column operations. `df.query('col1 > 5 & col2 < 10')` can filter rows using a string expression. These are not magic speed boosters for all cases, but can help in writing cleaner code and sometimes performance, especially with large DataFrames and simple expressions, by avoiding the creation of temporary intermediate Series.  

**6. Use NumPy or specialized libraries for heavy number-crunching:**  
If you find yourself needing to do very heavy computations on data (e.g., complex numerical simulations or very large matrix operations), consider extracting the underlying NumPy arrays (`.to_numpy()` or `.values`) and using NumPy, SciPy or numba for those sections. Pandas overhead can make very large iterative computations slow, so going to NumPy (which pandas uses internally anyway) can be an optimization. For example, to compute something like a cosine transform on a column of numbers, doing `np.cos(df['col'].to_numpy())` will be faster than `df['col'].apply(math.cos)`.  

**7. Chunking for large I/O:**  
When reading very large files, use the `chunksize` parameter in `pd.read_csv` to read in portions and process in chunks, rather than reading the entire file into memory if it's huge. For writing, similar approach might be needed if memory is an issue. This is more of a memory management tip, but relevant to performance when dealing with dataset size beyond RAM.  

**Summary:** To optimize pandas, remember: utilize vectorized operations, choose optimal data types (categoricals for repeated labels, smaller numeric types when possible), and be mindful of operations that implicitly loop or copy data. For extremely large data, consider using Dask (parallelized pandas) or a database, but within pandas, these practices go a long way to making your code efficient for intermediate-sized data (up to a few million rows typically).  

## Integrating Pandas with Text Data (NLP preprocessing)  
While pandas is primarily used for structured numeric data, it is also very useful for handling text data as part of your data analysis workflow. Typically, raw text needs to be cleaned and transformed into features (like word counts, presence of keywords, sentiment scores) for analysis or modeling. Pandas provides string handling functions and the ability to apply custom preprocessing functions, making it convenient to integrate with NLP tasks.  

**The `.str` accessor:**  
Pandas Series with dtype `object` (or string dtype) have a `.str` attribute which lets you vectorize string operations (similar to Python’s string methods, but applied to each element). Some useful ones include:  
- `str.lower()`, `str.upper()` – convert case.  
- `str.strip()` – trim whitespace (and `str.strip(chars)` to trim specific characters).  
- `str.contains(pattern)` – vectorized substring or regex search, returns boolean Series.  
- `str.replace(pattern, repl)` – replace substrings or regex patterns.  
- `str.split(sep)` – split strings into lists (often then used with `.explode()` to get one word per row).  
- `str.len()` – length of each string.  
- Many more: extract with regex, find, startswith, endswith, etc.  

**Example 1: Word count in text column**  
Suppose we have a DataFrame of customer reviews:  

```python
df_reviews = pd.DataFrame({
    "product": ["A", "A", "B", "B"],
    "review": [
        "Absolutely love this product, great quality!",
        "Pretty good, could be better.",
        "Not what I expected, but okay.",
        "Excellent value, I love it!"
    ]
})
# Count number of words in each review
df_reviews["word_count"] = df_reviews["review"].str.split().str.len()
print(df_reviews[["product", "review", "word_count"]])
```  

We create a new column `word_count` by splitting each review on whitespace and getting the length of the resulting list. The output might look like:  

```text
  product                                       review                             word_count  
0       A    Absolutely love this product, great quality!       6  
1       A                   Pretty good, could be better.       5  
2       B                  Not what I expected, but okay.       6  
3       B                     Excellent value, I love it!       5  
```  

Now, perhaps we want to do a simple sentiment analysis: count positive words in each review. We can define a list or set of positive words and use `.apply` or `.str.contains`. A straightforward approach:  

```python
positive_words = {"love", "great", "excellent", "good"}
def sentiment_score(text):
    # simple scoring: +1 for each positive word
    score = 0
    # Lowercase and remove punctuation for matching
    words = text.lower().replace('[^\w\s]', '', regex=True) if isinstance(text, str) else ""
    for word in words.split():
        if word in positive_words:
            score += 1
    return score

df_reviews["sentiment"] = df_reviews["review"].apply(sentiment_score)
print(df_reviews[["review", "sentiment"]])
```  

For our sample, this yields a sentiment score for each review:  

```text
review                                          sentiment  
"Absolutely love this product, great quality!"       2  
"Pretty good, could be better."                     1  
"Not what I expected, but okay."                    0  
"Excellent value, I love it!"                       2  
```  

We see the first review got 2 (for "love" and "great"), second 1 ("good"), third 0 (no positive words from our list), fourth 2 ("Excellent" and "love"). This is a simplistic sentiment measure, but it demonstrates using pandas to apply a custom function to text. (Note: We used Python string replace with regex via pandas; we could also use `re` module. Also, pandas has a built-in regex replace if given a pattern, but an easier way to strip punctuation could be with `str.replace` and a regex pattern, which might need a slight tweak above to use properly).  

We can now use pandas operations on these new numeric features. For example, group by product to see average sentiment:  

```python
avg_sentiment = df_reviews.groupby("product")["sentiment"].mean()
print(avg_sentiment)
```  

Output:  

```text
product  
A    1.5  
B    1.0  
Name: sentiment, dtype: float64  
```  

So product A’s average sentiment is 1.5, product B’s is 1.0 in this small sample, indicating reviews for A were slightly more positive on average.  

**Text data wrangling with pandas** can also involve:  
- Removing or filtering out rows based on text content (e.g., filter out reviews that are too short: `df[df["review"].str.len() > 10]`).  
- Joining text data with structured data. For instance, if you have a table of users and another of their tweets, you might merge them to get user attributes alongside tweet text for analysis.  
- Creating new features from text: length of text, number of special characters, does it mention a keyword, etc. All these can be done using `.str` operations and vectorized comparisons.  

**Integration with NLP libraries:**  
You might use pandas as the data table to organize text and results, while delegating heavy NLP tasks to specialized libraries (NLTK, spaCy, scikit-learn's text vectorizers, etc.). For example, you could use scikit-learn’s `CountVectorizer` to get a word frequency matrix from a Series of text, then convert that to a DataFrame or join back with the original DataFrame. Or use spaCy to get sentiment or named entities and store those in new columns. Pandas is flexible enough to store lists of tokens or even entire parsed objects in cells (though keep an eye on performance/memory if doing so).  

**Efficient string operations:** Under the hood, pandas uses vectorized methods that are quite efficient for many string tasks, but very large text data or complex regex might benefit from specialized libraries or compiled routines. Still, for moderate text data, using pandas to clean and structure it is usually convenient and fast enough.  

In summary, pandas can serve as a bridge between raw text and structured analysis: it can hold your text data, help you clean it (lowercase, remove unwanted characters, etc.), and transform it into features or summary statistics that can be analyzed just like any other numeric data. This makes it a valuable tool in NLP preprocessing pipelines and text-based data analysis.  

## Integrating Pandas with Image Data (Using NumPy Arrays)  
Images are not a typical tabular data source, but you can still use pandas to assist in managing and analyzing image-derived data. Images can be represented as numerical arrays (via libraries like PIL/Pillow or OpenCV to read image files into NumPy arrays). While specialized libraries handle image processing, pandas can be useful to organize results, image metadata, or even pixel data in a tabular format for analysis.  

Here are a few ways pandas can interact with image data:  

- **Storing image metadata:** You can create a DataFrame where each row corresponds to an image, with columns for metadata (filename, image width, height, format, mean pixel intensity, etc.). This makes it easy to filter or group images (e.g., find all images above a certain resolution, or average intensity per category of images).  
- **Storing images as arrays:** You could have a DataFrame where each cell (or each row/column combination) holds a pixel value, but for large images this isn’t memory-efficient. Instead, one might use pandas to store a smaller representation (like flattening the image to a row vector if the dataset is not huge, or storing a reference to the NumPy array).  
- **Pixel-level analysis:** For small images or downsampled data, you could use pandas for analysis. For example, treating an image as a 2D DataFrame can allow you to use pandas operations to compute row/column summaries or apply filters.  

**Example 1: Image as DataFrame for analysis**  
Suppose we have a small 5x5 grayscale image (for demonstration, we'll simulate one with a NumPy array). We can convert it to a DataFrame:  

```python
import numpy as np
# Simulate a 5x5 image with values 0-24
img_array = np.arange(25).reshape(5, 5)  # each value could represent a pixel intensity
img_df = pd.DataFrame(img_array)
print("Image DataFrame:\n", img_df)
# Compute mean intensity of each row and each column:
row_means = img_df.mean(axis=1)
col_means = img_df.mean(axis=0)
print("Mean intensity per row:", row_means.tolist())
print("Mean intensity per col:", col_means.tolist())
```  

The `img_df` printed might look like:  

```text
Image DataFrame:
    0   1   2   3   4
0   0   1   2   3   4
1   5   6   7   8   9
2  10  11  12  13  14
3  15  16  17  18  19
4  20  21  22  23  24

Mean intensity per row: [2.0, 7.0, 12.0, 17.0, 22.0]
Mean intensity per col: [10.0, 11.0, 12.0, 13.0, 14.0]
```  

Each row’s mean increases (image getting brighter toward bottom-right corner), and each column’s mean increases left to right. This is trivial from our constructed data, but imagine this was an actual image – such analysis could tell you about lighting gradients, etc.  

We can also use pandas to filter pixel values or reshape the image:  

```python
# Flatten image to long format using stack (row, col, value)
pixels = img_df.stack().reset_index()
pixels.columns = ["row", "col", "value"]
# Filter pixels above a certain intensity
bright_pixels = pixels[pixels["value"] > 20]
print(bright_pixels)
```  

This would list all pixels with value > 20:  

```text
    row  col  value
21    4    1     21
22    4    2     22
23    4    3     23
24    4    4     24
```  

We see all these are in row 4 (the last row) with col 1-4, reflecting our data. We could then pivot back if needed:  

```python
# Reconstruct image from pixels table
reconstructed = pixels.pivot(index="row", columns="col", values="value")
```  

This would give back the original 5x5 matrix.  

While this example is contrived, it shows that you can use pandas to manage the pixel values. In practice, for a single large image, you wouldn't do this (NumPy or image libraries are better suited). But for analysis of many images, you might do something like:  

**Example 2: Image dataset analysis**  
Imagine you have 100 images and want to compare their average brightness and contrast. You could loop through images with PIL or OpenCV, compute metrics for each (mean pixel value, standard deviation of pixel values), and store those in a DataFrame with columns ["image_name", "mean_intensity", "std_intensity", "width", "height"]. Then you can use pandas to:  
- Sort images by brightness,  
- Filter images that are dark or very high contrast,  
- Group by some category (if images have labels like "indoor" vs "outdoor") to get average properties per group, etc.  

Additionally, if you have image data in a machine learning context (like the MNIST digit dataset which is 28x28 grayscale images of digits), you could flatten each image into a 784-length vector and have each as a row in a DataFrame (with an accompanying label column). Pandas can then be used to explore correlations between pixel intensities, or to quickly compute statistics per pixel column across the dataset. This is more for analysis than for efficient storage, but it showcases integration.  

**Integration with NumPy/PIL/OpenCV:**  
Typically, you use pandas to *organize* the results of image processing, rather than perform the processing. For instance:  

```python
from PIL import Image
import os

data = []
for fname in os.listdir("images/"):
    img = Image.open(os.path.join("images/", fname)).convert("L")  # open as grayscale
    arr = np.array(img)
    mean_val = arr.mean()
    std_val = arr.std()
    data.append({"filename": fname, "mean_intensity": mean_val, "std_intensity": std_val,
                 "width": arr.shape[1], "height": arr.shape[0]})
df_images = pd.DataFrame(data)
print(df_images.describe())
```  

This would give summary stats for the image dataset (like average of means, etc.). With `df_images`, we could then do things like `df_images[df_images['mean_intensity'] < 50]` to find very dark images, or plot a histogram of `std_intensity` to see distribution of contrast.  

In summary, while pandas isn't an image processing library, it complements them by handling the **data management** part: storing results, merging image data with other data (e.g., if each image has a label or is associated with a certain time or subject, pandas can merge pixel data with that metadata), and performing analysis on numerical representations of images. By treating image arrays as data, you can apply all the familiar pandas techniques (grouping, joining, filtering, stats) to glean insights from image collections.  

## Real-World Case Studies  

Let's look at several practical scenarios where pandas shines, incorporating many of the advanced techniques we've discussed. Each case study focuses on a different domain and problem, demonstrating how pandas can be applied in practice.  

### Case Study 1: Financial Time Series Analysis  
**Scenario:** You are analyzing stock market data for a couple of companies. You have daily closing prices and want to derive insights such as returns, moving averages, and correlations.  

**Data Setup:** Suppose we have daily closing prices for two stocks (Stock A and Stock B) for a 3-month period. In a real case, you’d load this from a file or API; here we simulate it:  

```python
dates = pd.date_range("2021-01-01", periods=60, freq="D")
# Simulate stock prices with a random walk
np.random.seed(0)
priceA = 100 + np.cumsum(np.random.normal(0, 1, len(dates)))  # start ~100
priceB = 50 + np.cumsum(np.random.normal(0, 2, len(dates)))   # start ~50, more volatile
df_stocks = pd.DataFrame({"StockA": priceA, "StockB": priceB}, index=dates)
print(df_stocks.head())
```  

This yields a DataFrame with Date index and two columns for prices:  

```text
            StockA     StockB  
2021-01-01 101.7641   48.6551  
2021-01-02 102.1642   47.9360  
2021-01-03 103.1429   46.3097  
2021-01-04 105.3838   42.5744  
2021-01-05 106.2513   42.9723  
...            ...       ...  
```  

**Analyzing returns:** We can compute daily returns (percentage change) for each stock:  

```python
returns = df_stocks.pct_change()  # fraction change from previous day
print(returns.head())
```  

Example output:  

```text
              StockA    StockB  
2021-01-01       NaN       NaN    # no previous day for Jan 1  
2021-01-02  0.003932 -0.014780  
2021-01-03  0.009580 -0.033926  
2021-01-04  0.021726 -0.074554  
2021-01-05  0.008226  0.009346  
```  

We see StockA had small positive returns, StockB had some negative. We could analyze distribution of returns, risk etc., but let's continue with moving averages.  

**Moving average (rolling mean):** A common technical indicator is a moving average. Let's calculate a 7-day moving average of the stock prices to smooth short-term fluctuations:  

```python
df_stocks["StockA_MA7"] = df_stocks["StockA"].rolling(7).mean()
df_stocks["StockB_MA7"] = df_stocks["StockB"].rolling(7).mean()
```  

Now each stock has an MA7 column. For the first 6 days, it will be NaN (since a full 7 days aren't available), and from day 7 onwards it will show the average of the last 7 days' prices. You could use this to identify trends or crossovers (e.g., if price crosses above its MA, etc.).  

**Resampling to Monthly:** Perhaps daily data is too granular for your report; you want monthly summaries. Using resample:  

```python
monthly_summary = df_stocks.resample("M").agg({"StockA": ["mean", "std"], "StockB": ["mean", "std"]})
print(monthly_summary)
```  

This will produce a DataFrame indexed by month (last day of each month, since "M" is month-end frequency) and multi-level columns showing the mean and standard deviation of each stock's price during that month. For example:  

```text
            StockA             StockB           
              mean       std      mean       std  
2021-01-31  109.0       3.2      39.91     3.45  
2021-02-28  108.9       1.5      56.35    10.20  
2021-03-31  104.6       2.8      72.63     8.05  
```  

(These numbers are illustrative; actual values depend on the random simulation.) We see in January, StockA mean price ~109 with std ~3.2 (so relatively stable), whereas StockB mean ~39.9 with std ~3.45 (StockB dropped a lot in Jan, given it started near 50 and mean is 39.9). In Feb, StockB jumped up to mean 56 with high volatility (std 10.2). Such analysis shows trends and volatility at a monthly aggregate level.  

**Correlation analysis:** We might want to see if the stocks move together. We can quickly compute the correlation of returns:  

```python
corr = returns.dropna().corr()  # correlation matrix of returns
print(corr)
```  

This would output a 2x2 matrix (since we have two stocks):  

```text
        StockA  StockB  
StockA  1.000   0.1  
StockB  0.100   1.0  
```  

If the off-diagonals are near 0.1 as above (just an example), it means low correlation between StockA and StockB daily returns. If it was, say, 0.8, that indicates they move similarly.  

**Using MultiIndex for hierarchical data:** If instead of two specific stocks, you had many or maybe stocks grouped by sector, you could incorporate MultiIndex. For example, columns could be a MultiIndex with sector and stock, or the index could be MultiIndex with (Date, Stock). Then you could group by stock or sector easily. For demonstration: if we had our data in long format:  

```python
df_long = df_stocks.reset_index().melt(id_vars="index", var_name="Stock", value_name="Price")
df_long = df_long.rename(columns={"index": "Date"})
print(df_long.head())
```  

This would produce rows of Date, Stock, Price. With that, one could do: `df_long.groupby("Stock").resample("M", on="Date").mean()` to get monthly mean per stock (using the on= parameter to groupby a datetime). But that gets advanced; often it’s easier to keep things in separate columns or separate DataFrames per stock.  

**Key takeaways in this case study:**  
- Pandas makes it easy to handle time-indexed financial data: computing returns (`pct_change`), rolling statistics, and resampling to different frequencies.  
- We used a mix of vectorized operations and group-like operations (resample acts like groupby by time period).  
- MultiIndex or melt/pivot could be used to restructure data if needed for grouping by categories (like multiple stocks).  
- This kind of analysis can be extended: e.g., calculating beta of a stock against an index, using join to merge stock prices with some economic indicators by date, etc., all facilitated by pandas align-on-index magic.  

### Case Study 2: Text Data Analysis (Sentiment and Word Frequency)  
**Scenario:** You are analyzing customer feedback from surveys or social media. You have textual comments and want to quantify sentiment and extract common themes (keywords).  

**Data Setup:** Imagine we have a dataset of, say, tweets or reviews with a timestamp, user, and content. For simplicity, let's use a small example of user reviews we already partially saw:  

```python
data = [
    {"user": "u1", "product": "Phone", "review": "I love the new camera quality! Great phone.", "rating": 5},
    {"user": "u2", "product": "Phone", "review": "Good phone but battery life is short.", "rating": 3},
    {"user": "u3", "product": "Laptop", "review": "Excellent performance, I am very satisfied.", "rating": 5},
    {"user": "u4", "product": "Laptop", "review": "Not bad, but I expected more. Battery is okay.", "rating": 3},
    {"user": "u5", "product": "Phone", "review": "Terrible customer service. The phone stopped working in a week.", "rating": 1}
]
df_feedback = pd.DataFrame(data)
```  

Our DataFrame `df_feedback` has columns: user, product, review, rating. For example:  

```text
  user  product                                             review                                  rating  
0  u1    Phone       I love the new camera quality! Great phone.         5  
1  u2    Phone       Good phone but battery life is short.               3  
2  u3   Laptop       Excellent performance, I am very satisfied.         5  
3  u4   Laptop       Not bad, but I expected more. Battery is okay.      3  
4  u5    Phone       Terrible customer service. The phone stopped ...    1  
```  

**Sentiment analysis:** We can attempt a simple sentiment scoring similar to before. For demonstration, use a small lexicon of positive/negative words:  

```python
pos_words = {"love", "great", "good", "excellent", "satisfied"}
neg_words = {"terrible", "bad", "short"}  # etc.
def simple_sentiment(text):
    text = text.lower()
    score = 0
    for word in text.split():
        word = word.strip('.,!?')  # remove punctuation
        if word in pos_words:
            score += 1
        if word in neg_words:
            score -= 1
    return score

df_feedback["sentiment_score"] = df_feedback["review"].apply(simple_sentiment)
print(df_feedback[["review", "sentiment_score"]])
```  

This will give each review a crude sentiment score:  

```text
review                                                          sentiment_score  
"I love the new camera quality! Great phone."                   2  (love +1, great +1)  
"Good phone but battery life is short."                         0  (good +1, short -1 cancel out)  
"Excellent performance, I am very satisfied."                   2  (excellent +1, satisfied +1)  
"Not bad, but I expected more. Battery is okay."                -1 (bad -1)  
"Terrible customer service. The phone stopped working in a week." -1 (terrible -1)  
```  

Now that we have a sentiment score, we can use pandas to analyze by product or rating:  

- **Correlation with rating:** We can see if our sentiment score correlates with the given star rating. For example: `df_feedback[["rating", "sentiment_score"]].corr()`. (With the small sample, correlation might not be meaningful, but in a larger set we might expect a positive correlation).  

- **Average sentiment by product:**  

```python
print(df_feedback.groupby("product")["sentiment_score"].mean())
```  

Output might be:  

```text
product  
Laptop    0.5  
Phone     0.33...  
```  

Not very meaningful with so few data points, but in general one might find one product has more positive feedback than another.  

**Word frequency (TF analysis):** We might want to find the most common words in reviews for each product to see what topics are talked about. Pandas can help by splitting and counting. Using `.explode()` is convenient:  

```python
# Create a Series of all words in each product's reviews
df_feedback["review_words"] = df_feedback["review"].str.lower().str.replace('[^\w\s]', '', regex=True).str.split()
# Explode to get one word per row, keeping product info
words_exploded = df_feedback.explode("review_words")
# Now group by product and word to count
word_counts = words_exploded.groupby(["product", "review_words"]).size().reset_index(name="count")
# For each product, get top 3 words (excluding very common insignificant words like 'the', 'is', etc.)
common_words = word_counts[~word_counts["review_words"].isin(["the", "is", "am", "I", "a", "an", "but"])]
print(common_words.sort_values(["product", "count"], ascending=[True, False]).groupby("product").head(3))
```  

This is a bit complex, so let's break it down:  
- We split each review into words (after lowercasing and removing punctuation).  
- We explode the list of words so each word is its own row, alongside the product and other original columns.  
- Group by product and word and count occurrences.  
- Filter out some very common words ("stop words") which are not meaningful (in practice, you'd use a proper list of stopwords).  
- Then sort by count and take the top 3 for each product.  

For our data, this might yield something like:  

```text
 product  review_words    count  
 Laptop   battery         1  
 Laptop   excellent       1  
 Laptop   expected        1  
 Phone    phone           3  
 Phone    battery         1  
 Phone    love            1  
```  

Meaning: for *Laptop* reviews, words like "battery", "excellent", "expected" appear (each once in our small set); for *Phone* reviews, "phone" appears 3 times, "battery" once, "love" once, etc. This is of course trivial given the dataset, but in a real scenario with hundreds of comments, this would highlight themes (e.g., many people mention "battery" for phones, indicating it's a point of discussion).  

We could refine by merging with sentiment or ratings. For example, find common words in negative reviews (rating <= 2) vs positive reviews (rating >= 4) by filtering `df_feedback` before the explode step.  

**Time-based analysis:** If our data had timestamps (like tweets over time), we could use resample or rolling counts to see how sentiment or mention of certain keywords changes over time. For example, `df_feedback.set_index("timestamp").resample("W")["sentiment_score"].mean()` would give weekly average sentiment. This is powerful for social media monitoring or survey trends.  

**Key takeaways:**  
- Pandas can handle text data for analysis by leveraging string methods and the ability to explode and count.  
- We integrated a basic NLP step (sentiment via lexicon, word frequency) within pandas. For more advanced NLP, one would use dedicated libraries to get sentiment or topic modeling, then bring those results back into pandas for aggregation and analysis.  
- Grouping by categories (product) and summarizing text-derived metrics allows combining qualitative data (text) into quantitative insights.  

### Case Study 3: Image Data Processing and Analysis  
**Scenario:** You have a collection of images (say from a satellite or a medical imaging study, or just a set of photos) and you want to analyze some aspects of these images through their numerical data. This might include comparing brightness, size, or even pixel-by-pixel analysis.  

**Data Setup:** Let's say we have several images. We can't display actual images here, but we can simulate or describe them. For demonstration, we’ll simulate images as numpy arrays.  

Imagine we have 3 grayscale images of different sizes:  
- img1: 10x10 with random values,  
- img2: 8x8 with random values,  
- img3: 12x12 with random values.  

We want to analyze the distribution of pixel intensities and maybe compare their histograms or find the brightest image.  

```python
np.random.seed(42)
images = {
    "img1": np.random.randint(0, 256, size=(10, 10)),  # values 0-255
    "img2": np.random.randint(0, 256, size=(8, 8)),
    "img3": np.random.randint(0, 256, size=(12, 12))
}
# Create a DataFrame of image metadata and some computed stats
stats = []
for name, arr in images.items():
    stats.append({
        "image": name,
        "height": arr.shape[0],
        "width": arr.shape[1],
        "mean_intensity": arr.mean(),
        "max_intensity": arr.max(),
        "min_intensity": arr.min()
    })
df_images = pd.DataFrame(stats)
print(df_images)
```  

This DataFrame `df_images` might look like:  

```text
 image  height  width  mean_intensity  max_intensity  min_intensity  
 img1      10     10        129.37            255               0  
 img2       8      8        128.45            255               3  
 img3      12     12        128.01            254               0  
```  

So, all have similar mean intensities (since random uniform 0-255 ~ 127.5 expected), and we see min and max (some have 255 or 254, min 0 or small).  

**Finding the brightest/darkest images:** We can easily see which image has highest mean or max:  

```python
brightest = df_images.loc[df_images["mean_intensity"].idxmax(), "image"]
print("Brightest on average:", brightest)
```

We could also sort: `df_images.sort_values("mean_intensity", ascending=False)`. With random data they’re close, but if one image was systematically brighter, it would appear on top.  

**Pixel-level analysis:** Suppose we want to examine a specific image’s pixel distribution. We can convert one image to a DataFrame to use pandas operations. For example, take `img1`:  

```python
df_img1 = pd.DataFrame(images["img1"])
# Calculate how many pixels are above a threshold, say > 200
num_pixels_high = (df_img1 > 200).sum().sum()
total_pixels = df_img1.size
print(f"Fraction of very bright pixels in img1: {num_pixels_high/total_pixels:.2%}")
```  

This will compute a boolean DataFrame for `df_img1 > 200`, sum across columns (getting count per column) then sum that (total count of True values). This gives the count of pixels > 200, which we then divide by total pixels (10x10=100) for a percentage. The result might be, say, "Fraction of very bright pixels in img1: 5%".  

We could do this for all images by looping or using a group approach if we had them all in one DataFrame long form. We can actually combine all images' pixel data into one long DataFrame with a multi-index:  

```python
# Combine all pixel data into one DataFrame
pixels_list = []
for name, arr in images.items():
    df = pd.DataFrame(arr)
    pixels_long = df.stack().reset_index(name="value")
    pixels_long["image"] = name
    pixels_list.append(pixels_long)
pixels_df = pd.concat(pixels_list, ignore_index=True)
# Now pixels_df has columns: level_0 (row index), level_1 (col index), value, image
# Let's group by image and calculate fraction of bright pixels > 200 for each:
bright_fraction = pixels_df.groupby("image")["value"].apply(lambda vals: (vals > 200).mean())
print(bright_fraction)
```  

This will output something like:  

```text
image  
img1    0.07  
img2    0.05  
img3    0.09  
Name: value, dtype: float64  
```  

Meaning e.g. 7% of img1's pixels > 200, 5% for img2, 9% for img3. (Again random data, but in real images perhaps one image might have more bright areas).  

**Image processing integration:** If we wanted to do more complex processing like edge detection or color histograms, we’d likely use OpenCV or PIL to compute those, then use pandas to store/analyze the results. For instance, using OpenCV to compute a histogram of pixel intensities for each image and storing that histogram (maybe as a list or array) in a DataFrame for comparison. Or computing the average pixel intensity per row (which could indicate if the image has a gradient) and storing those as series. Pandas could then help find patterns (like grouping images by some category and averaging their histograms, etc.).  

**Key takeaways:**  
- Pandas is not manipulating the image itself (we rely on NumPy for that), but once the image is represented as numbers, we can use pandas to organize and analyze those numbers.  
- We demonstrated combining pixel data across images (melt/concat style) to do group comparisons – a powerful approach when dealing with many similar data items (images here).  
- By storing image stats in DataFrames, we can easily filter and sort images by characteristics (like find all images above certain brightness, or largest images by size).  
- This approach extends to other array-like data: for example, audio signals (1D arrays) or any sensor data grids can be treated similarly – use specialized methods to get numbers, then pandas to analyze those numbers.  

### Case Study 4: Marketing and Customer Analytics (Segmentation & Behavior)  
**Scenario:** You work for a retail company and have customer data and transaction data. You want to perform customer segmentation analysis and understand purchasing behavior.  

**Data Setup:** Let's consider two tables: customers and transactions (similar to earlier, but a bit more detailed for analysis).  

```python
customers = pd.DataFrame({
    "CustomerID": [1, 2, 3, 4, 5],
    "Name": ["Alice", "Bob", "Charlie", "Diana", "Ethan"],
    "Segment": ["Premium", "Standard", "Standard", "Premium", "Standard"],
    "Region": ["North", "North", "South", "South", "North"]
})
transactions = pd.DataFrame({
    "TransID": [1001, 1002, 1003, 1004, 1005, 1006],
    "CustomerID": [1, 2, 2, 4, 5, 5],
    "Amount": [200, 50, 75, 300, 20, 30],
    "Date": pd.to_datetime([
        "2025-01-05", "2025-01-06", "2025-02-10", "2025-02-15", "2025-03-01", "2025-03-15"
    ])
})
```  

In this data:  
- We have 5 customers. Two are Premium (Alice, Diana) and three Standard. They belong to regions North/South.  
- We have 6 transactions: Alice (ID1) had one transaction $200 on Jan 5; Bob (ID2) had two transactions ($50 on Jan 6, $75 on Feb 10); Diana (ID4) had one $300 on Feb 15; Ethan (ID5) had two ($20 on Mar 1, $30 on Mar 15); Charlie (ID3) apparently had no transaction in this period.  

**Segmentation Analysis:**  
First, let's merge customers and transactions to combine customer attributes with their purchase history:  

```python
cust_tx = pd.merge(customers, transactions, on="CustomerID", how="left")
print(cust_tx[["CustomerID","Name","Segment","Amount","Date"]])
```  

The merged table will look like:  

```text
CustomerID Name     Segment   Amount       Date  
1          Alice    Premium   200.0 2025-01-05  
2          Bob      Standard   50.0 2025-01-06  
2          Bob      Standard   75.0 2025-02-10  
3          Charlie  Standard    NaN        NaT  
4          Diana    Premium   300.0 2025-02-15  
5          Ethan    Standard   20.0 2025-03-01  
5          Ethan    Standard   30.0 2025-03-15  
```  

Charlie (ID3) has NaN for Amount/Date because he had no transactions (from left join).  

**Total and average spend per customer:** We can group by CustomerID to get total spent and number of transactions:  

```python
cust_summary = cust_tx.groupby(["CustomerID","Name","Segment"])["Amount"].agg(["count","sum","mean"]).fillna(0)
print(cust_summary)
```  

This yields a multi-index (CustomerID, Name, Segment) summary:  

```text
                    count    sum    mean  
CustomerID Name     Segment                        
1         Alice    Premium   1    200.0   200.0  
2         Bob      Standard  2    125.0    62.5  
3         Charlie  Standard  0      0.0     NaN  
4         Diana    Premium   1    300.0   300.0  
5         Ethan    Standard  2     50.0    25.0  
```  

We might replace 0 transactions mean NaN with 0 as well if we prefer, or interpret it separately. Charlie shows 0 count, 0 sum, NaN mean (no purchases).  

From this summary: Premium customers (Alice, Diana) had higher spend (200, 300) vs Standard (Bob 125, Ethan 50, Charlie 0). The count indicates Bob and Ethan each had 2 transactions, others 1 or 0.  

**Group by Segment:** Now, at a segment level, let's see average metrics:  

```python
segment_summary = cust_summary.groupby("Segment").agg({
    "count": "mean",  # avg transactions count per customer in segment
    "sum": "mean",    # avg total spend per customer in segment
    "mean": "mean"    # avg order value per customer in segment (averaged again)
})
print(segment_summary)
```  

Output might be:  

```text
          count    sum     mean  
Segment                          
Premium    1.0   250.0   250.0  
Standard   1.33  58.33   87.5  
```  

Interpreting this: On average, a Premium customer made 1 transaction in this period, spending $250 total (thus average order value $250). A Standard customer made ~1.33 transactions on average (since Bob and Ethan had more than one, Charlie had 0), spending ~$58 total on average. The average order value for Standard (the mean of means) shows $87.5, which is a bit skewed because Bob had ~62.5 mean order, Ethan 25, Charlie N/A, averaging out to 87.5? That seems off – likely because Charlie's NaN mean may have been treated differently. We might have wanted to treat Charlie's mean as 0 in that context, which `.fillna(0)` could be applied before grouping by segment. To avoid confusion, perhaps sum/total is clearer: Premium spend per customer is much higher.  

Alternatively, we could compute directly with the original data:  

```python
print(cust_tx.groupby("Segment")["Amount"].sum())
```  

Would give total revenue by segment: Premium 500, Standard 175. Or average per segment dividing by number of customers in segment (500/2 = 250, 175/3 ≈ 58.3) which matches the "sum" column above.  

**Customer retention/behavior:** We could also look at time between purchases for those with multiple transactions. For example, Bob’s purchases were Jan 6 and Feb 10 (~35 days apart), Ethan’s Mar 1 and Mar 15 (14 days apart). Pandas can calculate these by grouping by customer and sorting by date:  

```python
cust_tx = cust_tx.sort_values(["CustomerID", "Date"])
cust_tx["days_since_prev"] = cust_tx.groupby("CustomerID")["Date"].diff().dt.days
print(cust_tx[["CustomerID","Name","Date","days_since_prev"]])
```  

We’d see NaN for the first purchase per customer, and a number for subsequent ones: Bob would have 35 days, Ethan 14 days, others just NaN (single or no purchase). This helps identify purchasing frequency.  

**Pivot for region vs segment analysis:** Maybe we want to see how many customers in each segment & region, or total sales by segment and region. We can pivot or use groupby:  

```python
# Customers by Segment and Region
cust_count = customers.pivot_table(index="Region", columns="Segment", values="CustomerID", aggfunc="count")
print(cust_count)
# Sales (sum of Amount) by Segment and Region
sales_by_seg_region = cust_tx.pivot_table(index="Region", columns="Segment", values="Amount", aggfunc="sum", fill_value=0)
print(sales_by_seg_region)
```  

The first pivot counts CustomerID occurrences (essentially number of customers) per region-segment:  

```text
Segment  Premium  Standard  
Region                    
North       1         2  
South       1         1  
```  

Meaning in North region, 1 Premium (Alice) and 2 Standard (Bob, Ethan); in South, 1 Premium (Diana) and 1 Standard (Charlie).  

The second pivot sums Amount by region and segment:  

```text
Segment  Premium  Standard  
Region                     
North      200       175  
South      300         0  
```  

Interpretation: North Premium (Alice) spent 200, North Standard (Bob & Ethan) combined 175, South Premium (Diana) 300, South Standard (Charlie) 0 (no purchases). This could be visualized or used to decide where revenue is coming from (North vs South, Premium vs Standard).  

**Using Categoricals:** In a larger dataset, we might convert Segment and Region to categorical dtype since they repeat often and are few unique values, which would optimize memory (as discussed earlier).  

**Behavior tracking:** If we had data like website visits or other events, we could similarly merge or group by customer and sort by time to compute metrics like conversion rates (how many visits before a purchase, etc.). Pandas is often used in marketing analytics to join datasets (ad impressions, clicks, purchases) and then compute funnels or customer journeys.  

In our simple example, we could define an "active customer" as someone who made at least one purchase in the period, and filter accordingly. Or define recency (days since last purchase) using max Date per customer and today's date (assuming today for example is 2025-04-01, Charlie’s recency would be infinite since no purchase, others have some finite recency).  

For instance:  

```python
today = pd.Timestamp("2025-04-01")
last_purchase = cust_tx.groupby("CustomerID")["Date"].max()
cust_status = pd.DataFrame({
    "LastPurchase": last_purchase,
    "Recency_days": (today - last_purchase).dt.days
})
print(cust_status)
```  

This would list each customer’s last purchase date and days since then. Charlie will have NaT for LastPurchase (no purchase), which we might fill with a large number or handle separately.  

**Key takeaways:**  
- Pandas enables merging multiple data sources (customers + transactions) and then performing various analyses: grouping by segment, region, customer to derive KPIs like total spend, average order, frequency.  
- We showcased pivot_table for cross-tab analysis (Segment vs Region).  
- We illustrated computing new metrics (days between purchases) using groupby and diff on times.  
- These techniques form the basis of common analytics tasks: RFM (Recency, Frequency, Monetary) analysis for customers, segmentation by value/behavior, etc. Pandas is often the go-to tool to implement these calculations before feeding into reports or machine learning models for customer segmentation.  

---

With these case studies, we've seen pandas applied in finance, text analysis, image data, and marketing analytics. Each domain has its own data types and challenges, but pandas provides a consistent interface to manipulate and analyze the data once it's in a DataFrame. By mastering the advanced topics covered (multi-indexing, time series, groupby, reshaping, merging, handling missing/outliers, performance tuning, and integration with specialized data like text and images), you can tackle a wide variety of data analysis tasks efficiently. Use these techniques as building blocks to clean, transform, and glean insights from your datasets in any field.



# Advanced Pandas Quiz

Below are 29 intermediate to advanced quiz-style questions focused on various advanced Pandas topics, along with their answers and brief explanations. The questions include multiple choice, code-output predictions, and conceptual true/false formats. Use them to test and deepen your understanding of Pandas.

## Question 1  
**Which method is used to transform a DataFrame from a wide format to a long format in pandas?**

- **A)** `pivot_table`  
- **B)** `melt`  
- **C)** `pivot`  
- **D)** `unstack`  

**Answer:** B) `melt`.  
**Explanation:** The `pd.melt` function is designed to convert data from a wide format to a long format (one row per combination of original index and column) by unpivoting selected columns. In contrast, `pivot` (or `pivot_table`) does the opposite (long to wide), and `unstack` pivots a level of a MultiIndex from rows to columns (also wide format).

## Question 2  
**By default, what type of join does `pd.merge` perform if the `how` parameter is not specified?**

- **A)** Inner join  
- **B)** Outer join  
- **C)** Left join  
- **D)** Right join  

**Answer:** A) Inner join.  
**Explanation:** The default `how` in `pd.merge` is `"inner"`, which means it returns only the intersection of keys from both DataFrames. (Outer joins, left joins, and right joins require explicitly setting `how='outer'`, `how='left'`, or `how='right'`.)

## Question 3  
**True or False:** Using `df.set_index(['col1', 'col2'])` on a DataFrame will create a hierarchical (MultiIndex) index using `col1` and `col2`.  

**Answer:** True.  
**Explanation:** Setting multiple columns as the index will produce a MultiIndex (hierarchical index) on the rows. For example, `df.set_index(['col1','col2'])` results in an index with two levels (first level from `col1`, second level from `col2`).

## Question 4  
**True or False:** `pivot_table` can automatically aggregate data (using a mean by default) when there are duplicate entries for the pivot index/columns, whereas `pivot` (the DataFrame method) will raise an error if it encounters duplicate index/column combinations.  

**Answer:** True.  
**Explanation:** The `DataFrame.pivot` method does not allow duplicate index/column pairs and will error out if duplicates are present. In contrast, `pd.pivot_table` can handle duplicates by aggregating them using an aggregation function (default is mean if not specified), making it more flexible for summary tables.

## Question 5  
**True or False:** The DataFrame method `join` (when called like `df1.join(df2)`) by default joins on the index of the two DataFrames.  

**Answer:** True.  
**Explanation:** `df1.join(df2)` by default will align `df2` on `df1` by their index (performing a left join using the index). If you want to join on columns instead, you would need to use `pd.merge` or specify the `on` parameter in `join`. By default, `join` uses the index of both DataFrames.

## Question 6  
**True or False:** Calling `df.resample('M')` on a DataFrame with a DatetimeIndex will group the data by month, requiring an aggregation like `.mean()` or `.sum()` to combine the values in each monthly group.  

**Answer:** True.  
**Explanation:** The `resample('M')` operation bins the time-indexed data into monthly buckets (if `'M'` represents month-end frequency). The resample method returns a resampler object, and you need to follow it with an aggregation (like `.sum()`, `.mean()`, etc.) or another fill/ffill method to produce a result. Resampling is essentially grouping by time intervals.

## Question 7  
**Which of the following statements is NOT true about pandas window functions?**

- **A)** A rolling window with `window=3` computes metrics over 3 consecutive values at a time (sliding window).  
- **B)** An expanding window includes all values from the start of the sequence up to the current index for each calculation.  
- **C)** Both rolling and expanding return a special window object on which you must call an aggregation (like `.mean()` or `.sum()`) to get results.  
- **D)** An expanding window requires specifying a window size just like a rolling window does.  

**Answer:** D) An expanding window requires specifying a window size just like a rolling window does.  
**Explanation:** Option D is **not true**. An expanding window does **not** need a window size; it always uses all data up to the current point. Options A, B, and C are true: Rolling windows use a fixed-size sliding window, expanding windows grow from the start, and both rolling/expanding return a window object that needs an aggregation function call to produce numeric results.

## Question 8  
**For a DataFrame column with many repeated string values, which pandas data type would typically use the least memory for that column?**

- **A)** `object` (regular string dtype)  
- **B)** `category`  
- **C)** `int64`  
- **D)** `float64`  

**Answer:** B) `category`.  
**Explanation:** Converting a column of repeated strings to the `category` dtype can significantly reduce memory usage. A categorical dtype stores a lookup table of unique values and represents the column as integer codes, which is much more memory-efficient than an `object` array of repeated strings. (`int64` or `float64` wouldn't be applicable unless the data is numeric; they are for numeric data and wouldn't directly represent strings.)

## Question 9  
**Which of the following is NOT an advantage of method chaining in pandas?**

- **A)** It can make code more concise and readable by combining multiple operations in one expression.  
- **B)** It helps avoid creating unnecessary intermediate variables.  
- **C)** It always significantly improves the performance (speed) of the code.  
- **D)** It can reduce the likelihood of a `SettingWithCopyWarning` by avoiding chained indexing and using methods like `.assign`.  

**Answer:** C) It always significantly improves the performance of the code.  
**Explanation:** Method chaining is primarily about code readability and avoiding intermediate variables (options A and B are true benefits). It can also help avoid some `SettingWithCopyWarning` issues by using built-in methods (`.loc`, `.assign`, etc.) instead of chained indexing (option D is a potential benefit). However, method chaining does **not guarantee** faster code execution; performance is usually similar to writing the steps out separately (thus C is not a guaranteed benefit and is the false statement).

## Question 10  
**True or False:** If you group a DataFrame by multiple keys, for example using `df.groupby(['A', 'B']).sum()`, the resulting DataFrame will have a MultiIndex on its rows with levels corresponding to `A` and `B`.  

**Answer:** True.  
**Explanation:** By default, `df.groupby(['A','B']).sum()` will produce a result indexed by the group keys (`A` and `B`), which means the index of the result is a MultiIndex with those levels. (If you prefer the group keys as columns instead, you could use `as_index=False` in the groupby, but by default it uses a MultiIndex.)

## Question 11  
**Given a DataFrame `df` with a two-level MultiIndex on its rows (levels named `'Level1'` and `'Level2'`), which code snippet using `.loc` will correctly select the entry where `Level1 = 'Foo'` and `Level2 = 'Bar'` (returning all columns for that row)?**

- **A)** `df.loc['Foo', 'Bar']`  
- **B)** `df.loc[('Foo', 'Bar'), :]`  
- **C)** `df.loc[:, ('Foo', 'Bar')]`  
- **D)** `df.loc[ 'Level1' == 'Foo' & 'Level2' == 'Bar']`  

**Answer:** B) `df.loc[('Foo', 'Bar'), :]`.  
**Explanation:** To use `.loc` with a MultiIndex, you should pass a tuple of index values. `df.loc[('Foo','Bar'), :]` correctly specifies the index tuple `('Foo','Bar')` and `:` for "all columns." Option A is incorrect because `df.loc['Foo','Bar']` would be interpreted as trying to select row index `'Foo'` and column `'Bar'`. Option C is incorrect (that syntax would be for selecting columns if they had a MultiIndex). Option D is not valid pandas syntax. Alternatively, one could use `df.xs(('Foo','Bar'))` to achieve the same selection, but the question specifically asks for `.loc` usage.

## Question 12  
**True or False:** When merging two DataFrames on a key, if both DataFrames contain multiple rows with the same key (a many-to-many merge), the result can have more rows than either of the original DataFrames.  

**Answer:** True.  
**Explanation:** In a many-to-many merge (where the join key has duplicates in both tables), each combination of matching rows will appear in the result. This can lead to a Cartesian product of the duplicates, resulting in more rows than either input DataFrame. For example, if one DataFrame has 3 rows with key "X" and another has 2 rows with key "X", an inner join on that key will produce 3×2 = 6 rows for key "X" in the result.

## Question 13  
**What will be the output of the following code?** (Inspect the index and values carefully.)

```python
import pandas as pd
rng = pd.date_range('2021-01-01', periods=5, freq='D')
ts = pd.Series(range(5), index=rng)
result = ts.resample('2D').sum()
print(result)
```  

**Answer:** The code produces a Series resampled to 2-day frequency, summing values in 2-day bins. The output will have an index of Jan 1, Jan 3, Jan 5 (covering 2-day periods starting at those dates) with the sums:

```
2021-01-01    1
2021-01-03    5
2021-01-05    4
Freq: 2D, dtype: int64
```  

**Explanation:** The original `ts` had daily values 0,1,2,3,4 on Jan 1–Jan 5. Resampling with `'2D'` groups the data into 2-day chunks: Jan 1-2, Jan 3-4, Jan 5 (the last period has just one day). The sums are 0+1 = 1 for the Jan 1–2 period, 2+3 = 5 for Jan 3–4, and 4 for Jan 5 (since it's alone in its period). The result is indexed by the start of each 2-day period.

## Question 14  
**Which of the following uses `groupby().agg()` correctly to apply different aggregations to different columns?** For example, suppose you want the sum of `'col1'` and the mean of `'col2'` for each group in `df.groupby('key')`. Which option is valid?  

- **A)** `df.groupby('key').agg({'col1': 'sum', 'col2': 'mean'})`  
- **B)** `df.groupby('key').agg('col1': 'sum', 'col2': 'mean')`  
- **C)** `df.groupby('key')[['col1','col2']].agg(['sum','mean'])`  
- **D)** `df.groupby('key').agg(col1='sum', col2='mean')`  

**Answer:** A) `df.groupby('key').agg({'col1': 'sum', 'col2': 'mean'})`.  
**Explanation:** Option A correctly passes a dictionary to `.agg()`, mapping each column to a different aggregation function. Option B is not valid Python syntax (missing braces). Option C would apply both `'sum'` and `'mean'` to *both* columns, resulting in a MultiIndex in columns (not the specified one-function-per-column behavior). Option D in this form is not a correct usage of `.agg` (pandas does not interpret `col1='sum'` as applying sum on col1 unless you use the newer named aggregation syntax, which requires a different tuple format). The dictionary mapping in Option A is the standard way to specify different aggregations for different columns.

## Question 15  
**What is the output of the following code?**

```python
import pandas as pd
s = pd.Series([10, 20, 30, 40, 50])
result = s.rolling(window=3, min_periods=2).mean()
print(result.tolist())
```  

**Answer:** The resulting list will be: `[nan, 15.0, 20.0, 30.0, 40.0]`.  
**Explanation:** A rolling window of 3 with `min_periods=2` means we need at least 2 values to compute a mean. The series `s` has values [10, 20, 30, 40, 50]:
- For the first element (index 0), there is only one value in the window ([10]), which is fewer than 2, so the result is NaN.  
- For the second element (index 1, window [10,20]), the mean is 15.0.  
- For the third element (index 2, window [10,20,30]), the mean is 20.0.  
- For the fourth element (index 3, window [20,30,40]), the mean is 30.0.  
- For the fifth element (index 4, window [30,40,50]), the mean is 40.0.  

So the printed list of results is `[nan, 15.0, 20.0, 30.0, 40.0]` (with the first value being `NaN` due to insufficient data in that initial window).

## Question 16  
**What will `result` look like after executing the code below?**

```python
import pandas as pd
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Alice', 'Bob'],
    'Year': [2020, 2020, 2021, 2021],
    'Score': [85, 90, 95, 88]
})
result = df.pivot(index='Name', columns='Year', values='Score')
print(result)
```  

**Answer:** The pivot will create a table with `Name` as the index and `Year` as the columns. The output will be a DataFrame with two rows (one for each Name) and two columns (2020 and 2021), with the scores filled in:

```
Year   2020  2021
Name             
Alice    85    95
Bob      90    88
```  

**Explanation:** Here, `'Name'` is used as the index and `'Year'` as columns, with `'Score'` as the values. The data had Alice and Bob each in 2020 and 2021 with given scores. After pivoting:
- Row index "Alice" has columns 2020 = 85 and 2021 = 95.
- Row index "Bob" has columns 2020 = 90 and 2021 = 88.  
This matches the original data arranged in a wide format. (Each Name-Year combination was unique, so pivot fills them directly without needing aggregation.)

## Question 17  
**True or False:** If you use multiple aggregation functions on a DataFrame groupby, for example `df.groupby('X').agg(['mean', 'sum'])`, the resulting DataFrame will have a MultiIndex on its columns (with one level for the original column names and another for the function names).  

**Answer:** True.  
**Explanation:** When you pass a list of functions to `agg`, pandas produces a hierarchical column index. Each original column will expand into sub-columns for each aggregation. For instance, grouping by `'X'` and aggregating `['mean','sum']` on other columns will result in column names like `('col1','mean')`, `('col1','sum')`, `('col2','mean')`, etc., which is represented as a MultiIndex in the columns. (You can flatten these later if needed, but by default it’s a MultiIndex on columns.)

## Question 18  
**If you want to remove all groups from a grouped DataFrame where the group's total sum is less than 100, which groupby operation is most suitable?**

- **A)** `groupby().filter(...)`  
- **B)** `groupby().apply(...)`  
- **C)** `groupby().transform(...)`  
- **D)** `groupby().agg(...)`  

**Answer:** A) `groupby().filter(...)`.  
**Explanation:** The `.filter` method on a GroupBy is specifically designed to **exclude** entire groups based on a condition. For example, `df.groupby('key').filter(lambda grp: grp['value'].sum() >= 100)` will keep only groups with sum at least 100. The `.apply` could also be used to manually filter, but it's more complex and not as direct. `.transform` and `.agg` are for transforming or aggregating data, not for removing entire groups based on a group-level condition.

## Question 19  
**What happens if you attempt to assign a value that is not already in the categories of a pandas Categorical column?**

- **A)** The new value is automatically added to the existing categories of the column.  
- **B)** Pandas raises an error or exception.  
- **C)** The new value is silently converted to `NaN`.  
- **D)** The column is converted from `category` back to `object` dtype to accommodate the new value.  

**Answer:** B) Pandas raises an error.  
**Explanation:** Categorical data types have a fixed set of allowed values (categories). If you try to assign a value not in this set, pandas will raise an error (specifically, a `TypeError` saying that you "Cannot setitem on a Categorical with a new category"). The categories are not automatically expanded (you would have to explicitly add the new category first using `.cat.add_categories`). It also will not convert to object dtype implicitly or turn the value into NaN — it just disallows the operation.

## Question 20  
**Which pandas method allows you to insert a custom function into a method chain by passing the DataFrame to that function?**

- **A)** `.apply()`  
- **B)** `.transform()`  
- **C)** `.pipe()`  
- **D)** `.eval()`  

**Answer:** C) `.pipe()`.  
**Explanation:** The `.pipe()` method is designed to make method chaining more flexible by allowing you to call an arbitrary function on the DataFrame (or Series) within the chain. It passes the object to the function you specify. For example, `df.pipe(func, arg1, arg2=val)` will call `func(df, arg1, arg2=val)` and return the result, allowing you to continue the chain. `.apply()` and `.transform()` are for groupby or DataFrame column-wise operations, and `.eval()` is for evaluating expressions; neither is meant for integrating arbitrary functions into a chain in the same way that `.pipe` is.

## Question 21  
**You want to create a new column `'X_norm'` in DataFrame `df` that is each value of column `'X'` minus the mean of `'X'` within that value's group (grouped by `'Category'`). Which approach would best accomplish this?**

- **A)** `df.groupby('Category')['X'].apply(lambda g: g - g.mean())`  
- **B)** `df.groupby('Category')['X'].transform(lambda x: x - x.mean())`  
- **C)** `df.groupby('Category').agg(lambda x: x - x.mean())`  
- **D)** `df['X'] - df.groupby('Category')['X'].mean()`  

**Answer:** B) Use `groupby().transform` with the mean, i.e. `df.groupby('Category')['X'].transform(lambda x: x - x.mean())`.  
**Explanation:** The `transform` function is ideal for this because it returns a result of the same size as the original data, enabling you to assign it as a new column. Option A using `.apply` would also compute the correct values, but it often returns a combined Series that might need re-alignment; `transform` handles this more directly and is built for element-wise group transformations. Option C with `.agg` won't work because `.agg` expects the function to return a single aggregated value per group, not a series. Option D as written will not align properly (the group mean Series would have the category as index, not the original DataFrame's index, leading to misalignment or NaNs). Therefore, option B is the cleanest and correct approach: it produces a Series of the same length where each entry is value minus group mean.

## Question 22  
**Which method call will swap the levels of a MultiIndex on a DataFrame?**

- **A)** `df.swaplevel(0, 1)`  
- **B)** `df.swap_levels(0, 1)`  
- **C)** `df.swapindex(0, 1)`  
- **D)** `df.swapaxes(0, 1)`  

**Answer:** A) `df.swaplevel(0, 1)`.  
**Explanation:** The correct method is `swaplevel` (note no "s" at the end). For a DataFrame with a MultiIndex (on rows or columns), `df.swaplevel(i, j, axis)` will swap two index levels. By default `axis=0` (row index). Option B is slightly misnamed (no method `swap_levels` exists). Option C is not a pandas method. Option D, `swapaxes`, swaps entire axes (rows vs columns), not MultiIndex levels.

## Question 23  
**True or False:** `df.groupby(level=1)` will group a DataFrame by the second level of its row MultiIndex (assuming the DataFrame has a MultiIndex on its rows).  

**Answer:** True.  
**Explanation:** The `level` parameter in `groupby` allows you to group by an index level. `level=1` refers to the second level (since levels are 0-indexed). So `df.groupby(level=1)` groups the rows of `df` according to the values in the second index level. This is a convenient way to group by an index level without having to reset the index or reference the level by name.

## Question 24  
**True or False:** `tz_localize` is used to set a timezone on a timezone-naive DatetimeIndex (making it timezone-aware without changing the actual times), whereas `tz_convert` is used to convert times in a timezone-aware DatetimeIndex to a different timezone.  

**Answer:** True.  
**Explanation:** `tz_localize` attaches timezone information to naive timestamps. For example, `df.index.tz_localize('UTC')` will mark the timestamps as UTC without altering the clock times. `tz_convert`, on the other hand, converts timestamps from one timezone to another (shifting the clock time accordingly). You must have a timezone-aware index to use `tz_convert`. In summary: localize sets the timezone (no time math, just labeling), convert changes the timezone (adjusting times).

## Question 25  
**When merging two DataFrames, which parameter of `pd.merge` can be used to add a column indicating the source of each row (whether it came from the left DataFrame only, the right DataFrame only, or from both)?**

- **A)** `how='outer'`  
- **B)** `indicator=True`  
- **C)** `suffixes=('left','right')`  
- **D)** `validate='one_to_one'`  

**Answer:** B) `indicator=True`.  
**Explanation:** The `indicator` parameter, when set to True, adds a column named `_merge` to the result of `pd.merge`. This column labels each row as `'left_only'`, `'right_only'`, or `'both'` depending on whether the merge key was found only in the left DataFrame, only in the right, or in both. The `how` parameter (`outer`, `inner`, etc.) controls the type of join but doesn’t add such a column. `suffixes` controls column name suffixes for overlapping names, and `validate` checks merge assumptions; neither create a source indicator column.

## Question 26  
**True or False:** Setting `margins=True` in `pd.pivot_table` will add an extra row and column to the pivot output that contain the totals (margins) of the values (often labeled "All").  

**Answer:** True.  
**Explanation:** The `margins=True` option in `pd.pivot_table` adds summary totals. It produces a row (and column, if applicable) that show aggregate values (using the specified aggfunc, default sum or mean) across all groups. These are typically labeled "All" by default. This is useful for seeing overall totals alongside the breakdown in the pivot table.

## Question 27  
**True or False:** Converting a DataFrame column from dtype `float64` to `float32` will typically reduce its memory usage roughly by half (at the cost of some precision).  

**Answer:** True.  
**Explanation:** A `float64` (64-bit) uses double the number of bits of a `float32` (32-bit) for each value. Thus, changing to `float32` will approximately halve the memory footprint for that column. The trade-off is that `float32` has lower precision and a smaller range of representable numbers. Similarly, using smaller integer types (int8, int16 vs int64) or using categorical types for repetitive strings can also reduce memory usage.

## Question 28  
**Which of the following pandas operations is least suitable to include in the middle of a method chain (because it does *not* return a DataFrame or Series)?**

- **A)** `.assign(...)`  
- **B)** `.pipe(...)`  
- **C)** `.query(...)`  
- **D)** `.plot(...)`  

**Answer:** D) `.plot(...)`.  
**Explanation:** `.plot()` returns a matplotlib object (e.g., an Axes or a figure), not a DataFrame/Series. Including it in the middle of a chain would break the chain because you can’t call DataFrame methods on the matplotlib object. In contrast, `.assign`, `.pipe`, and `.query` each return a DataFrame (or Series in some cases) and thus can be used in the middle of a method chain. Typically, you would end a chain with `.plot()` (if used at all in a chain), or more commonly, break the chain to plot separately.

## Question 29  
**True or False:** `df.groupby('Category')['Value'].rank()` will assign rank 1 to the smallest `Value` within each category, ranking the values only among others of the same `Category`.  

**Answer:** True.  
**Explanation:** When you call a method like `.rank()` on a grouped column (`df.groupby('Category')['Value'].rank()`), pandas will compute the rank within each group. This means each category's values are ranked independently of other categories. The smallest value in each `'Category'` group gets rank 1, the second smallest rank 2, and so on. (This is equivalent to using `df.groupby('Category')['Value'].transform('rank')` in this case. The ranking is done group-wise.)