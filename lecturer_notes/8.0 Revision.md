# Machine Learning Fundamentals: Comprehensive Revision Quiz
*Covering KNN, Decision Trees, Random Forests, Ensemble Methods, Regression, and Gradient Descent*

---

## Instructions

This quiz contains 30 multiple-choice questions designed to test your understanding of fundamental machine learning concepts. The questions progress from basic conceptual understanding to more nuanced applications. Take your time to think through each question before checking the answer.

**Topics Covered:**
- K-Nearest Neighbors (KNN)
- Decision Trees
- Random Forests
- Ensemble Methods
- Linear and Polynomial Regression
- Gradient Descent

**Scoring Guide:**
- 26-30 correct: Excellent understanding
- 21-25 correct: Good grasp with minor gaps
- 16-20 correct: Adequate but needs review
- Below 16: Significant review needed

---

## Questions

### Question 1
In K-Nearest Neighbors, what happens when we increase the value of k from 1 to a larger number?

A) The model becomes less flexible and the decision boundary becomes smoother  
B) The model becomes more flexible and prone to overfitting  
C) The computational cost decreases significantly  
D) The model always performs better on test data

**Answer: A**

**Explanation:** When k increases, the model considers more neighbors for each prediction, which creates smoother decision boundaries and reduces the model's sensitivity to noise in the training data. This makes the model less flexible (higher bias, lower variance). With k=1, the model can create very complex, jagged decision boundaries that might overfit to noise, while larger k values create more generalized, smoother boundaries.

---

### Question 2
Which of the following is NOT a characteristic of the KNN algorithm?

A) It learns explicit parameters during training  
B) It requires storing all training data  
C) It's a lazy learning algorithm  
D) It can be used for both classification and regression

**Answer: A**

**Explanation:** KNN is called a "lazy" or "instance-based" learning algorithm because it doesn't learn explicit parameters during training. Instead, it simply stores all the training data and makes predictions by finding the k nearest neighbors at prediction time. This is in contrast to "eager" learning algorithms like linear regression that learn parameters (weights/coefficients) during training and then discard the training data.

---

### Question 3
When using KNN for regression, how is the final prediction typically calculated?

A) By taking the majority vote of the k nearest neighbors  
B) By using the target value of the single nearest neighbor  
C) By taking the median of the k nearest neighbors' target values  
D) By taking the average of the k nearest neighbors' target values

**Answer: D**

**Explanation:** For KNN regression, the prediction is typically the average (mean) of the target values of the k nearest neighbors. This provides a smooth estimate that incorporates information from multiple similar instances. While median could theoretically be used and might be more robust to outliers, the mean is the standard approach. Majority voting is used for classification, not regression.

---

### Question 4
What is the primary purpose of feature scaling in KNN?

A) To reduce computational complexity  
B) To handle missing values in the dataset  
C) To reduce the dimensionality of the data  
D) To ensure all features contribute equally to distance calculations

**Answer: D**

**Explanation:** Feature scaling is crucial in KNN because the algorithm relies on distance calculations. If features have vastly different scales (e.g., age in years vs. income in dollars), the feature with larger values will dominate the distance calculation. Scaling ensures that all features contribute proportionally to the distance metric, preventing any single feature from overwhelming others due to scale differences.

---

### Question 5
In a decision tree, what does "information gain" measure?

A) The total amount of information in the dataset  
B) The depth of the tree after splitting  
C) The number of instances correctly classified  
D) The reduction in entropy achieved by splitting on a particular feature

**Answer: D**

**Explanation:** Information gain measures how much the entropy (uncertainty) decreases when we split the data using a particular feature. It's calculated as the difference between the entropy before the split and the weighted average entropy after the split. The feature that provides the highest information gain is chosen for splitting because it best separates the classes, reducing our uncertainty about the target variable.

---

### Question 6
Which statement about decision tree pruning is correct?

A) Pruning always improves training accuracy  
B) Pruning increases the depth of the tree  
C) Pruning is only applied during tree construction  
D) Pruning is done to reduce overfitting by removing branches

**Answer: D**

**Explanation:** Pruning is a technique used to reduce overfitting in decision trees by removing branches that don't significantly improve the model's performance on validation data. While pruning typically reduces training accuracy (since we're removing decision nodes), it often improves generalization to new data by creating a simpler, more generalizable model. Pruning can be done during construction (pre-pruning) or after (post-pruning).

---

### Question 7
What is the main advantage of Random Forests over individual decision trees?

A) Random Forests are always faster to train  
B) Random Forests require less memory  
C) Random Forests work better with small datasets  
D) Random Forests reduce overfitting through ensemble averaging

**Answer: D**

**Explanation:** Random Forests address the main weakness of decision trees (tendency to overfit) by combining multiple trees trained on different subsets of data and features. This ensemble approach reduces variance through averaging, leading to better generalization. While individual trees might overfit to their training data, the average of many diverse trees provides a more robust and stable prediction.

---

### Question 8
In Random Forests, what does the "random" refer to?

A) Random selection of the target variable  
B) Random initialization of tree parameters  
C) Random selection of the splitting criterion  
D) Random subsampling of both instances and features

**Answer: D**

**Explanation:** The "random" in Random Forests refers to two sources of randomness: 1) Bootstrap sampling (random selection of training instances with replacement for each tree), and 2) Random feature selection (at each split, only a random subset of features is considered). This double randomness ensures that the trees in the forest are diverse, which is crucial for the ensemble's effectiveness.

---

### Question 9
Which of the following is a characteristic of bagging (Bootstrap Aggregating)?

A) It works only with decision trees  
B) It requires the base models to be weak learners  
C) It combines models using weighted voting  
D) It trains multiple models on different subsets of training data

**Answer: D**

**Explanation:** Bagging involves training multiple models on different bootstrap samples of the training data and then averaging their predictions (for regression) or using majority voting (for classification). While Random Forests use bagging with decision trees, bagging can be applied to any base model. Unlike boosting, bagging doesn't require weak learners and uses simple (often unweighted) averaging rather than weighted combinations.

---

### Question 10
In boosting algorithms like AdaBoost, how are training examples weighted?

A) All examples are weighted equally throughout  
B) Correctly classified examples get higher weights  
C) Random weights are assigned to examples  
D) Misclassified examples get higher weights in subsequent iterations

**Answer: D**

**Explanation:** Boosting algorithms like AdaBoost adaptively adjust the weights of training examples. After each weak learner is trained, examples that were misclassified get higher weights, making them more important in training the next learner. This forces subsequent models to focus on the "hard" examples that previous models got wrong, gradually improving performance on difficult cases.

---

### Question 11
What is the key difference between bagging and boosting?

A) Bagging can only be used for classification, boosting for regression  
B) Bagging requires more computational resources than boosting  
C) Bagging works only with tree-based models  
D) Bagging trains models in parallel, boosting trains them sequentially

**Answer: D**

**Explanation:** The fundamental difference is in the training process: bagging trains multiple models independently (in parallel) on different bootstrap samples, while boosting trains models sequentially, with each new model learning from the mistakes of previous models. This sequential nature allows boosting to adaptively focus on difficult examples but makes it more sensitive to noise and outliers.

---

### Question 12
In linear regression, what does the coefficient of a feature represent?

A) The correlation between the feature and target  
B) The importance ranking of the feature  
C) The probability that the feature affects the target  
D) The change in the target variable for a one-unit change in the feature, holding other features constant

**Answer: D**

**Explanation:** In linear regression, each coefficient represents the expected change in the target variable when the corresponding feature increases by one unit, while keeping all other features constant. This is the marginal effect of that feature. It's important to note that this interpretation assumes a linear relationship and that correlation doesn't imply causation.

---

### Question 13
Which assumption of linear regression is violated when the residuals show a clear pattern when plotted against predicted values?

A) Linearity  
B) Independence of errors  
C) Normality of residuals  
D) Homoscedasticity (constant variance)

**Answer: D**

**Explanation:** When residuals show a clear pattern (like a funnel shape or curve) when plotted against predicted values, it indicates heteroscedasticity - the variance of errors is not constant across all levels of the predicted values. This violates the assumption of homoscedasticity. A good model should show residuals randomly scattered around zero with no clear pattern.

---

### Question 14
What is the primary purpose of polynomial features in regression?

A) To reduce the number of parameters in the model  
B) To handle missing values in the dataset  
C) To normalize the input features  
D) To capture non-linear relationships between features and target

**Answer: D**

**Explanation:** Polynomial features allow linear regression models to capture non-linear relationships by creating new features that are powers or interactions of the original features. For example, adding x² as a feature allows the model to fit quadratic relationships. While the model remains linear in parameters, it can now represent curved relationships in the feature space.

---

### Question 15
In the context of gradient descent, what does the learning rate control?

A) The number of iterations the algorithm runs  
B) The initial values of the parameters  
C) The convergence criterion for stopping  
D) The size of steps taken towards the minimum

**Answer: D**

**Explanation:** The learning rate (often denoted as α or η) controls how large steps the algorithm takes when moving towards the minimum of the cost function. A learning rate that's too large might cause the algorithm to overshoot the minimum and fail to converge, while a learning rate that's too small will make the algorithm converge very slowly, requiring many iterations.

---

### Question 16
What problem can occur if the learning rate in gradient descent is too large?

A) The algorithm converges too slowly  
B) The algorithm gets stuck in local minima  
C) The algorithm requires more memory  
D) The algorithm overshoots the minimum and fails to converge

**Answer: D**

**Explanation:** If the learning rate is too large, the algorithm takes steps that are too big, potentially jumping over the minimum and oscillating around it without ever settling down. This can cause the cost function to increase rather than decrease, or to oscillate without converging. Finding the right learning rate is crucial for successful training.

---

### Question 17
Which of the following best describes stochastic gradient descent (SGD) compared to batch gradient descent?

A) SGD always converges faster to the global minimum  
B) SGD requires more memory than batch gradient descent  
C) SGD can only be used for linear models  
D) SGD updates parameters using one sample at a time, making it noisier but potentially faster

**Answer: D**

**Explanation:** Stochastic Gradient Descent updates parameters using one training example at a time (or small mini-batches), rather than using the entire dataset like batch gradient descent. This makes SGD updates noisier but allows for more frequent updates and can be computationally more efficient for large datasets. The noise can actually help escape local minima in non-convex problems.

---

### Question 18
What is the curse of dimensionality, and how does it particularly affect KNN?

A) It refers to overfitting in high-dimensional spaces  
B) It causes computational complexity to increase exponentially  
C) It only affects the training phase of algorithms  
D) It means that distance becomes less meaningful as dimensions increase

**Answer: D**

**Explanation:** The curse of dimensionality refers to various phenomena that arise in high-dimensional spaces. For KNN specifically, as the number of dimensions increases, all points tend to become roughly equidistant from each other, making the concept of "nearest" neighbors less meaningful. This degrades KNN's performance because it relies on the assumption that nearby points are similar.

---

### Question 19
In decision trees, what is the purpose of setting a minimum number of samples required to split a node?

A) To increase the tree's depth  
B) To speed up the training process  
C) To handle missing values better  
D) To prevent overfitting by avoiding splits on very small groups

**Answer: D**

**Explanation:** Setting a minimum number of samples required to split a node is a form of pre-pruning that helps prevent overfitting. If we allowed splits on very small groups (e.g., 2-3 samples), the tree might create very specific rules that don't generalize well. By requiring a minimum number of samples, we ensure that splits are based on statistically meaningful patterns rather than noise.

---

### Question 20
Which metric is commonly used to measure node impurity in classification trees?

A) Mean Squared Error  
B) Mean Absolute Error  
C) R-squared  
D) Gini impurity or Entropy

**Answer: D**

**Explanation:** For classification trees, Gini impurity and entropy are the most common measures of node impurity. Both measure how "mixed" the classes are in a node - a pure node (containing only one class) has impurity of 0, while a node with equal representation of all classes has maximum impurity. Mean Squared Error and Mean Absolute Error are used for regression trees.

---

### Question 21
What is the main benefit of using cross-validation when tuning hyperparameters?

A) It reduces training time  
B) It eliminates the need for a test set  
C) It automatically selects the best algorithm  
D) It provides a more reliable estimate of model performance on unseen data

**Answer: D**

**Explanation:** Cross-validation provides a more robust estimate of how well a model will perform on unseen data by training and evaluating the model multiple times on different subsets of the data. This helps prevent overfitting to a particular train-validation split and gives us more confidence in our hyperparameter choices. It doesn't eliminate the need for a final test set for unbiased performance evaluation.

---

### Question 22
In Random Forests, what typically happens to bias and variance compared to individual decision trees?

A) Both bias and variance increase  
B) Both bias and variance decrease  
C) Bias decreases, variance increases  
D) Bias slightly increases, variance significantly decreases

**Answer: D**

**Explanation:** Random Forests typically have slightly higher bias than individual deep decision trees (because averaging multiple models can smooth out some of the fine details each tree might capture), but they have significantly lower variance. The ensemble averaging effect dramatically reduces the variance, leading to better generalization despite the small increase in bias. This is a classic example of the bias-variance tradeoff.

---

### Question 23
Which statement about ensemble methods is most accurate?

A) Ensemble methods always outperform individual models  
B) Ensemble methods require all base models to be of the same type  
C) Ensemble methods always reduce both bias and variance  
D) Ensemble methods work best when base models are diverse and reasonably accurate

**Answer: D**

**Explanation:** Ensemble methods work best when the base models are diverse (make different types of errors) and reasonably accurate (better than random guessing). If all models make the same mistakes, combining them won't help. If the models are too weak (worse than random), combining them can actually hurt performance. Diversity ensures that errors cancel out when combining predictions.

---

### Question 24
In linear regression, what does R-squared measure?

A) The correlation between features  
B) The magnitude of the coefficients  
C) The number of significant features  
D) The proportion of variance in the target variable explained by the model

**Answer: D**

**Explanation:** R-squared (coefficient of determination) measures the proportion of variance in the target variable that is explained by the model. It ranges from 0 to 1, where 0 means the model explains none of the variance (no better than predicting the mean), and 1 means the model explains all the variance (perfect fit). It's a useful metric for understanding how well the model captures the relationship in the data.

---

### Question 25
What is regularization in the context of machine learning?

A) A technique to speed up training  
B) A way to handle missing data  
C) A feature selection technique  
D) A method to add constraints to prevent overfitting

**Answer: D**

**Explanation:** Regularization involves adding constraints or penalties to the model to prevent overfitting. Common types include L1 regularization (Lasso, which encourages sparsity) and L2 regularization (Ridge, which penalizes large coefficients). By constraining the model's complexity, regularization helps ensure the model generalizes well to new data rather than memorizing the training data.

---

### Question 26
Consider this Python code snippet for KNN:
```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
```
What does `weights='distance'` mean?

A) All neighbors contribute equally to the prediction  
B) Only the nearest neighbor's distance is considered  
C) The algorithm uses Euclidean distance only  
D) Closer neighbors have more influence on the prediction

**Answer: D**

**Explanation:** When `weights='distance'` is used, closer neighbors have more influence on the prediction than farther neighbors. The contribution of each neighbor is weighted inversely by its distance - closer neighbors get higher weights. This is often more sensible than uniform weighting because intuitively, more similar (closer) instances should have more say in the prediction.

---

### Question 27
In gradient descent, what does it mean when we say the algorithm has "converged"?

A) The learning rate has reached zero  
B) All gradients have become positive  
C) The number of iterations has reached the maximum  
D) The cost function has stopped decreasing significantly

**Answer: D**

**Explanation:** Convergence in gradient descent means the algorithm has reached a point where the cost function is no longer decreasing significantly between iterations. This typically indicates that we've reached a minimum (local or global) where the gradients are very small. We usually define convergence using a threshold - if the change in cost between iterations falls below this threshold, we consider the algorithm converged.

---

### Question 28
What is the primary advantage of mini-batch gradient descent over both batch and stochastic gradient descent?

A) It always finds the global minimum  
B) It requires less memory than both alternatives  
C) It works only with neural networks  
D) It balances computational efficiency with gradient estimate quality

**Answer: D**

**Explanation:** Mini-batch gradient descent processes small batches of data (typically 32-512 samples) rather than the entire dataset (batch) or single samples (stochastic). This provides a good balance: the gradient estimates are less noisy than SGD but more computationally efficient than batch gradient descent. It also allows for better utilization of vectorized operations and parallel computing.

---

### Question 29
Which of the following would be the best approach for a dataset with 10 features and 50 samples?

A) Deep neural network with many hidden layers  
B) Random Forest with 1000 trees  
C) Complex ensemble methods  
D) Simple models like logistic regression or small decision trees

**Answer: D**

**Explanation:** With only 50 samples and 10 features, complex models are likely to overfit severely. Simple models like logistic regression, small decision trees, or KNN with appropriate k values would be more appropriate. The key principle is to use simpler models when you have limited data. Complex models need large amounts of data to learn meaningful patterns without overfitting.

---

### Question 30
In the context of polynomial regression, what does "degree" refer to?

A) The number of features in the dataset  
B) The number of training samples  
C) The learning rate for optimization  
D) The highest power of the input features used in the model

**Answer: D**

**Explanation:** In polynomial regression, the degree refers to the highest power of the input features included in the model. For example, a degree-2 polynomial for one feature x would include terms like x and x². Higher degrees allow the model to fit more complex, curved relationships but also increase the risk of overfitting, especially with limited data. explains none of the variance (no better than predicting the mean), and 1 means the model explains all the variance (perfect fit). It's a useful metric for understanding how well the model captures the relationship in the data.

---

### Question 25
What is regularization in the context of machine learning?

A) A technique to speed up training  
B) A method to add constraints to prevent overfitting  
C) A way to handle missing data  
D) A feature selection technique

**Answer: B**

**Explanation:** Regularization involves adding constraints or penalties to the model to prevent overfitting. Common types include L1 regularization (Lasso, which encourages sparsity) and L2 regularization (Ridge, which penalizes large coefficients). By constraining the model's complexity, regularization helps ensure the model generalizes well to new data rather than memorizing the training data.

---

### Question 26
Consider this Python code snippet for KNN:
```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
```
What does `weights='distance'` mean?

A) All neighbors contribute equally to the prediction  
B) Closer neighbors have more influence on the prediction  
C) Only the nearest neighbor's distance is considered  
D) The algorithm uses Euclidean distance only

**Answer: B**

**Explanation:** When `weights='distance'` is used, closer neighbors have more influence on the prediction than farther neighbors. The contribution of each neighbor is weighted inversely by its distance - closer neighbors get higher weights. This is often more sensible than uniform weighting because intuitively, more similar (closer) instances should have more say in the prediction.

---

### Question 27
In gradient descent, what does it mean when we say the algorithm has "converged"?

A) The learning rate has reached zero  
B) All gradients have become positive  
C) The cost function has stopped decreasing significantly  
D) The number of iterations has reached the maximum

**Answer: C**

**Explanation:** Convergence in gradient descent means the algorithm has reached a point where the cost function is no longer decreasing significantly between iterations. This typically indicates that we've reached a minimum (local or global) where the gradients are very small. We usually define convergence using a threshold - if the change in cost between iterations falls below this threshold, we consider the algorithm converged.

---

### Question 28
What is the primary advantage of mini-batch gradient descent over both batch and stochastic gradient descent?

A) It always finds the global minimum  
B) It balances computational efficiency with gradient estimate quality  
C) It requires less memory than both alternatives  
D) It works only with neural networks

**Answer: B**

**Explanation:** Mini-batch gradient descent processes small batches of data (typically 32-512 samples) rather than the entire dataset (batch) or single samples (stochastic). This provides a good balance: the gradient estimates are less noisy than SGD but more computationally efficient than batch gradient descent. It also allows for better utilization of vectorized operations and parallel computing.

---

### Question 29
Which of the following would be the best approach for a dataset with 10 features and 50 samples?

A) Deep neural network with many hidden layers  
B) Random Forest with 1000 trees  
C) Simple models like logistic regression or small decision trees  
D) Complex ensemble methods

**Answer: C**

**Explanation:** With only 50 samples and 10 features, complex models are likely to overfit severely. Simple models like logistic regression, small decision trees, or KNN with appropriate k values would be more appropriate. The key principle is to use simpler models when you have limited data. Complex models need large amounts of data to learn meaningful patterns without overfitting.

---

### Question 30
In the context of polynomial regression, what does "degree" refer to?

A) The number of features in the dataset  
B) The highest power of the input features used in the model  
C) The number of training samples  
D) The learning rate for optimization

**Answer: B**

**Explanation:** In polynomial regression, the degree refers to the highest power of the input features included in the model. For example, a degree-2 polynomial for one feature x would include terms like x and x². Higher degrees allow the model to fit more complex, curved relationships but also increase the risk of overfitting, especially with limited data. models can smooth out some of the fine details each tree might capture), but they have significantly lower variance. The ensemble averaging effect dramatically reduces the variance, leading to better generalization despite the small increase in bias. This is a classic example of the bias-variance tradeoff.

---

### Question 23
Which statement about ensemble methods is most accurate?

A) Ensemble methods always outperform individual models  
B) Ensemble methods work best when base models are diverse and reasonably accurate  
C) Ensemble methods require all base models to be of the same type  
D) Ensemble methods always reduce both bias and variance

**Answer: B**

**Explanation:** Ensemble methods work best when the base models are diverse (make different types of errors) and reasonably accurate (better than random guessing). If all models make the same mistakes, combining them won't help. If the models are too weak (worse than random), combining them can actually hurt performance. Diversity ensures that errors cancel out when combining predictions.

---

### Question 24
In linear regression, what does R-squared measure?

A) The correlation between features  
B) The proportion of variance in the target variable explained by the model  
C) The magnitude of the coefficients  
D) The number of significant features

**Answer: B**

**Explanation:** R-squared (coefficient of determination) measures the proportion of variance in the target variable that is explained by the model. It ranges from 0 to 1, where 0 means the model explains none of the variance (no better than predicting the mean), and 1 means the model explains all the variance (perfect fit). It's a useful metric for understanding how well the model captures the relationship in the data.

---

### Question 25
What is regularization in the context of machine learning?

A) A technique to speed up training  
B) A method to add constraints to prevent overfitting  
C) A way to handle missing data  
D) A feature selection technique

**Answer: B**

**Explanation:** Regularization involves adding constraints or penalties to the model to prevent overfitting. Common types include L1 regularization (Lasso, which encourages sparsity) and L2 regularization (Ridge, which penalizes large coefficients). By constraining the model's complexity, regularization helps ensure the model generalizes well to new data rather than memorizing the training data.

---

### Question 26
Consider this Python code snippet for KNN:
```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
```
What does `weights='distance'` mean?

A) All neighbors contribute equally to the prediction  
B) Closer neighbors have more influence on the prediction  
C) Only the nearest neighbor's distance is considered  
D) The algorithm uses Euclidean distance only

**Answer: B**

**Explanation:** When `weights='distance'` is used, closer neighbors have more influence on the prediction than farther neighbors. The contribution of each neighbor is weighted inversely by its distance - closer neighbors get higher weights. This is often more sensible than uniform weighting because intuitively, more similar (closer) instances should have more say in the prediction.

---

### Question 27
In gradient descent, what does it mean when we say the algorithm has "converged"?

A) The learning rate has reached zero  
B) All gradients have become positive  
C) The cost function has stopped decreasing significantly  
D) The number of iterations has reached the maximum

**Answer: C**

**Explanation:** Convergence in gradient descent means the algorithm has reached a point where the cost function is no longer decreasing significantly between iterations. This typically indicates that we've reached a minimum (local or global) where the gradients are very small. We usually define convergence using a threshold - if the change in cost between iterations falls below this threshold, we consider the algorithm converged.

---

### Question 28
What is the primary advantage of mini-batch gradient descent over both batch and stochastic gradient descent?

A) It always finds the global minimum  
B) It balances computational efficiency with gradient estimate quality  
C) It requires less memory than both alternatives  
D) It works only with neural networks

**Answer: B**

**Explanation:** Mini-batch gradient descent processes small batches of data (typically 32-512 samples) rather than the entire dataset (batch) or single samples (stochastic). This provides a good balance: the gradient estimates are less noisy than SGD but more computationally efficient than batch gradient descent. It also allows for better utilization of vectorized operations and parallel computing.

---

### Question 29
Which of the following would be the best approach for a dataset with 10 features and 50 samples?

A) Deep neural network with many hidden layers  
B) Random Forest with 1000 trees  
C) Simple models like logistic regression or small decision trees  
D) Complex ensemble methods

**Answer: C**

**Explanation:** With only 50 samples and 10 features, complex models are likely to overfit severely. Simple models like logistic regression, small decision trees, or KNN with appropriate k values would be more appropriate. The key principle is to use simpler models when you have limited data. Complex models need large amounts of data to learn meaningful patterns without overfitting.

---

### Question 30
In the context of polynomial regression, what does "degree" refer to?

A) The number of features in the dataset  
B) The highest power of the input features used in the model  
C) The number of training samples  
D) The learning rate for optimization

**Answer: B**

**Explanation:** In polynomial regression, the degree refers to the highest power of the input features included in the model. For example, a degree-2 polynomial for one feature x would include terms like x and x². Higher degrees allow the model to fit more complex, curved relationships but also increase the risk of overfitting, especially with limited data.

---

## Scoring and Next Steps

**Count your correct answers and refer to the scoring guide at the beginning.**

If you scored below your target:
- Review the explanations for questions you missed
- Focus on the underlying concepts rather than memorizing answers
- Practice implementing these algorithms from scratch
- Work through more examples with real datasets

**Key concepts to reinforce:**
- Bias-variance tradeoff across all algorithms
- When to use which algorithm based on data characteristics
- The importance of proper validation and evaluation
- How hyperparameters affect model behavior

Remember, understanding these fundamentals deeply will serve you well as you progress to more advanced machine learning topics!